index,channel,episode,chapter,start_time,end_time,text,file_name_prefix,prev_index,next_index
1,a16z,A Nuclear Comeback: Are New Reactors the Answer?,The Promise of Advanced Nuclear Reactors,0,197,they recognize we have this insatable need for energy like we're not going backwards in our energy consumption so if we're going to have new energy generation has to be clean energy deliveries of fuel are a clear vulnerability natural gas obviously can hold an entire nation hostage the typical construction timeline is really like 6 to 15 years on the big reactors right now the really exciting thing for me is that at the really far into the scale the portable micro reactors we haven't really achieved that yet you could actually produce these in a factory because they're portable you could do mass production maybe nuclear energy is a lot safer than we actually originally realized the radiation exposure from living next to a coal plant is higher than the radiation exposure from living next to a nuclear power plant 105 years from now the idea that we can't just immediately turn on a reliable and enduring power source for a community it's going to be unimaginable like it's just will be a solved problem what might surprise some people to learn is that nuclear energy accounts for 20% of the electricity in the United States but what I think will surprise very few people is to learn that this carbon-free energy source has quite the stored history over the last few decades resulting in new reactors slowing almost entirely to a halt however the past few years have been what some people might call a comeback story in 2023 we saw America's first newly built reactor come online in over three decades but we're also seeing startups build entirely new types of reactors public discourse shifting and even the US government itself recently announcing its intent to Triple nuclear power production by 2050 so in today's episode originally recorded in the heart of Washington DC back in January and a16 Z's American dynamism Summit we talk about this truly unique moment in time for nuclear energy a6c General partner David ovich joins forces with duck bernauer CEO of micro reactor company radiant and Dr Katherine Huff assistant Secretary of the office of nuclear energy as they collectively discuss nuclear Energy's role in our country's future because remember energy is vital to many of the industries that we talk about here energy pow is the data centers that run our clouds the electric cars that drive on our streets and of course is fuel for the fact iies that build our future so if anything feels certain is that we're going to need more energy not less so tune in here as this group of policy makers Founders and funders discuss why increasing our nuclear capacity should be a national priority and what it'll take to reverse this multi-decade trend oh and if you'd like to get an inside look into a16 Z's American dynamism Summit you can watch several of the stage talks from the event featuring policy makers like Congressman Jake aen CL or Senator Todd young and of course both Founders and funders building toward American dynamism you can find all of the above at az.com admit all right let's get started nuclear has quite the stored,A Nuclear Comeback: Are New Reactors the Answer? - 001,,22
2,a16z,A Nuclear Comeback: Are New Reactors the Answer?,Nuclear Energy's Current Landscape,197,410,started nuclear has quite the stored history and in the last 50 years in particular progress has really slowed and I'd love to get your take Dr Huff what's your take on the key factors that you really attribute to that the expense of different kinds of energy technology typically determine what utilities are going to select and there was a serious period of time where for example natural gas was an extremely cheap option to build quickly with low capital investment and that I think is the primary driver for a lot of reduction in the builds for nuclear and then as you lose that muscle it becomes more and more expensive to get it back right much like if you stop working out yeah totally and I think there's a lot of public opinion around nuclear some people that to again that slowdown over the last few decades Doug is there anything you'd add there and maybe other misconceptions that you think the public holds I think over the past 50 years A lot has happened you know uh solar and wind technology really came about and was deployed kind of in the middle of this nuclear story we're telling um and these you know forms of lowcost energy um but that are not resilient forms of energy and by the time we deployed them significantly to the Grave we started to look for a source of power that you can scale that you can throttle up and down on demand um and at the same time we scientists started to care about climate and then I think the the public really has come around to really care about climate and not just to care but to want to do something about it and so I think nuclear has this really cool uh new role to fill um instead of natural gas which I agree with Dr Huff uh the low cost of it definitely caus us to adopt that to fill that need rather than nuclear but I think nuclear can leap frog it yeah and I think we are seeing renewed interest which is exciting especially over the last few years David what's your take on that is there a really strong why now and is it just climate or is there another series of factors at play I think that we have the sensationable thirst for energy and we have so many things that power the way we live and the way we want to live and the way we want to work that require electricity um and so we need all forms of energy to be increased I think um some are better than others some have a longer feature I think ahead of them than others and so for me nuclear is this base load energy you don't need the wind to be blowing you don't need the sun to be out um and it can be delivered in a lot of form factors uh and so to me that makes it very exciting and worth a lot of really really worth the investment in rebuilding the muscle to to use the analogy that Dr Huff uh made earlier to really rebuild the muscle of how do we build nuclear power plants and what kind of plants do we want to build and to power what kind of loads and in what kind of circumstances um and so I think that as people I think climate's a huge part of it people want they recognize we have this insatable need for energy like we're not going backwards in our energy consumption that that ship has sailed um so if we're going to have new energy uh generation it has to be clean energy and I think that there's also been I think A Renewed interest in re-educate in people getting re-educated as to what are the risks and opportunities with nuclear energy and I think that's come from a lot of different places um whether it's from government or from industry or from Academia that maybe nuclear energy is a lot safer than we actually originally realized um and that's really worth the the time spent there to to see if that's a viable way to generate the kind of energy that we need in the future I mean,A Nuclear Comeback: Are New Reactors the Answer? - 002,21,23
3,a16z,A Nuclear Comeback: Are New Reactors the Answer?,Vulnerabilities in Fuel Delivery,410,616,a viable way to generate the kind of energy that we need in the future I mean it's been a long time right we're we're not in the same place as many decades ago there are new reactor designs which we will definitely get to but Dr Huff I want to talk about your house Tes Tony that you did recently and you referred to our current approach to nuclear as quote a national security vulnerability and you reinforced that the office of nuclear energy is doing a few things so first keeping the existing Fleet of reactors operating and online deploying new Advanced reactor Technologies sustaining and securing the nuclear fuel cycle and expanding nuclear energy cooperation and something I'd love to get you to touch on is really that role of nuclear in America's Global standing and security thanks for that and I think it is that that why now question ties to this um you know in the testimony the particular component of our approach that I think is an active vulnerability is the fuel cycle security but energy security and energy resilience that nuclear energy can provide to support those more variable sources to be on no matter what day it is what time it is to to not require refueling consistent deliveries of fuel we've seen in for example uh embattled Russia Ukraine Invasion we've seen that deliveries of fuel are a clear vulnerability natural gas obviously can hold an entire nation hostage if you can not dispatch that power then it's not useful to your resilience nuclear power on the other hand even existing conventional plants they only need to be refueled once every 18 months maybe two years right and so you know they can they can run alone as an island for quite a while and this really underpins what we can see as an energy security and energy resilience that that frankly in today's geopolitical Universe represents our access to sovereignty as Nations our you know continued operation as you know independent states you know the US has a number of other features that make it a secure Nation but there are a lot of other countries that really can be threatened by um another nation that would use energy as a weapon yeah and we have many technologists building towards these new reactors and improving that fuel cycle but can you speak specifically to the government's role in that what is the government's role in securing that fuel cycle and that GL Global cooperation while also ensuring that we're getting these new reactors built in the USA yeah it's it takes a lot of pieces right with tax credits from the inflation reduction act and grant funding from the bipartisan infrastructure law we're able to do things like encourage subsequent license renewals to be economic for existing nuclear power or in the case of you know the bipartisan infrastructure law dollars Advanced demonstration programs but the office of nuclear Energy's Focus has historically been on R&D programs the kind of R&D that takes startups like radiant and you know support technological advancements and through small grants we've tried to expand our support of as many companies as possible and there are lots of them more than we could possibly support actually and it's I think a feature of many decades of working through the National Laboratories which we manage and operate from the Department of energy um and ensuring that we have a basis a tech strong technological capability to support the kinds of you know scientific Explorations that we need to propel those Technologies forward so let's talk,A Nuclear Comeback: Are New Reactors the Answer? - 003,22,24
4,a16z,A Nuclear Comeback: Are New Reactors the Answer?,Nuclear Energy's Timeline,616,815,"Explorations that we need to propel those Technologies forward so let's talk about some of those new technologies Doug let's throw it over to you can you actually just break down the different generations of reactors and also where we've come right as I said before it's been several decades where are we now in that technology yeah thank you um I'm G to ask Dr Huff to correct me on anything I get wrong here I'm actually not an expert in all the history of nuclear and every kind of reactor that there is I'm really an expert in what I'm doing yeah um which is portable uh high temperature gas reactor the generations roughly um you know people use these gen 1 2 3 and four terms uh gen one are really the reactors we first figured out they're kind of pre-enrichment they're usually like a graphite moderated reactor um and then generation 2 reactors were very different because we started to do in M with these big gas diffusion enrichment plant and in that time period uh the US was really the for at the Forefront of everything I think we had over 400 uranium mines operating in the US um and really uh we were the Great developers and exporters of our technology to the rest of the world and then gen 3 uh really are meant to be uh an advanced form of those Gen 2 reactors that can make use of enrichment and they're like uh accident tolerant fuels um or uh ability to recycle fuel they kind of these Advanced features and then Gen 4 is really meant to represent these things that are much farther away that are kind of future that you know are like you know they perfectly produce hydrogen let's say and and make a hydrogen economy possible so you have like big reactors right they're a gwatt they're for a million people or a million homes let's say so it's more than a million people this kind of the right scale um and then we've got smrs and the purpose of smrs is really to take that big reactor and build it really fast okay and the typical construction timeline is really like six to 15 years on the big reactors right now depending who makes them even for the exact same reactor like an ap1000 built in the US is very slow built in Asia is very fast um but you can make the reactor smaller you can make it faster the idea behind SMR is to you know make something that's um maybe for about not a million homes but maybe about 250,000 homes right a quarter of the size but be able to build it really quickly and if we could achieve that it would be a great economic success but the the spectrum is getting longer and longer I think one of the interesting things happening now people are looking smaller and smaller and looking at micro reactors and there's kind of two categories of micro reactors um portable and non-portable and if you do a fully portable micro reactor this is around the scale of like a thousand homes so it's a thousand times smaller than the reactor all the way at the other end of the spectrum um and you have Micro reactors which are not portable which are you know 10 or 20 times larger maybe like 10 or 20 megawatt electric something like that um the really exciting thing for me is that at the really far end of the scale the portal micro reactors we haven't really achieved that yet and you could actually produce these in a factory because they're portable you could do mass production um and then ship them around and very quickly deploy them in all these little areas where you have you know equivalent of a thousand homes which could also be like you know a workplace that has 2500 people a mine a military base a hospital um in some key kind of remote region so I like this scale better than than like thinking about the advanced ones more about like deployment and like how can you the use case put them out yeah in different",A Nuclear Comeback: Are New Reactors the Answer? - 004,23,25
5,a16z,A Nuclear Comeback: Are New Reactors the Answer?,Portable Microreactors and Mass Production,815,891,"deployment and like how can you the use case put them out yeah in different areas maybe we can talk about the use case right so when we're talking about let's say a military base what is the current state right if we're not using these micro reactors what is being used today and what's the trade-off there if we can actually get to that future reactor current state in literary base is that they have backup generators like any any site that has critical infrastructure um those backup generators will have storage tanks they'll be 40,000 to to 150,000 gallons of diesel um on those sites and they only only use it in a backup scenario so it requires they put batteries all over the installation and uh if they're is an outage they're typically going to run out of that diesel um especially if there's you know something like the that Colonial pipeline Ransom were attack where we lost an ability to move fuel in a huge multi-state area um and they they run out a fuel before their their time frame which is um usually a 14-day resilience time frame um so they've got a problem and they're looking for Solutions and they're actually very interested in um both categories of micro reactor because those are this around the scale of the base all of the larger ones an SMR or a gigawatt class reactor would be too large for them David let's bring you",A Nuclear Comeback: Are New Reactors the Answer? - 005,24,26
6,a16z,A Nuclear Comeback: Are New Reactors the Answer?,Nuclear Energy's Role in America,891,1616,"gigawatt class reactor would be too large for them David let's bring you into this conversation obviously we've invested in radiant and I want to get your sense of where you see Capital being deployed in this new ecosystem as new reactors are coming online what's the opportunity here and where do you see again some of those private dollars actually being deployed to yeah so let me let me I guess back up and and both Doug and and Dr Huff talked touched upon some really important I think points that relate to why having modular or just numerous points of energy generation spread across our grid and possibly amongst our allies is really an important concept because I think we we touched on it but for people that are that are not spending all their time in energy it may not be obvious why energy and defense or and National Security and and sovereignty are really really interrelated um so I'll give it I'll just give one example and this is this is maybe not what Dr Huff was referring to but take take an Ireland country like Taiwan that does not have its own energy Independence um you could imagine a blockade of a country like Taiwan where um coal oil or other fuel sources that are normally supposed to to go deliver fuel to the island are prevented from reaching their ports um at that point a country like Taiwan may only have a week or two or three weeks of fuel on the island and rather than having some kind of a kinetic or or war on on Taiwan a blockade would just be equally potentially as devastating um you can imagine hospitals running out of their generator Supply military bases uh not not able to turn the lights on runways have no Runway lights I mean it you know it Cascades from there um and just all the infrastructure eventually just quickly starts to fall apart and so for for countries that that same example actually applies to the us we do have lots of geographic things that protect us as a country we have two major oceans on both sides we have lots of resources um we obviously have our own fuel supplies uh but our grid is very brittle and one there's multiple ways to make the grid more resilient but one way is just adding capacity in distributed Fashions so that when there are power line issues or fuel transfer issues you're not to reliant on these major sources of power or for fuel to power entire parts of our country and so that that's just critically important that we increase the resiliency of our grid by adding redundancies and and lots of power generation and if we are able to do that by creating these more uh modular reactors even if they're not mobile um but modular reactors we could I one of the issues with nuclear historically is that it costs a lot of money to build not just time but a lot of money one of the reasons it cost a lot of money is we don't actually make that much of it and if we made more of it and we sort of developed that muscle we could make more reactors more cheaply right um and by doing that we could place them in strategic places across the country make sure they're close to our key Air Force bases and military bases uh in places where we need really reliable and enduring energy um so that that's why it's so important for National Defense and for security um and why our allies carry care about it as well no country wants to be totally at the mercy uh for energy of of other countries and so that's important okay so with that and I did I say anything wrong there perfect okay I think I got that right so I heard I heard deploy more more energy more yep and and then you know we didn't even talk about we didn't even talk about data centers but like you know we're going through an AI Revolution right now and you know it's going to bring lots of cool apps to our phones and our devices and our new vision goggles or whatever um and all kinds of new devices we don't even have yet um our cars are becoming electric and we got to charge those things up and so all those things need power uh and so we just need way more resiliency and way more capacity on the grid um and again that's going to come from lots of ways but nuclear is a is a really really good way that's why um I think there's this is like this much more palpable energy okay so now to your question of the capital why are we investing in nuclear and where is that money going to come from was that part of the question well yeah I think just as we get these new reactor designs coming into this ecosystem like is there a new play is there a reason why now private dollars are yeah so look I I think the National Lab system which is really a unique and special thing about America um other countries have things that they try to replicate the National Lab system but there's nothing quite as robust um and the National Lab system has many roles and Dr Huff can speak to this better than than I can since I think she helps oversee the National Lab system uh in her current role um but they not only do they provide research but they're actually part of the supply chain of fuel for nuclear power plants um they provide grants and funding uh for private uh industry to work on nuclear reactor designs there's a competition element that the National Lab system Fosters and the doe Fosters uh and and so that has been all wonderful but I think that we have noticed that there's an opportunity to be an accelerant to what's happening in the National Lab system and what's happening in which is closely tied to Academia to say hey look maybe there's a commercial opportunity and actually maybe it's possible when we think about all these data centers that people want to build you know a lot of utilities are private even if they're regulated that they're private companies maybe that we could say hey look maybe there's an opportunity to really jump start a different kind of power industry uh and that's a bet that we're willing to make we think that there's Tailwinds from a regulatory standpoint uh we think there's Tailwinds from an economic standpoint of building reactors there's a talent Tailwind so Doug worked at SpaceX when SpaceX first started there was only one company that really reliably put things into space and it was NASA um and now and now we have space doing it so often that it's it's almost a non-event now when when they launch uh satellites into space and Rockets into space and we think the same thing can be true with nuclear and it doesn't seem like the kind of Market where only one company can win uh as Doug mentioned there's all kinds of different approaches to nuclear for different use cases and so that's pretty exciting and I think that since since our investment in radiant what uh I've discovered is that there's a huge amount of what I call Downstream Capital so other investors who have larger pools of capital that are maybe not as risk tolerant as as as we are in recent horror wits but who want to do project financing uh or who want to fund large scale capital projects they have all they're very interested and then companies like Microsoft have spun up nuclear teams nuclear energy teams to figure out how do they procure energy that comes from a nuclear power plant uh and so that that to me just says look there there it's unclear exactly what the road map is going to look like my two colleagues here will know better than I do but there there's just a lot of momentum and enthusiasm for something that we know is possible there's no scientific risk like we're with nuclear that's another important thing is we invest in all kinds of things there's no scientific risk with nuclear energy or minimal SC like we know how it works the the science has understood yeah we've known for a while we we can come up with better designs uh and better programs and we need new kinds of fuels but we know how it works it's not science fiction um it's very real yeah it's science reality so that's why I'm excited about it and that's why I think there's a lot more capital an interest in it now and people recognize it's a a predicate for for everything else we want to do yeah I mean something you mentioned several times there is the economics and even you talked about space and that whole industry being rethought due to the economics fundamentally shifting so can we talk about that and the role of regulation in impacting some of these projects I think a lot of people cite vogul as a project where the economics were you know far out of proportion at least relative to the original project plan and a lot of people think you know that's an example of where people aren't willing to invest in nuclear Dr Hoff can you just speak to maybe how regulation plays a role in enabling some of these projects and whether any of that is changing or maybe whether Vogal is an outlier how do you think about that Vogal in a very real sense is a first of a Kind build it's not the first of a kind you know as was already mentioned you know those AP 1000s can be built faster and different environments but those different environments aren't different just because of Regulation they are also different because of the work capacity available so you know you look at a Chinese build of an ap1000 and comparative ogal and they had real differences in the workforce availability and I think th that's one of the longer polls in the tent not to divert from your question about regulation but I do think you know the nuclear Regulatory Commission does an incredibly good job keeping nuclear reactors operating safely they have an incredible safety record here in the United States and NRC makes sure that that's true it makes it easy for me to say nuclear nuclear power is safe it's going to continue to be safe here in the US US nuclear technology is some of the safest in the world and people should import it rather than you know some different technology and you know we know how to do it well it can increase timelines it can increase costs but I think even more critical is going to be Workforce and supply chain issues that can delay the deployment of me mega projects so regardless of whether you're a nuclear reactor that you're building or whether you're looking at building a bridge or a highway system or a rail line these Mega projects in the billions of dollars take years they sometimes take significantly more time than they should and each day in a project like that is another day on which you are holding billions of dollars of capital and not making profit and the cost of capital then starts to play into the total cost of the project and so the timeline on which you can deploy a reactor depends on yes regulation but also Workforce availability and supply chain issues and like simple project management adding up all of these things the US has lost this muscle being able to do this efficiently in these big big Mega projects whether it's an airport or a nuclear reactor and by executing Vogal we have succeeded at getting there with some reactors I mean the Vogal unit 4 will turn on you know in a few months Vogal unit 3 has turned on and is providing clean power to the people of Georgia and in the course of doing so it has ensured this the availability of some Supply chains around nuclear it has trained thousands of workers that are otherwise you know excellent skilled crafts workers and are now nuclear trained skilled crafts workers electricians and boiler makers and uh welders and everyone else you know all of the Building Trades and Etc I mean they had peaks of you know staff on site you know around 8,000 people it's a huge number of people on site building a reactor you know Union crafts workers from 48 states um and so that is the thing that I would point to as something that I would worry about in the longer term around the profitability of you know reactors is that you know we've now shown that ap1000 can be built if you were to replicate that particular reactor you should see some learnings right because now you've got a bunch of workers you can draw on you've got some Supply chains you can draw on but so too can all the other reactor companies that are planning to build new technologies they'll share some of the supply chain they'll share some of the workers and if we don't do it tomorrow a lot of those workers will go and build win turbin they have other things to do this is a really tight environment um to have you know enough skill set for the kinds of builds we need to do across the energy space Not Just nuclear so is that what you would point to it's not so much the regulation but ensuring that we have that Workforce is there anything you would do or say that we can do to improve that Outlook yeah I think you know a focus on trade school instead of merely universities you know I say this as a former and future University Professor it is absolutely important the trade schools and community colleges and you know Union training programs all be stood up at the capacity we need for nuclear builds wind turban build outs you know solar panel build outs the kinds of transmission build outs we're going to need um and regulation certainly can get faster but I would focus instead of lowering the barriers and accelerating the process so Doug",A Nuclear Comeback: Are New Reactors the Answer? - 006,25,27
7,a16z,A Nuclear Comeback: Are New Reactors the Answer?,Challenges of Portable Micro Reactors,1616,1803,"focus instead of lowering the barriers and accelerating the process so Doug obviously you're building in this space how do you think about those relationships whether it's with Regulators or with the large workforces that are needed in some of these cases how do you think of those relationships becoming productive I have so much I want to unpack uh so I think what Dr Huff was talking about um I want to connect a little further so a really really big plant that was built like that is amazing it's awesome and all that Workforce that we trained and I think that can apply across this entire spectrum of the different real sizes that any successful project should be cross pounding that other project and that's not just from regulatory sense but from just gaining that experience that learning by doing and getting the cost to be lower so I'm excited to be part of that way down at the tiny end of the spectrum where we're our reactors are 1,000 times smaller but the the regulatory environment uh does need to change and I think we were already working on it there a bunch of NRC modernization efforts um coming by Direct through Congress um we've been working on developing things like 10 CFR 53 and that will be an ongoing and continuing effort but I think for it to really succeed we need reactors to get built to get fueled to demonstrate and the doe to a large degree are already fully supporting that um and just for the audience can you break down what some of those changes are reference well I think some of the changes are are really just broad broad spectrum we don't have reactors that are this small that can be built in a factory I'm just going to talk about uh portable micro reactors only um you know for us to succeed at doing that our timeline has not changed since we started a company in 2020 we want to do a fuel demonstration in 2026 um we're we're going to go through doe authorization licensing this path that exists at the National Labs to go faster than normal um to do a test reactor at a test facility where you've got all the National Labs support the expertise the poster radiation experiment Labs um we're going to do that with our first unit uh our second unit though needs to go through NRC licensing and so we've got to staff up in our uh a little 45 person company uh these parallel paths to support going along both sets of regulations and I think the the two could be actually woven together in a really practical Manner and I know that people have thought about this for a while we just haven't achieved it so so that's one of the things we could do um but we've never cited a factory to mass-produce reactors um what's funny about that is the regulations actually exist if you go look at like the original code 10 cfr50 has a thing called a manufacturing license it's in there and unused but I think um if we willing to how many how many manufacturing licenses are there there zero there's zero there never been used well in the US this is just us reactor Factory there there there exists no such sounds like a good idea though yeah yeah the only way a micro reactor is going to get to the economics right is if it's built more like airplanes absolutely so we've been looking at that code learning about it figuring out what our questions are talking with the NRC",A Nuclear Comeback: Are New Reactors the Answer? - 007,26,28
8,a16z,A Nuclear Comeback: Are New Reactors the Answer?,The Evolution of Nuclear Reactors,1803,2281,code learning about it figuring out what our questions are talking with the NRC um actually our uh kidos micro reactor is now officially in pre-application um only very recently uh we're on the nrc's website they're planning for uh us in their budget so we can get that uh sorted out on time um so we'll be uh citing a reactor very quickly we want to deploy a unit in 2028 we're going to to build our unit with as much support as we can gather we're not going to uh change our timeline um and we started to feel real support from the doe I just I want to say thanks uh for the support we have at um we're working with idah National Laboratory um R is committed to it being ready in 2026 uh to go into the the Dome there's an old uh experimental breeder reactor Dome that was converted to now do these micro reactor demonstration experiments and a lot of work and effort and funds have gone to build that structure and uh we're still on Target and ready to go use it as soon as it's available it's so exciting actually this feed study that radiant is doing you know they're in the first set of three companies that are going to sort of tell us exactly what they would do inside this former containment structure that housed one of the coolest reactors we've ever built out there in the in Idaho that reactor is over and now there's room for new reactors to try things out in a safe sandbox yeah and I know we're early stages but this picture of an assembly line of reactors is one that you know a few years ago might have sounded outlandish but now there are Builders creating this where will we be like let's say in a decade if this does come online can you just paint a picture Doug of you know where these reactors could be deployed and how maybe broadly they might be deployed and the use cases for them yeah absolutely so we'll start the 10 years in in 2026 so ideally we Fuel and demonstrate at full power in the dome uh and then in by 2028 we had One commercial unit just a few years later to do that we're we're really running two regulatory efforts in parallel um and then three units in 2029 eight units in 2030 scaling on up until we're at 2036 we should be making 50 units a year a reactor a week coming off the line um and the reactor we're developing it's a heavy unit but it can fit in a c17 aircraft or on a truck and you can move it around get it wherever it needs to go in the world um the optimal use case is really replacing diesel generators in some remote region and then a reactor lasts for 5 years approximately out in the field and then is shut down and we bring it back to that factory to refuel it so it is not only a new reactor construction Factory um but a line producing a bunch of new cores and a refueling facility all co-located on the same you know 25 acre or so plot of land and and so what we would do is have a population of about a thousand of these out in the world because we're planning for a 20-y year licensing time frame so youve got the 50 a year and about 20 years they last and so there's kind of a thousand of them that we can go and put in the Thousand most important places that there are so these are like North Slope in Alaska these really remote communities that uh the ocean freezes up for them and they have to store huge amounts of diesel and it can't get new over the winter so you've got to plan ahead and have enough um and even when they can get new the price variability is incredibly injust unjust give of that like how much they can flate by an order of magnitude and you can't plan ahead for your family's budget if you if you have to be planning ahead for diesel power that changes on the day-to-day time frame on the market especially in a geopolitical situation letting a town lease a reactor for 20 years is very doable yeah and it just dawned on me as all of you were saying that that you might imagine that people in the public might think oh I don't want a reactor in my backyard but at the same time in this scenario you could imagine that this Alaskan town would beg for that right we don't want this variance we actually like please give us this reactor now I think a lot of people would want a reactor near them maybe not in their literal backyard but I think that's mostly because they'd rather have a pool um not for any safety related reason uh but I think they want one near them heated pool and I think that if you're in a natural disaster area and you're hoping that FEMA is going to come in and they might come in and provide you some tents and shelters but it's very hard to provide Power in a real serious natural disaster whether it's wildfires whether it's hurricanes uh and the two things you need immediately after disaster clean water and you need power and you can't do clean water from a generator too it takes way too much fuel but you can do clean water from a reactor uh you can hook up a reactor to something that will clean water very easily and provide people with the water they need to survive and with energy and that to me the fact that you can bring that in on an 18wheeler is just supremely powerful uh and there's nothing like that today that exist in the world and the number of lives that it could change is tremendous so separate from all the defense related National Security related things that's just one more example of many um of where having the ability to quickly truck in or fly in reliable and enduring power it's the idea that we will to me you ask 10 years where are we going to be 10 or 15 years from now the idea that we can't just immediately turn on a reliable and enduring power source for a community it is going to be like it's going be unimaginable like it just it will be a solved problem I want to add one thing to that um you know not only is it a mass-produced reactor you can truck in but you can truck out so this use in FEMA right uh for temporary use uh is perfectly what the kidos micro reactor designed to do uh reactors don't carry themselves away and everything that was radioactive can be fully removed just on a normal truck um and you leave a green field the day you leave um that's never been seen before nuclear I don't think um I wanted to share that point yeah you can take it and move it somewhere else yeah it's a brilliant application I mean like people see these United Rentals trucks around that like when you go to a concert there's like you know the big United Rentals thing that's got this big generator and it's like makes all the noise yeah U it's like look we just have United Rental reactors like you know why not that's actually what we ran um when we did the hyperloop project at SpaceX I was in charge of all the electrical work for it we rented a big Diesel jensa and that ran this futuristic tube that we pumped down to vacuum and and ran Vehicles up to 350 M hour in but it could be every reactor I want to connect one other thing too we talked about this Alaskan Town there's one more thing that really motivates me uh about what we're doing in a lot of places they use diesel generators only for Prime power um the health implications of that are dramatic right a diesel gen set operating of course will produce uh CO2 but more critically more importantly it's producing carcinogens fumes that people breathing in that area they're breathing carcinogens um and if you look at um what happens in a town over 20 years of span um if you pick a diesel gen set instead of a reactor um there's something like 12 uh deaths that are going to occur prematurely from the use of diesel normal natural measure just just at a rate ending people's lives prematurely so that's one of the things that really motivates me um and on the regulatory side uh I think we we got to think about that case I think we've got to make it possible at some point in the future for that the people the decision makers in that that town this theoretical little town to be able to pick the nuclear reactor the clean technology that's going to save lives and to have a equal bar for regulations so that they can pick it because one of the barriers right now will be the regulations for nuclear very challenging to site these little reactors and it's because they don't exist and we haven't planned for it yet but that's um what I think we need to start working on now so that 10 years from now that future is achievable mhm and when we think about,A Nuclear Comeback: Are New Reactors the Answer? - 008,27,29
9,a16z,A Nuclear Comeback: Are New Reactors the Answer?,Nuclear’s Public Perception and Safety,2281,2495,"that 10 years from now that future is achievable mhm and when we think about these communities just so we can attack this question head on Dr Huff can you just speak to waste right that's something that comes up a lot from these reactors old reactors do we have a way currently today to safely store nuclear waste from these reactors yes this is a technically solved problem right now all the spent fuel is stored safely where it is it's a solid it's not a glowing green goo it's a ceramic it's more like a teacup right um now defense waste is a distinct thing but the commercial nuclear fuel in this country has never caused any you know radiation harm to humans it is stored safely in either pools or um in drycast storage it is however at 70 locations across the country in places where the department of energy promised to take it off of their hands they didn't intend to store it there for the long term and while it is safe for the long term as it currently stand stands it is the Department of energy responsibility to take it and consolidate it into a you know one or more Consolidated interim storage sites to reduce the number of communities that live near those uh facilities that they didn't agree to in the long term and so we're working through a consent-based process to identify locations that would be amimal to this um it's a really exciting process that worked really successfully in uh Finland to site a whole final repository and is working in Canada they're down to two sites uh for their final repository which is much more complicated than an interim storage facility um so it's our responsibility to do we're doing it there's no technical question about you know is it possible to safely store spent nuclear fuel we do it every day we've continued to do it we transport spent nuclear safe fuel safely across the United States successfully no problem just to expand a little bit to going back to this sort of what does the future look like 10 20 30 years from now right in addition to micro reactors saving the world at the sort of edge of you know accessibility to power at the edge of you know um viability of other options right where diesel generators might sit at the edge of that small size scale we also see real opportunities to directly replace one for one coal facilities right unabated fossil facilities across the country represent a real opportunity for those 100 200 300 megawatt units even bigger and they should be a real Boon to the communities in them because interestingly the radiation exposure from living next to a coal plant is higher than the radiation exposure from living next to a nuclear power plant we can reduce the radiation yeah because you know we we really you know there's no emissions from nuclear power and the emissions from unabated fossils actually can really um include a lot of heavy metals and whatnot I we're in this place where you know I think it's it's really important that communities especially communities around retiring and retired coal sites can have better Health outcomes just like what doug was saying about micro reactors and Diesel the same can be said about small modular reactors and and you know larger scale fossil plants and that motivates me too when we think about the 200 to 300,000 premature debts every year caused by pollution unnecessary pollution most of which is from power generation we can save those people yeah so it needs to be addressed at every scale it turns out that burning Trace radioactive materials and releasing them and these other energy forbs is much less safe than nuclear waste which is kept in containers and shipped and moved safely yeah has caused no accidents",A Nuclear Comeback: Are New Reactors the Answer? - 009,28,30
10,a16z,A Nuclear Comeback: Are New Reactors the Answer?,The Global Need for Nuclear Power,2495,3039,kept in containers and shipped and moved safely yeah has caused no accidents absolutely well maybe to come full circle Dr Huff in your recent testimony you mentioned that recently at cop 28 the US and 24 other countries signed an agreement to Triple nuclear power by 2050 that's very exciting but it also sounds quite lofty and so what do you really think needs to be in place we touched on some of these things whether it's regulation the workforce Etc public opinion A lot of these are shifting in terms of tithes as well I should say so what's your take on how we actually achieve that goal and reverse this multi-year trend yeah let me be clear you know these 24 countries signed together to say we recognize that we need to get to tripling nuclear power we didn't say we knew it would be possible you know I think the agreement here is that we recognize that there's Gap that has to be filled by Clean firm power and that Gap is gigantic and a huge fraction of that gigantic Gap must be filled by nuclear power or else we're never going to get to Net Zero and so this agreement is that you know the intergovernmental panel and climate change the Ia the Ia Etc have all done a bunch of studies we our individual countries have done studies about what it's going to take to get to Net Zero and it's going to take tripling nuclear power how do we get there um we are going to have to build new nuclear power at a rate unparalleled now not so crazy dissimilar from the rates of gigawatts we added in the' 70s 80s if we don't start tomorrow building reactors then the rate goes up so importantly if we don't build any new reactors next year then we're going to have to build slightly more every year between now and 2050 so the slower we are at startup the harder it's going to be to build out a supply chain appropriate for building the number of reactors we have to build um you know if you wait until the last minute to do all your homework you have to write a whole essay in one hour but if you spend the week ahead of time then you only have to write a few words a day and that's the situation we're in we have a little time but we have to start tomorrow we cannot wait until the last minute I think I think there's a couple points you brought up that I think about a lot um one you mentioned the supply chain for nuclear energy uh we need to be the source of fuel right now uh America is a source of nuclear fuel um but there are other countries that make a lot of nuclear Fuel and I think you know I I think about our American dynamism practices and investing in companies that support the national interest one of the things that I think is in the National interest is to be the Premier Source for nuclear fuel for not just the us but also our allies um and that's something we can do and we certainly certainly could do um much like I think storing nuclear spent nuclear fuel um or recycling nuclear fuel some people have this there's atmospherics around it like PR atmospherics and people are like oh I don't know if I want that they they ignore about all these other things they have in their backyard they they oh but that sounds bad because I saw a commercial once where I saw The Simpsons exactly um and they don't want to fish with three eyes which it's not a real thing um and and so I think we have an opportunity to really invest in the supply chain for nuclear resources and nuclear fuel um and I think it'd be really cool to see something kind of like the chips act for nuclear first of all I think it would be a very bipartisan thing in in this country um I also think it's something where we could really encourage investment abroad there are a lot of countries that would love to be having more would like to have more nuclear energy as Dr Huff uh said and I think we have an obligation to be a leader there um there are things called one two3 agreements that I believe the state department oversees today that sort of regulates the amount of nuclear information and nuclear sort of business that we can transact with certain countries um but it's still an onerous agreement and it's still there's different standards to that agreement um and there there could be a real National priority put on elevating those standards or making them more accessible or dis disseminating them more widely um while still maintaining and especially if we're the source of nuclear fuel for these countries it still gives us the levs of control that we want to enable countries to have more nuclear power but in a way that we think is safe and reliable and um you know represents the interests of our of our country I I do think there's some regulatory Improvement that that is gaining momentum here and we want to see more of I also think there can be a much larger International focus on you know America exports all kinds of Technology we export defense products we export all kinds of things and there's no reason why we shouldn't be exporting more um nuclear uh reactors and we and we do like ap1000 but we we could be doing much more um and that to me is exciting an exciting opportunity at least when you think about the commercial aspects that it's not just the US that has this insatable need for energy but it is a is a global opportunity absolutely and if we don't do it I think that other countries will y I guess that's that's the flip side of it is like you know right now um for instance there's a country that that uh very much would like nuclear reactors for energy um and the US is not allowed to sell into that country and currently the only other country bidding on a reactor is China um and I just think it'd be better if we could bid in that country also so absolutely Doug anything you'd add there in terms of you could say a wish list you're building in this space and are so many different factors that come together what do you hope to see whether it's the supply chain the workforces the regulation that we've talked a lot about uh I'm thinking we talk very long term I'm thinking much more shortterm about my wish list because I have a very tight schedule um operating the Dome is just two years away 23 months um uh I need to make sure I get access to fuel um something David mentioned it's really challenging and Dr has been helping we talk about this regularly and I appreciate it um but it's still a challenge for us uh I think a a real micro reactor demonstration program from the federal side would probably be the single biggest thing we could do to accelerate our efforts to commercialization um and I think that would help cross-pollinate every other project that we have going on Dr Huf I'm going to close with you anything else you'd like to share with the folks in the room but also we have so many people listening who may have varying degrees of education on nuclear the state of it in our country what would you like to leave people with about the years ahead yeah I think there's an incredible amount of money to be made there are lives to be saved there is democracy to preserve sovereignty to deploy abroad and we have unquestionably some of the best technology in the world that's American design it's an American invention uh we are the First Nation to ever you know sustain a fusion chain reaction on purpose we have the largest nuclear Fleet in the world we are poised to lead this as we transition into a cleaner energy system but we have to see Private Industry step up and say I will be the first to sign a contract to build the next radiant or whatever and I want to see as many contracts on the books as possible in the next couple of years or else we are going to have a much bigger supply chain challenge in the next 20 years than we have today every few months that are delayed between now and the order books that we need to show that deployment the harder it's going to be to build out as much as we need to get to Net Zero we have to get to Net Zero full stop not only have we promised the world we're leading the world and I intend to still be around in 2050 and I'd like to be able to breathe here here here here that's a great place to end off we thank Dr Huff I think for her effort in doe and really pushing forward I would say A Renewed and re-energized attitude towards nuclear policy it's you know innovators like radiant and others that are really leading the way and give us some good work to work with well I mean I think that's why we brought all three of you in right we have all sides of the equation we have the funders the builders and the policy makers um all in the room because that's all required for the future now if you have made it this far don't forget that you can get an inside look into A6 Z's American dynamism Summit at az.com Summit there you can catch several of the exclusive stage talks featuring policy makers like deputy secretary of defense Kathleen Hicks or Governor Westmore of Maryland plus both Founders from companies like andril and coinbase and funders like Mark Cuban all building toward American dynamism again you can find all of the above at az.com slad Summit and and we'll include a link in the show notes,A Nuclear Comeback: Are New Reactors the Answer? - 010,29,
11,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Intro,0,80,I really freaked out about this stuff in 2018 or 2019 or so I didn't know if I was crazy or press in I still don't know for sure welcome to the Logan Bartlett show on this episode what you're going to hear is a conversation I have with CEO and co-founder of anthropic Dario amade we talk about a bunch of different things including predictions for the future of artificial intelligence his upbringing how he came to found anthropic and spin out from open AI as well as what problems he thinks can be solved by artificial intelligence in the near future really interesting conversation with a very thoughtful person in an important position of power running one of the leading AI companies in the world if we can avoid the downsides then the stuff about curing cancer extending the human lifespan solving problems like mental illness this all sounds utopian but I don't think it's outside the scope of what the technology can do so chance that something goes quite catastrophically wrong on the scale of human civilization you know it might be somewhere between 10 and 25% if you're enjoying this discussion and conversation with Dario please do subscribe to this podcast on on whatever Channel you're listening to and if you're here to listen to more topics of artificial intelligence I would encourage you to subscribe to Red points podcast on the subject unsupervised learning which you can find on any podcast later [Music] now Dario thanks for doing this thanks,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 001,,79
12,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Joining OpenAI,80,871,now Dario thanks for doing this thanks for having me so I normally don't tell people's backgrounds in a linear fashion but I I actually haven't heard yours I don't know if you've ever really told it in in earnest like childhood growing up sort of what led you to starting anthropic so maybe can you share a little bit about like your background childhood growing up yeah yeah so I I don't know that my childhood was that interesting or that that different from you know from people who are in Tech or found companies I mean I was always I was always really interested in math it felt like it had uh you know a sense of sense of objectivity right you know one kid could say oh the show is great and the other kid could say oh it's terrible but you know if if when you're doing math you're like oh man there's an objective answer to this so that was always very interesting to me and uh you know I grew up with a younger sister who's one of my co-founders and we always wanted to to save the world together um so it's actually actually actually kind of amusing that you know we're we're we're working we're working on something together that you know at least at least potentially could have could have very very wide scope um so yeah I mean in terms of how got from there to anthropic uh my interest in math led me to be uh you know physics major undergrad um but near the end of undergrad uh I started reading it was initially the work of Ray czw um who you know I think is a bit crazy about a lot of things but just the basic idea that there's this this acceleration there's this exponential acceleration of compute and that that's going to provide us enough compute somehow we had no idea how then we had no idea it was going to be neural Nets you know will somehow get us to like very powerful Ai and I I found found that idea really convincing so I was about to start grad school for theoretical physics and you know decided as soon as I got there that I wanted to do biophysics and computational Neuroscience because you know if there was going to be AI you know it didn't feel like AI was working yet and so I wanted to study the closest thing to that that that there was which was you know our which was our brains you know the the the closest it's a natural intelligence so therefore the closest thing to to an artificial intelligence that that that exists so I studied that for a few years and kind of worked on networks of real neurons and then uh you know shortly after I graduated I was at Stanford for a bit and then I saw a lot of the work coming out uh of of Google of Andrew ing's group at Stanford and so I said okay uh you know I should I should get involved in this area you know my my reaction at the time was oh my God I'm so late to this area the revolution has already happened and this was what year is this yeah 2014 right so so you know I was just like oh my God like you know this tiny community of 50 people like they're the Giants of this field it's too late to get in if I rush in maybe I can get some of the scraps um that was that was my mentality when I when I kind of entered entered the field and now of course it's you know it's it's nine years nine years later than that and you know every you know I I I interview someone every day who's like I you know I really want to get into this field um and so I ended up working with Andrew at BYU for a year I ended up working at Google brain for a year uh then I was one of the first people to join open AI in 2016 uh I was there for about uh 5 years uh and by the end of it I was VP of research was driving a lot of the research agenda um we built gpt2 gpt3 reinforcement learning from Human feedback which is you know me of course the method that's used in chat GPT and used along with other methods in our in our model Claude uh and you know one of the big themes of those five years was this idea of scaling that you can put more data more compute into the AI models and they just get better and better and I think that you know that thesis was really was really Central and the second thesis that was Central is you don't get everything that way you know you can scale the models up but there there are questions that are unanswered it's uh you know ultimately sort of the fact value distinction you scale the model up it learns more and more about the world but you're not telling it how to act how to behave what goals to pursue and so that that dangling thread that free variable was the was the second thing and so those were those were really kind of the two lessons that I that that that I learned and of course those ended up being the two things that that anthropic is really about what year was it that you joined openai uh it was 2016 and what was the original connection was it just that this seemed to be where the smart people were going yeah so I was actually uh initially invited uh to join the organization before it existed uh in uh late 2015 as it was uh as it was forming and uh decided decided not to um but then a few months after it started I kind of you know a bunch of smart people ended up joining and so I said oh maybe I'll maybe I'll do this after all you were there for a number of years and at some point you made the decision that anthropic was going to be I guess it wasn't initially you've had an unusual path it wasn't initially a uh company right originally it was started as a quasi research lab is that fair I mean our our our our our strategy has certainly evolved but actually it was a for-profit public benefit corporation uh since the beginning uh and we had even since the beginning we had something on our website saying you know we're doing research for now but you know we see the potential for commercial activity down the road so I think all of these things we kind of kept open his potentialities but you know you're right for the first for the first year and a half it was mostly Building Technology and we were agnostic on you know what exactly we were going to do with that technology or when um we kind of we kind of wanted to keep our options open we felt it was better than saying you know we're about this or we're about that and what was the thought at the time of hey we should go do something on our own was that your thought was it a group of people yeah it was it was definitely the thoughts of a group of people so there were seven co-founders who left um and then I I think in total we got four 14 or 15 people from open AI which was about 10% of the the size of the organization at the time um it you know was it's funny to look back on those days because you know in those days we were the language model part of open AI like we we you know we along with a couple people who stayed uh you know were were those who had developed and scaled the the language models there were many different other things going on at open AI right there was you know there was a robotics project a theorum improving project projects to play video games some of those still exist um but uh you know we felt that we were this kind of coherent group of people we had this review about you know language models and scaling which to be fair I think the organization supported but then we also had this view about you know we we need to we need to make these models safe in a certain way and you know we need to do them within an organization where we can really believe that these principles are Incorporated top to bottom open AI had a whole bunch of different things and still does experimenting around like was it evident at what point along the way was it evident that large language models were something that there was a lot of wood to chop and a lot of opportunity around yeah I mean I think I don't know it was obvious at different times to different people um so you know for for me I think there were there were a couple things you know the general scaling hypothesis I wrote this document called the Big Blob of compute in in 20 2017 um which I'll you know I'll probably uh probably publish at some point although it's primarily of you know like historical interest now um and so very much in my mind and I think the minds of you know like a small number of other of other uh of of of of of other people on both the team that left and the team that that didn't leave and you know some some in other places in the world as well it was clear that there was really something too scaling and then as soon as I saw gpt1 which was done by Alec Radford um who's still at still at open AI our team actually had nothing to do with gpt1 but we recognized it immediately and saw that the right thing to do with it was was to scale it and so for me everything was clear at that moment and even more clear as we kind of scaled up to gpt2 when you know we saw that you know the model was capable of my my favorite thing was like you know we were able to get the model to perform a regression analysis so you know you give it like you know the price the price of a house and ask it to predict the number of square feet or something like that you gave it a bunch of examples then you gave it one more price and you're like how many square feet it didn't do great but it did better than random and in those days I'm like oh my God this is like some kind of General reasoning and prediction engine like oh my God what is this thing I have in my hands right it's it's completely crazy so I I you know it has been my view ever since then that you know this this would be not just language models but you know language models is an Exemplar of the kinds of things you can scale up you know this this this would be uh you know really really Central to to the future of technology did you consider yourself like a founder or a CEO prior to actually doing it um no so I I I I really kind of never thought of myself that way like if you went back to you know me in childhood it would have been very unsurprising you know for me to be a scientist but you know I never kind of thought of myself as like a founder or a CEO or a business person right you know I always thought of myself as a scientist someone who discovers things but I think just having been at several different organizations convinced me that in fact I did have my own vision of how to run a company or how to run an organization because I'd seen so many and I thought well I I don't know I'd actually do it this way right and so sort of the the contrast you know not that not that I disagreed with every decision that made but just watching all these decisions go by took me to the point where I'm like actually I do I do have opinions on these questions right I I do have an idea of how you would grow an organization how you would run a research effort how you would bring the products of that research organization out into the world in a way that's you know makes business sense but is also responsible I don't think I don't think I really had those thoughts naturally but as I was brought into contact with organizations that that that did that then then I became excited about those things I kind of almost almost reluctantly learned that I I actually had strong opinions on these things it not the draw it in contrast to any specific names but uh maybe uh just in others in the field that I would consider large language Foundation model as a product what's what's something foundational uh I guess not to use a cute term but something foundational that you believe at anthropic that you would draw in distinction to others in the space yeah I would I would say a couple things um so you know one is just this idea that we should be building in safety from the beginning um now I'm aware we're kind of not the only ones who who who've who've said that but I feel like we've built that we've really built that in from the beginning we've thought about it from the beginning um we've started from a place of caution and kind of you know commercialized things brought things out into the world starting from hey you know can we can we open these switches one by one and see what actually makes sense um I think in particular you know a way a way I would would think about it is that what we're aiming to do is not just to be successful as a company on our own although we are trying to do that um but that we're also trying to kind of set a standard for the field set the pace for the field um so this is this is a concept we've called race to the top uh so you know race to the bottom is a is a popular term where you know everyone is you know competing to you know lower cost or delivers things as fast as possible and as a result they cut corners and things get worse and worse um so that that Dynamic is real and we always think about how not to contribute to it too much um but there's also a concept of race to the top which is that if if you do something that looks better it naturally has the effect that other players end up doing the same thing and this this happened for interpretability for a couple of years we were the only org that worked on interpretability seen inside neural Nets um there various corporate structures that uh we' We've that we've uh that we've uh you know that we've we've implemented that you know hope others Others May emulate um and uh you know recently we released this uh responsible scaling plan that I could talk more about later but but generally we're trying to set the pace we're trying to do something good inspiring also viable and encourage others to do the same thing and you know at the end again maybe we win in a business sense and of course that's great but maybe someone else you know maybe someone else wins in a business sense or you know we all win we all split it but the thing that matters is that the standards are increasing I want,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 002,78,80
13,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Are scaling and AI safety intertwined?,871,1231,split it but the thing that matters is that the standards are increasing I want to talk about uh all that stuff in a second but why do you think philosophically that like AI development or the scaling of models and safety why are they intertwined I've heard you maybe coiled together uh in in in different ways yeah yeah so I think this actually isn't such an unusual thing like I think this is true it's true in most Fields right like you know the the the common analogy is like is like Bridges you know make building a bridge in making it safe they aren't exactly the same thing but they both involve all the same principles of like civil engineering and it's kind of hard to work on bridge safety in the abstract aside from you know you know kind of outside the the context of like a concrete bridge like you know what you need to do is you need to look at the bridge you're like okay well these are the forces on it you know these are the this is the stress tensor this is you know the strength of the material or whatever that's the same thing you come up with in building the bridge if differs in any way maybe it differs in thinking about the edge cases like in safety you have to worry what goes wrong .1% of the time whereas in building the bridge you have to think about the the you have to think about the median case but you know it's it's all the same civil engineering it's all the same you know for forces forces of mechanical physics and I think it's I think it's the same um in AI in Ai and large language models and and in particular safety is itself a task like you know is this thing the model's doing right or wrong where where where you know right or wrong could be something as prosaic as is the model telling you how to hotwire a car or as you know scary and sophisticated as is the model going to help me build build a bioweapon or is it going to you know take over the world and make swarms of Nanobots or you know whatever futuristic thing figuring out whether the model is going to do that and the behavior of the model is itself an intellectual task of the kind that models do and so the problem and the solution to the problem are mixed together in this way where uh you know every time you get a more powerful model you also gain gain the capability to understand and potentially reign in the models so we have this problem where these two things are just are just mixed together in in in a way that's I think hard I think hard to untangle and I think that's the usual thing I think the only reason that's surprising is that the community of people who thought about AI safety was historically very separate from the community of people who developed AI like they had a different they had a different attitude they came to it from a you know kind of a a you know philosop more philosophical perspective more of a moral philosophy perspective whereas those who built the technology were engineers but just because the communities were different doesn't doesn't you know that that doesn't imply that the actual the actual content turned out to be different so I don't know that's that's my view on it it was becoming a business and commercializing your effort it if you could scale and figure all the safety stuff out without being a business would you would you pick that path or is the business side of it inherently intertwined as well as something that interests you yeah yeah so I'd say a few things on that one is I think it's I think it's actually going to be very difficult to build or would have been very difficult to build models of the scale that that we want without without being a commercial Enterprise I mean uh you know people make jokes about VCS been willing to pour huge amounts of money into anything but like you know I think I think that's that's only true up to a point right like you know there's there's there's business logic there's business logic behind it you guys you guys have LPS like things things need to you know it's it's not it's not just all hype train right things things need to make things things need to make sense eventually and so you know we're now getting to the point where you need you know certainly multiple billions of dollars and I think soon tens of billions of dollars to build models at the frontier and to safety with those models models at the frontier requires you to have intimate access to those models particularly for tasks for tasks like interpretability so first of all yeah I I just I just think it's very hard um on the other hand you know I or in support of that point uh I I also think that there are some things that that you learn from the from the business from the business side of things um some of it is just learning the muscle and the operation of things like trust and safety um so you know you know today we deal with with trust and safety issues like oh you know people are trying to use a model for for for inappropriate purposes right the the you know not not not things that are going to end the world but things that you know we we rather that people we'd rather that people not do I think the ultimate significance of being able to develop methods to address those things and you know kind of enforcing those things in practice when they're used at scale by users it allows us to practice for the cases that are really really high stakes uh and I think without that organizational institutional practice it might be uh it might be difficult to kind of just be thrown into the Shark Tank you know congratulations you built this amazing model you know it can cure cancer but also you know someone could make a biop plague that would kill that would kill a million people you've never built a trust in safety org you have to deploy this model in the world and make sure we do one and not that would that would just be a very a very difficult thing to do and I I don't think we would get it right now all that said I mean I I I will freely admit you know my my passion is the my passion is the science and the safety right you know that's that's kind of my first my first Passion um the you know the the the the business stuff is the business stuff is quite a lot of fun um you know I think you know we just just watched all the different customers you know just learning about the whole business ecosystem has been great um but you know definitely my my first Passion is you know is is is is is the science of it and making sure it goes well was there a serious debate,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 003,79,81
14,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Anthropic Early Days,1231,1444,is is the science of it and making sure it goes well was there a serious debate about like being a business versus not in the early days was that like a real conversation among yeah so I think I think certainly everyone was aware from the beginning that you know we you know that there was a good chance that we would commercialize the models at some point you know we we had this thing on our website I'm not sure if it's up there anymore but you can see it on the way back machine that said you know for now we're doing research but we see commercial po potential down the road so everyone who joined saw that and everyone who joined knew that um but there was a question of you know when exactly should we do it so there was a there was a period around I think it was April May June of 2022 when we had kind of the first version of Claude um which was actually like a smaller model than Claude one but you know we were we were training the model that that would become Claude one at that at that time and we realized that with RL from Human feedback we didn't have our constit tional AI method yet that this thing was actually great to interact with and you know all of our employees were having fun interacting with it on slack we showed it to some to a small number of external people and they had lots of fun they had lots of fun interacting with it so it definitely occurred to me and others that hey there there could have been a lot of commercial potential to this I don't think we anticipated the explosion that happened at the end of the year like we definitely saw potential I don't think we saw that much potential um but yeah we you know we definitely had a debate about about it and I I wasn't sure uh quite quite what to do I think our concern was that with the rate at which the technology was progressing a kind of big loud public release might accelerate things so fast that the ecosystem might not might not um know how to handle it and you know I didn't want our kind of First Act on the public stage you know after we'd said it you know after we put so much effort into being being being uh responsible to to accelerate things to accelerate things so greatly um I I generally feel like we made the right call there I think it's actually pretty debatable um you know there's many many Pros many cons but I think overall overall we made the right call uh and then you know certainly as you know as soon as soon as the other models were out and kind of the gun had been fired then we started putting these things out we're like okay all right you know now now there's definitely a market in this people people people people know about it and so you know we should we should we should we should we should get out ahead and you know indeed we've we've we managed to you know put ourselves among you know among the top two or three players in this space was that gun being fired and chat GPT sort of taking off was that similar to the maybe fear that you had in in of like hey this might start a race yeah yeah similar and in fact more so um so you know I think we saw it with you know Google's you know Google's reaction to it you know that there was definitely you know just judging from the Public Public statements you know a sense of fear an existential threat and I you know I think they respond in a very economically rational way I don't I don't blame them for it at all um but you put the two things together and you know it it really created an environment where things were you know racing forward very quickly and look I I love technology as much as the next person there was something like you know super exciting about the whole You Know M make them dance oh we're responding with something you know I mean I can I can get just as excited about this as everyone but given the rate at which the technology is progressing um you know there was a worrying aspect about this as as well um and so in this case I'm at least at least on balance glad that you know we we weren't the ones who fired that starting gun yeah got it well you,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 004,80,82
15,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Amazon's Investment in Anthropic,1444,1459,know we we weren't the ones who fired that starting gun yeah got it well you recently announced an investment from Amazon before that you did a round with spark and a little bit more traditional venture capitalist the round with Amazon it's complicated and I can't go into the details but it's not a full closed Price corporate uh round and all that before,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 005,81,83
16,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,FTX investment in Anthropic,1459,1550,details but it's not a full closed Price corporate uh round and all that before that you you had an unusual round as well with FTX right or was it how did that come to be yeah so um you know honestly there there was actually very little to that so you know there there's kind of a community of people who who cared a lot about AI safety you know back when he was doing FDX before he before he committed all the fraud or was caught committing all all the fraud that he committed uh Sam bankman freed was you know presented himself as someone who cared a lot about issues like pandemics AI safety um so you know he was he was known to people in my community and and honestly there's not much to tell I you know I only talked to him a handful of times the entity is still related to FTX right so there's potential that the anthropic investment could one day make FTX people whole depending on how it all plays out that's that's one that's one of the ironies that you know there's a there's a there's a bankruptcy estate or bankruptcy trust that owns these non-voting shares and uh you know so far they've they've uh they they've declined to sell them off but they're interested in doing so in a general sense and so I'm told there's people that are very interested in buying those shares from them you know I I can't comment on the market dynamics there and we don't we don't really control them right it's it's a sale between different parties but but but hey if those if those shares lead to you know the people who had their money stolen getting some or all of their money back then you know that's that's that's kind of a random chance but certain certainly a good thing be a,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 006,82,84
17,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Anthropic's Business Today,1550,1851,"that's that's kind of a random chance but certain certainly a good thing be a good outcome um so what do the business of anthropic look today you guys are focusing mostly on Enterprise customers yeah we are focusing mostly on Enterprise customers I would say we have both an Enterprise product and a consumer product um it makes a lot of sense if you're building one of these models to at least try to offer them in every direction that you can right because the thing that's expensive in terms of both money and people is building the base model once you have it wrapping it into consumer product versus wrapping it in an API while both of those things do take substantial work you know are not as expensive as the the you know kind of the the base work and the model um and so we have a consumer product that honestly is doing pretty well but at the same time you know our real Focus definitely is definitely is Enterprise um we've found that some of the properties of the model in a in a practical sense right the safety properties of the model in a very practical sense as opposed to kind of you know like a philosophical or future sense um are actually useful for the Enterprise use cases um you know we try to make our models helpful honest and harmless um honesty is a very good thing thing to have you know in knowledge work settings you know number of our customers are in like the finance industry the legal industry starting to get stuff on the health side different you know productivity productivity apps um those are all cases where a mistake is bad right you know you're doing some financial analysis you're doing some legal analysis like you you really you really have a premium on like make sure the thing knows what it doesn't know uh so you know giving you something misleading is much wor than than not giving you not giving you anything at all I mean that that's true across the board but I think it's especially true and true and true it's especially true in those Industries um for for Enterprises often kind of like inappropriate or embarrassing speeches you know something something that they're they're very concerned about even if it happens very rarely uh and so the ability to kind of steer and control the models better I think is is very appealing to to is very appealing to a number of Enterprise customers um another thing that's been helpful is this you know we have this longer context window so context window is like how much information the model can take in and process so our context window is 100,000 tokens tokens are this weird unit it really corresponds to 70,000 words uh but you know the next the model with the next biggest context window is GPT 4 where there's a version of it that has 32k tokens 32,000 um which is three times less but the main gp4 has 8,000 which is about 12 times less um and so just the ability to for example something you can do with Claude that you can't do with any other model is you know read a midsize book or novel or textbook or something just stick it into the context window upload it uh and and then start to ask questions about it uh and so that's something you can't do or can't do nearly as easily with any other model and then another thing that's actually been appealing is raw cost um so the sticker price of Claude 2 is is about 4X less than the sticker price of gp4 um and the way we've been able to do that I can't I can't go into the details um but we've worked a lot on algorithmic efficiency for both training and inference um so we're able to produce something that's you know in the same ballpark as gp4 and better for some things uh and we're able to produce it at a substantially lower cost um and we're in fact excited to extend that cost Advantage because you know we're working with custom chips with various different different uh companies and we think that could give an enduring advantage in terms of in terms of in terms of inference cost so so all of those are particularly helpful for the for the for the Enterprise case and and you know we've we we found pretty strong pretty strong Enterprise adoption um you know even even in the face of you know competition from multiple companies hi I'm Logan Bartlett the host of this podcast this is not an ad as you may know we do not advertise or monetize this podcast in any way okay I just wanted to take a quick second to tell you that we have a bunch of killer guests coming on over the course of the next few weeks and so if you're enjoying these conversations behind the scenes with both entrepreneurs and investors please do subscribe to our channel so you don't miss out now back to the episode what something that's perhaps unintuitive to someone that isn't living and breathing this every day about Enterprise interest in artificial intelligence as someone sitting at that Nexus first of all I see a huge advantage to f FKS who think in terms of the long term um so there's there's a fact that's kind of you know the bread and butter for those of us who are building the technology but you know getting getting it across to the customers is I think one of the most important things which is the pace at which the models are getting better some of them get it others are starting to get it but the reason this is important is you know put yourself in the shoes of a customer right they've got they've got",Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 007,83,85
18,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Dario's Advice For Builders,1851,2066,is you know put yourself in the shoes of a customer right they've got they've got our model clawed they want to you know they want to build something and you know typically they want to start with something small and of course naturally they think in terms of what can the model do today and what I always say is do that we got to start we got to iterate from somewhere but also think in terms of where the models are going to be in one or two years it's going to be a one or twoyear Arc to you know to go from proof of concept to small scale deployment to large scale deployment to True product Market fit for whatever it is that that you're launching um so you should basically skate where the puck is going to go you should think okay the models can't do this today but look they can do it 40% of the time that probably means they can do it 80 or 90% of the time in one or two years so let's have the faith the leaf of Faith to build for that instead of building for uh you know what the models are a what the models are able to do today and if you think that way the possibilities of what you can do in one to two years are much more expansive and we can talk about having a kind of longer term partnership where we where we build this thing together and I think you know the customers that have that have thought that way are you know ones that that that you know we we've been able to work together with on a path towards creating a lot of value um we also do lots of things that are just targeted as as at what you can do today but often the things I'm most excited about are those that see the potential of the technology and by starting to build now they'll have the thing tomorrow as soon as the model of tomorrow comes out instead of it being another year to build to build after that this is something that I think is uh particularly true of of anyone and particularly any leader but uh you would said recently attaching your incentives to the approval or cheering of a crowd in some ways destroys your mind and in some ways can destroy your soul you haven't been as public as other folks in the in the space have been I assume that's very purposeful stylistically and ties into that can you talk a little bit about your thought part of it it's just it's just kind of it's just kind of my my my style for one thing I mean you know I think I think as a scientist you know I prefer to speak when there's kind of something clear and substantive to say um you know I mean I'm not totally low profile I mean I'm on this podcast right now and I I I've been on a few so you know given the general volume of the field there's some need to to get the message out there and that need will probably increase over time um but I think I have noticed and you know it's not just you know Twitter social media but some phenomen on that's a little bit connected to them that you know you can really if you if you think too much in terms of kind of pleasing people in the short short term or you know making sure that you say something making sure that you say something popular it can really kind of lead you down to bad path and I've you know I've seen that with a lot of you know with a lot of very smart people I mean I'm sure you could name some as well I'm not going to give any names who who have gotten caught up in this and and you know years later you look at them and you're like wow this is a really smart person who's Who's acting much dumber than they are um and I you know I think the way it happens I don't know I could give an example right so you know take you know like a debate that's important to me which is like you know should we build these things fast or should we make these systems safe so there's an online community mostly on Twitter of you know people who who are you know think think we should slow down and then kind of an online community of Builders who are really excited about you know we should make this stuff fast and if you go to certain corners of Twitter like you get these really extreme versions of each one right on,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 008,84,86
19,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Should we pause AI progress?,2066,2187,Twitter like you get these really extreme versions of each one right on one hand you get people who say like we should stop building AI we should have a global worldwide pause right I think I think that doesn't work for a number of reasons I mean we have our responsible scaling plan is sort of incorporate some aspects of that so I think it's not an unreasonable uh you know discussion or debate to have but you know there's this kind of really extreme position and then that's that's kind of created this polarization where there's this this other extreme position that's like we have to build as fast as possible any any regulation is just regulatory capture you know we just need to maximize the speed of progress the most extreme people say things like it doesn't matter if Humanity's wiped out AI are the future I mean that's a really extreme position and and so you know think of kind of you know the the the position of someone who's kind of trying to be thoughtful about this trying to build but build carefully if you if you kind of enter that free too much if you uh you know if you feel like you have to make those people happy what can end up is either you get polarized on one side or another and then then you kind of repeat all the slogans of that of that side and you become a lot dumber than you would otherwise um the you know if if you're if you're really good at dealing with Twitter you know you can uh you can try and make both people happy but you know that that involves a lot of playing to both sides and i c i c I certainly don't want to do that that's what I talk about in kind of kind of losing your soul um you know the truth is the actual the you know the position that I think is actually responsible might be something that would make all of those people boo instead of instead of all of them cheer right and and and so you just you just have to be very careful if you're if if you're taking that as your barometer you know who's yelling at me on on Twitter who thinks I'm great on Twitter you're not going to arrive at the position that makes everyone boo that that might just be the correct position what timeline do you think about then when you're saw it's not the instantaneous dopamine de of a tweet you mentioned talking to Enterprises about 1 to two years and what can but like what timeline are you,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 009,85,87
20,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Future of AI,2187,2275,Enterprises about 1 to two years and what can but like what timeline are you solving for yeah I mean I guess I guess I kind of think like 5 to 10 years from now everything will be like a little bit more clear um and you know it it'll I think it'll be more clear which decisions were good decisions which decisions were bad decisions you know I think you know certainly less than that time scale is a time scale on which you know if dangerous things with these models are indeed possible as I believe they are but I could be wrong I think they may play out on that that time scale and you know we we'll we you know we'll be able to see like which companies addressed these dangers well um or were the dangers not real and and people like me warned about them and we were just totally wrong or uh you know will it turn out that some that some tragedy happened and you know people people like me should have been more extreme and worrying about it or or will it turn out that you know that that companies like anthropic you know picked picked the right path and you know na at a dangerous situation well I I don't know which how it's going to turn out I mean I hope it turns out that you know we navigated a dangerous situation well and we averted catastrophe and there were there were hard tradeoffs and you know we we addressed them uh skillfully and thoughtfully that's my hope for how it's going to turn out but I don't know that it's going to turn out that way but but you know I feel like looking at that you know in five years in 10 years that's that's just going to be a fair Judgment of all the things that that I'm saying and doing what what would you,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 010,86,88
21,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Dario's Biggest AI Safety Concerns,2275,2697,Judgment of all the things that that I'm saying and doing what what would you want the average person listening who is aware of AI knows what anthropic is knows what uh open AI is knows what Google and others are doing in the space about safety and about risk what would you want them to know from your perspective yeah so I think you know if I were if I were to kind of just put it in a few sentences I think what I would say is look I have I have two concerns here one is the concern that people will misuse powerful AI systems people misusing technology is nothing new um but but one thing that I think is new about AI is that it its ability to put all the pieces together is much greater than any previous technology I think in general we've always been protected by the fact that you know if you take a VIN diagram of people who want to do really bad things and and you know people who have strong Technical and operational skills generally overlap has been pretty small if you're a person who has a PhD or is capable of running to larger organizations you have better things to do than come up with evil plans to you know murder people or destroy Society right it's just you know not not very many people are motivated in that direction and then you know the people the people who are you know often they're they're just I mean not not all of them but in many cases not that bright or not that skilled um the problem is you know now could we take unskilled person plus skilled AI plus bad motives and so you know I testified in Congress about this about the risk of bioweapons I think cyber is another area um bunch of stuff around National Security and you know the relationships between between nations and and stability so that's one corner of and then I think the other corner of it is what the AI systems themselves may do um and you know there's lots you can you know kind of find the internet and in various communities on this um but but I often put it in a simple way just one the systems are getting much more powerful two we there there's obviously not much of a barrier to getting the systems to act autonomously in the world right people have taken GPT 4 for instance and turned it into Auto GPT there was even a worm GPT which was you know supposed to act as a computer worm so you know powerful Smart Systems that can take action so you know kind of Very Long Leash of of human supervision and because of the way they're Tred they're not easy to control I mean we all saw Bing and Sydney so you put those three things together and you know there's at least you know I think some chance that as the systems get more and more powerful you know they're they're going to do things that we don't want them to do and it may be it may be difficult to fully control them to fully Reign them in I think that's further out than the misuse but it's you know it's something we should think about we've touched on a few of the different things you guys have done from a safety so I want to talk through the I guess the three ones I took down so you you long-term benefit trust and public benefit Corporation can you explain what that is and how you decided to do that yeah yeah so we were incorporated as a public benefit Corporation from the from the beginning which means um basically public benefit Corporation it's actually very much like a C Corp uh except uh the the investors can't sue the company for fail for failing to maximize uh for failing to maximize profits um you know I think I think in practice in the in the vast in the vast majority of cases it operates like a normal company I mean I think that's one theme I want to get through here like 99% of what we do um you know we would we would make the same you know the same decision that that a normal company would um you know most of the time the you know the the the the the the the the logic of business which is basically the logic of providing Mutual Mutual value um also makes sense from a public benefit Corporation but there's maybe this one one% of key decisions you know I might I might think about the you know the the the the delay of release of Claude um decisions that might relate to hey we have a very powerful model but we need to make really sure that you know this thing can't create a biop plague that'll kill millions of people before we before before we release it so I think there going to be a few key moments in the company where this where this makes a difference um and then ltbt you know as I said a public benefit Corporation is it's not that different from a C Corp um the idea of the ltbt is to have a set of uh um uh so ltbt is long-term benefit trust so right now the governance of anthropic is pretty much like that of a normal Corporation but we have a plan that was written into our original series a documents and has been iterated on since then that will will gradually hand over the ability to appoint a majority of anthropics board seats to a a kind of trust of people and on that trust of people we've we've we selected the original ones but then it becomes it becomes self- sustaining we selected for kind of three types of experience uh one type is experience in AI safety one type is experience in National Security topics as I think this is going to become relevant and another type is you know thinking about things like philanthropy and the macroeconomic distribution of income um so I I think of those as my best guess as to kind of the you know the the the three topics where something that kind of TR you know kind of transcends the the kind of ordinary activity of companies is is going to come up and is that who you ultimately report into when this structure is finalized that'll be the the board that anthropic answers to or is so this set of five people appoints a majority but not all of the corporate Board of the company so there's basically two there's there's two two of these bodies um and and the the ltbt appoints the corporate board um now look that said I mean we all we all kind of you know in in in practice the company is almost always run you know dayto Day by the CEO right like you know it's it's very even even speaking of the you know even speaking of the corporate board not just for anthropic but but any other company I mean you know think of you know think of as a CEO how many decisions you directly make yourself versus how many it's like oh I have to get the board on board with I mean you know there are some when you when you when you you know when you when you when you when you when you when you raise money when you you know issue new employee when you issue new employee shares when you make a major strategic decision um the ltbt is kind of an even more rarified body um and you know i' I I you know I've set the expectation with them that you know their role is to you know get involved in the things that you know that really involve critical decisions for Humanity you know there might only be three or four such decisions in the entire history of,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 011,87,89
22,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,How Anthropic Deals With AI Bias,2697,2969,"might only be three or four such decisions in the entire history of anthropic now constitutional AI can you talk about what that is and what the inputs into it were yes so constitutional AI is a method that we developed around the end of of last year um so easiest to explain it by contrasting it with this previous method called reinforcement learning from Human feedback which I and some other people were the co-inventor of at at at open aai uh in 2017 so reinforcement learning from Human feedback the way it works is okay I've trained giant language model I paid my tens of maybe hundreds of millions of dollars to to train it uh and and now I I want to give it some sense of how to act uh so you know there there are questions you can ask the model that don't have any clear factually correct answer so you know I could say what do you think of this politician or what do you think of this policy or what should i as a human do in this situation uh and you know the the model doesn't doesn't have any definite definite answer to that so the way RL from Human feedback works is you hire a bunch of contractors you give them examples of how the model is behaving and the humans kind of you know they kind of give feedback to the model they say this answer is better than that answer and then and then and then over time the model updates itself to learn to do what whatever is in line with what the human contractors say um one of the problems with this is you know one it's expensive it requires a lot of human labor but but in addition to that it's it's very opaque right you know if I if I serve the model in public and uh you know someone says hey why is this model biased against conservatives or why is this model biased against liberals or why is this model just give me weird sounding advice or why does it you know G why does it give things in a weird style uh I can't really give any answer I can just say well I hired 10,000 contractors I don't know and uh you know this this was the statistical average of what of what the contractors generally generally proposed or the you know the mathematical generalization of it it's not a very satisfying answer so the method we developed is called constitutional Ai and the way that works is you have a set of explicit principles that you give to the model um you know so you know the principles could be something like on on a political question you know present both sides of the issue and don't take a position yourself you know say here are some arguments for here are some typical arguments against opinions differ uh so uh with that you basically just as with RL from Human feedback you you have the model give an you have you have the model give answers but then you have the model critique its own responses uh for whether they're in in line with the model Constitution and so you can run this in Loop basically the model is both the generator and the evaluator with the the Constitution as kind of the pin source of Truth uh and so this allows you to eliminate human contractors and instead go from this set of principles now in practice we find it useful to a augment that method with human contractors so that you can get the best of both worlds but you use less human contractors ctors than you were before you have more of a guiding principle and you know than if than if someone uh you know then then if someone calls me up in Congress and says hey why is your model woke or why is your model anti-woke or why is your model doing this crazy thing you know I can point to the Constitution and I can say hey these are our principles you could have one of two objections maybe you don't agree with our principles you know fine we can have a debate about that or or it's a technical issue these are our principles somehow the train of our model you know wasn't perfect and wasn't in line with those principles and I think separating those two things out is is I think I think very useful and even like Enterprise customers have found this to be a useful thing the kind of customizability and the ability to separate the two out and and the inputs into this were so you use the UN Declaration of Human Rights Apple's terms of service what else went end coming up with the principles around yeah there were there were some uh there were some principles that uh were developed for use by an early uh Deep Mind chatbot um but yeah apple terms of service un Declaration of Human Rights we added some kind of other things like we asked the model to respect copyright um so this is one way to you know to greatly reduce the probability that the model outputs copyrighted text for btim you know we can we can all debate uh you know what the what the status is of you know you know models that we train on corpuses of data but we can all agree that on the output side we don't want the model to Output vast reams of copyrighted text that's a that's a bad thing to do and we we aim not to do that and so we just put in the Constitution not to do that one of the other things",Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 012,88,90
23,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Anthropic's Responsible Scaling Policy,2969,3396,and so we just put in the Constitution not to do that one of the other things that you introduced around safety broadly speaking is responsible scaling policy can you talk about what that is yes um so a responsible scaling policy is this set of commitments that we recently released that is a framework for how to safely make more and more powerful AI systems and confront the greater and greater dangers that we're going to face with those systems um so maybe the easiest way to understand it is you know to think of kind of the two sides of the spectrum um so one extreme side of the spectrum is like build things as fast as possible like you know release things as much as possible um you know maximize technological progress and I you know I understand that position and have sympathy for it in in many other you know in in in many other contexts I just think you know AI is a part you know particularly tricky technology you have to put e slac in your Twitter bio if you believe that I think maybe maybe I should put both yeah yeah and so the the The Other Extreme position which you know I also have some sympathy for despite it being absolutely the opposite position is you know oh my God this stuff is really scary and the most extreme version of it was you know we should just pause we should just we should just stop you know we should just stop building the technology um you know for indefinitely Or for some specified period of time and I think my problem with that has always been well okay let's say we pause for 6 months what do you actually gain from that what do you what what do you do in those six months um particularly with the more powerful models being needed for safety of more powerful models it's kind of like you frozen time you stop the engine what what do you what do you get at the end of it and if you were to pause for an indefinite length of time then you raise these questions like well how do you really get everyone to stop there's an International System here there's dictators who want to use this stuff to take over the world I mean you know people use that as an excuse but it's also true uh and and so you know that extreme position doesn't doesn't make too much sense to me to me either but what does make sense to me is hey let's think about caution in a way that's actually match to the danger right now you know whatever we're worried about in the future right now today's systems they they have a number of problems but I think they're the problems that come with any new technology not these kind of special problems of you know bioweapons that would kill millions of people or you know the models um you know kind of uh you know kind of taking over the world in some form uh so so you know let let's have let's have relatively normal precautions now but L then let's define uh a point at which you know when the model has certain capabilities we should be more careful um so the way we've set this up is we've defined something called AI safety levels so there's something called biosafety levels in the US government which is like you know for a given virus you define how dangerous it is it gets categorizes like bsl1 or bsl3 or bsl4 and that determines the kind of contain measures and procedures you have to take to you know to control that virus so we we think of AI models we think of AI models in the same way there's value in working with these very powerful models but they have dangers to them and so we have these various thresholds between asl1 and asl2 between asl2 and asl3 between asl3 three and asl4 and at each level there's there's certain criteria that that we have to meet um so right now we're at asl2 as we've defined it before we get to asl3 we have to we have to develop security that we think is sufficient to prevent any kind of anyone who's not a super sophisticated State actor from stealing the model that's one thing another thing is we have to make sure that when the models reach a certain level of capability we're really really certain that they're not going to provide a certain class of dangerous information and to figure out what that is you know we're going to work with some of the best biocurity experts in the World some of the best cyber security experts in the world to understand what really would be dangerous compared to what can be done you know today today with a Google search we've defined these tests and these thresholds very carefully and and so how does that relate to the two sides of the spectrum um compared to a pause the ASL thresholds could actually lead us to pause because if we get to a certain capability of model and we don't have the relevant Safety and Security procedures in place then we have to stop developing more powerful models so the idea is there could be a pause but it's a pause that you can get out of by solving the problem it's a pause you can get out of by developing the right safety measures and so it incentivizes you to develop the right safety measures and in fact incentivizes you to avoid ever having to pause in the first place by proactively developing the the the the right set of safety measures um and and as we go up the scale we may actually get to the point where you have to very affirmatively show the safety of the model where you have to say you know yes like you know I'm able to look inside this this model you know with with with an x-ray with with interpretability techniques and say yep I'm sure that this model is not going to engage in this dangerous Behavior because you know there there isn't any circuitry for doing this or there's this reliable suppression circuitry so so it's really a way to shoor in a lot of the safety requirements put them in the critical path of making the model and and and hey if you can be the first one to solve all of these problems and therefore safely safely scale up the model not only will you have solved the safety problems but that kind of aligns the business incentives with with with the safety incentives Our Hope of course is that others will adopt the responsible scaling plan and that eventually it can also be an inspiration for policies so that you know every so that so so that so that everyone is held to some version of the responsible scaling plan and you know how does it relate to to this this other thing of like build as fast as we can well look I mean one way to think about it is like the responsible scaling plan doesn't slow you down except where it's absolutely necessary it only slows you down where it's like there's a CR there's a critical danger in this specific place with this specific type of model therefore you need to slow down it says nothing about you know stopping at some certain amount of compute or stopping for no reason or stopping for a specific amount of time it says keep keep building until you get to certain thresholds if you can solve the problems with those thresholds then keep building after that it's just that as the models get more and more powerful you know safety has to build along with the capabilities of the model and our hope is that if we do that and others do that it creates the right culture internally at anthropic and it creates the right incentives for for um you know the ecosystem and companies other than anthropic you know I'm aware that since we published our responsible scaling plan um several other organizations are internally working on responsible scaling plans you know for all I know one of them one or more of them might be out by by the time this this podcast is out hopefully they put out something hopefully they try and make it better than ours uh that would that would be a win for us you have some aspects of your,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 013,89,91
24,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Testifying in front of Congress,3396,3565,than ours uh that would that would be a win for us you have some aspects of your job and you alluded to this earlier 99% of your job probably looks mostly like a normal company would but your time I would guess is probably not 99% how much your time is spent on stuff that is uh weird for a normal startup CEO testifying in front of Congress or whatever that bucket is versus just day-to-day operations of running a business or is it hard to disentangle yeah I mean it's so I I don't know I I would say it's maybe 75 25 or something like that5 normal 75 normal 25 25% weird I mean it certainly takes a lot of time to you know to talk to large number of customers to you know to hire for various roles to you know look at Financial metrics to inspect the building of the models I mean that eats up a lot of time but you know I also spend a decent amount of time say talking to government officials about what the future is going to look like you know thinking about the National Security implications trying to trying to advise folks on you know what can go wrong we did this whole project of you know working with some of the world expert biocurity experts on you know what would it really take for the model to to help someone to do something very dangerous there are certain missing steps in biow weapon synthesis um for obvious reasons I'm not going to go into what those steps are I don't even I I don't even know all them but you know we spent spent a good number of months and you know decent amount of my of my personal time along with the incredibly hard work of the team the team that worked on it um you know thinking about this and you know presenting it to officials within our government within other Allied governments and you know that's that's just a pretty pretty unusual thing to do you know I mean that that that that you know that that feels something like something more out of a military thriller or something like that um so you know that's that's unusual speaking to Congress is unusual um you know thinking about where where you know like where we're going to be in like you know three or four years like you know you know are the models going to you know like run rampant on the Internet or something like that like spend a good good deal of time thinking about you know how do we how do how do we how do we how do we prepare for that scenario you know another thing is like you know thinking about you know could could at some point you know could could the models at some point be moral significant entities right that's really wacky really strange like I I still don't know how you know how how to be sure of that or how or or or or or or or how you'd measure that it might be you know might be an important thing it might not be an important thing but you know we take it we take it we take it seriously and so there is definitely this weird ju position of like you know I'm like you know looking looking for a chief product officer one day and like you know thinking about bioweapons and you know you know model moral status the next day,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 014,90,92
25,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Will AI destroy humanity?,3565,3600,you know you know model moral status the next day there's a Vox article that said something uh to the effect of uh an employee predicted there was a 20% chance that a rogue AI would destroy Humanity within the next decade to the reporter I guess that was around is I mean does all this stuff weigh heavily on the organization on a daily basis or is it mostly consistent with a normal startup for the average employee yeah so I don't know I'll give my own experience and it's kind of the same thing that I you know that I recommend that I recommend to others uh so you know really freaked out about this stuff in 2018 or 2019 or so when you know when I,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 015,91,93
26,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,GPT3 vs GPT4,3600,3746,really freaked out about this stuff in 2018 or 2019 or so when you know when I first believed that you know turned out to be you at least in some ways correct that the models would would scale scale very rapidly and you know they would have this this this import to the world was there a specific thing that made you realize that or that you saw or was it a bunch of things with gpt2 is what is what did it for me it's what made it real I mean gpt3 was more so and you know Claude and gp4 are you know of course even more impressive but like the moment where I I really kind of believed the scaling trends that that that we had been seeing that you know was really real and would lead to real things was like the first time I looked at gbt to I was like oh my this is like this is crazy this is you know there's there's nothing like there's nothing like this in the world like it's crazy that this is possible and was it in particular the jump between the prior version to that and seeing that Delta yeah the the it was the Delta and it was just the things that it was capable of like it felt like a general it felt like a general induction or general reasoning engine for years after that people said models couldn't do reasoning I looked at gpt2 and I'm like yeah you scale this thing up it it's really going to be able to see see any pattern and reason about reason about anything I mean again we still don't know that for sure this is still unsure there are still people who say it can't and for all I know they're right but uh that was kind of the moment that that I that that I saw it and so you know I had a very very difficult year or so where you know I tried to you know come where I tried to come to terms with what I believe to be the significance of it now 5 years later I'm much more in a position where it's like this is this is the job we got to do right where you know you signed up to do this you have to be a professional and you have to address risk in a sensible way and you know I found it useful to uh you know to think about the strategies used by people who professionally address risk or deal with dangerous situations right people who are on you know military strike teams people in the intelligence Community people who kind of you know deal with you know High you know like um high stakes uh you know critical decisions for you know for National Security or disaster relief or something like that I mean you know doctors surgeons you talk to all these people and like you know they have they have you know they have techniques for thinking of these decisions rationally and you know making sure that they don't get caught up in them too much um and so I you know I I I I I kind of try to adopt those techniques and I've told other people in the org to think to think in that way as well you had made a,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 016,92,94
27,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,The memification of a CEO,3746,4170,"other people in the org to think to think in that way as well you had made a comment that you don't like the concept of personalizing uh companies in this whole hey the memeification of a CEO in some regard is that is that just a personal tax to who you are do you think it's actually like a societal issue that yeah I mean it's definitely my personal style but you know I think this is closely connected to you know the thing about thing about Twitter like you know I think people should think about companies and the incentives they have and the actual substance of the decisions they make nine times out of 10 you know if someone seems kind of you know I don't know Charming or relatable or you know you know you talk to them on Twitter and it seems like they're someone who you know you could sit down with them and really like you know that that could just be very misleading right it's you know it's not it's not necessarily a bad sign but I think it's pretty uncorrelated to like what what is the actual effect that the company's having in the world um and I think people should focus on that and kind of we've tried to focus on that in terms of the structural elements right the you know I'm the only one who is ultimately responsible for these decisions the ltbt is kind of designed as this as this check as the supervisory body so you know everyone doesn't have to look what's Dario going to do in this situation and then anthropic is only one company within a space of many other companies there are also government actors there and and this is this is the way it should be no one person no one entity should you know should have too much say over any of this um I think I think that's always unhealthy we've talked about a lot lot of the uh negative sides or implications that come around with running anthropic I'm sure there's some positive ones other than being in the eye of the nucleus or storm or all the stuff that's going on today do you have any uh weird data points on like number of applicant or like the inbounds you get or all that yeah yeah I mean we can talk about you know amazing positive stuff in the short term and I'm also excited about positive stuff in the long term so maybe maybe let's take those one by one and you know I think you know if I you know I think we should talk more about the positive stuff I mean I often see it as my duty to make sure people are aware of the concerns in a responsible and sober way but that doesn't mean I'm not excited about about all the potential I am so speaking about you know anthropic in particular I mean you know millions of people have S have signed up to use Claude um uh thousands of Enterprises are are thousands of Enterprises are using it and you know smaller smaller number of very large uh smaller number of of of of very large players have started to adopt it so you know I've just I've just been excited by some of the use cases like you know when we look at you know particularly Legal Financial um things like accounting uh when you know when you see suddenly people are able to talk to documents right you can just uh you know you can just upload a company's financial documents you can just upload a legal contract and ask questions that you know you would have needed a human to spend many hours on uh and so you know this this is just in a very practical way this is just like saving people's time and providing them with services that that they just it would be very difficult for them to have otherwise so I don't know it's it's hard not to be excited by that and of course excited by you know all the all the amazing things the technology can do I mean you know I know of someone who has like you know used it to like translate math papers from Russian and you know they were good enough that you know it all it all made sense and they they were able to to kind of uh you know they were able to understand something that it would have been very difficult for them to understand it uh before um in in the long run I mean I'm even more excited I mean I've talked about this a little before but you know having been in biology and Neuroscience I'm very convinced that the limiting factor there was that the basic systems were getting too complicated for humans to make sense of um if you look at the history of science things like physics there's very simple principles in the world you know we we managed to to to solve those because I mean physics not fully solved but you know many many parts of of the basic operation of our world we understand and then within biology things like you know viral disease or bacterial disease it's very simple you know there's there's something invading your body um you know you need to find some way to like Kill The Invader without hurting yourself and because you and The Invader are pretty different biologically it's not that hard so we've solved the problem what's left is things like cancer Alzheimer's disease the aging process itself um you know to some extent things like heart disease and you know I I worked on you know I worked on some of those things in in you know my in my my career as a biological scientist and just just the complexity of it right it's like you know you're trying to understand you know how how proteins you know build cells and how the cells get disregulated it's like you know there's like 30,000 different proteins and each one of them has like you know 20 different post transational modifications and each of those interacts with the other proteins in this like really complicated web you know that that you know makes one cell run that's just one type of cell and there's like hundreds of other type of cells uh and and so one of the things we've already seen with the language models is that they know more than than you or I do right a language model you know they know about the history of Samurai and Japan at the same time as they know about the history of cricket in India you know at the same time as you know they can they can tell you something thing about you know uh you know like you know the biology of the liver or something like that like you list enough of these topics and there's no one on earth who knows who has who has that breath even to the level that even to the level that a language model does even with all the things that it says wrong right now and and so my hope is that in terms of biology right now we have this network of like you know thousands of experts who all have to work together if you can have one language model that can connect all the pieces and you know not just kind of like big data will help biology that's not my thesis here my thesis is that they'll be able to do and work along with the humans a lot of things that human biologists human medicinal chemists do and and really track the complexity and and be a match for the complexity of you know these these disease processes that are happening that are happening in our body and so I'm hopeful that we'll have you know another kind of Renaissance of medicine you know like we had in the late 19th century or early 20th century when you know all these diseases we didn't know how to cure we like oh we discovered penic pen penicillin we discovered vaccines so you know I'll take cancer as one like like it you know any any biologist or medicinal chemist you know who I said could we cure cancer in five years they'd be like that's that's  insane there's so many different types of cancers these breakthrough you know we have all these breakthroughs that handle one really narrow type of cancer I think if we get this AI stuff right then we maybe we could really do that so I I I know it's hard to be hard to be more inspiring than that yeah totally you said you've been right about",Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 017,93,95
28,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,What are you most surprised by with AI?,4170,4623,to be more inspiring than that yeah totally you said you've been right about a lot of things related to AI but you've also been wrong and surprised by a bunch what what have you been most wrong about or surprised by yeah I don't know so I I mean I've been wrong about I've been wrong about a bunch of stuff like I think how this prediction stuff works is like if if you're thinking about the right things if you're predicting the right things you know you only have to be right about like 20 20% of stuff for you know for it to have these huge huge consequences right if you if you if you predict five things that like no one in the world thinks is going to happen and would have enormous consequences and and you're right about one of them I mean it's it's a little bit like VC right you know it's like you know if you if you you know if in in in 1999 you invested in Google and four four companies that no one heard of um that's a pretty good portfolio end up being wrong about about lots of stuff so like one example of that uh I don't know I could come up with a few examples but but one is uh I thought certainly going back in like 2019 or so when you know I first kind of saw the the scaling situation I thought that we were going to scale for a while with these pure language models and then what what we needed to do was immediately start working on agents acting in the world not necessarily robotics but like you know there had been all this stuff on go uh Starcraft DOTA these other video games all of which used reinforcement learning before or the era of the large language model so I thought we were going to put the two together almost immediately and that almost all the training by by now by 2023 2024 was was going to be uh you know the these these these large language models that were already as big you know already as big as they could usefully be made would would uh would kind of act in the world uh but we found instead is we've just kept scaling the language models and I you know I still think all the RL stuff is going to be promising it's just we haven't gotten to it because it's the lowest hanging fruit because it's simpler to just spend more money to make to make these models bigger than to design design something new it's it's completely economically rational and and they the models just keep just keep getting better and better um which you know I didn't didn't doubt that they would get uh didn't doubt that they would get better but I I I guess I imagine that things would happen in a little bit of a different order do you think data will be a scaling issue in the near term yeah so um I think there's actually some chance um I would say there's a 10% chance that we get blocked by data um the reason I mostly don't think it is you know the the deeper you look on you the internet's a big place and the deeper you look the more high quality data you find and this is without even getting into kind of Licensing of private data this is just you know kind of you know this is this is just publicly available data uh and then there are a bunch of promising approaches which I won't get into detail about for how to make synthetic data uh and uh you know then again I can't get into detail but we thought a lot about this and I I bet the other llm companies have thought a lot about it as well and I would guess that one of those two at least one of those two paths very likely to pan out but it's not it's not a slam dunk um I don't think we've proven yet that this will work at the scale we need it to work this will work for a you know a 10 billion doll model that you know that that needs God knows how many trillion uh God knows how many trillion words fed into it real or synthetic those numbers are so big for people and the amount of money money that uh is being spent to train these models um where for the average person listening like where does all that money go into uh and how how should they think about like the need over time to continue to iterate on this yeah so you know what I'll say is at least to my knowledge no one has trained a model that costs billions of dollars today um people have train models that cost I think a order hundred million um but I think billion doll models will be trained in 2020 24 and my guess is in 2025 2026 several billion dollar maybe even 10 billion dollar models will be will be trained there's you know there's enough compute in the industry and enough ability to do data centers that that's possible and you know I think I think it will happen right if you look at what anthropic has has raised you know has raised so has has raised so far at least it's been publicly disclosed um you know we're at we're at roughly $5.5 billion or so we're not going to spend that all on one mod model um but you know we certainly we certainly are going to spend you know multiple billion dollars on training a model sometime in the next sometime in the next in the next two or three years where does that go it's almost all compute it's almost all gpus or custom chips um uh and uh you know and and the data center and data center that surrounds them 80 to 90% of our cost is capital and almost all our Capital cost is compute um you know the the number of people necessary to train these models the number of Engineers and researchers is growing uh but it's the cost is absolutely dwarfed by uh by by you know is DFT by the cost of compute you know of course we also have to pay for like the buildings people work in but you know that that again is some some tiny fraction of of of what the cost of compute is maybe ending on a uh on an optimistic note here and we touched on a bunch of like the potential medical breakthroughs and things like that but why should people be optimistic about what anthropic doing about the future Ai and everything that's going on yeah I don't know so I'd answer the question in two ways I mean one I'm optimistic about solving the problems I mean I am getting super excited about the interpretability work like people didn't necessarily think this was possible I still don't know whether it's possible to you know to really do a good job interpreting the models but I'm very excited and very pleased by the progress we've made I'm also excited about just you know the wide range of ways we've been able to deploy the model safely like the wide range of of of happy customers who who who who just say you know this this model has been able to solve a problem that we had it it solved it reliably we haven't had you know all we haven't had all these safety problems we've managed to solve them we've deployed something safely in the world it's being used by lots of people that's that's great that's one level of great and I think the second level of great is this this this this this this thing you you alluded to with like you know medical breakthroughs mental health breakthroughs like I think you know energy breakthroughs are already doing pretty well but you know I imagine AI can speed up Material Science very very very very substantially um so you know I think I think a a if if we solve all these problems I think a world of abundance really is a reality I don't think it's utopian given what I've seen that the technology is capable of and you know of course there are people who will look at the flaws of where the technology is right now and say it's not capable of those things and they're right it's not capable of those things to but if the if the scaling laws that I'm talking about really continue to hold then I think I think we're going to see some some really some really radical things um you know one of one of the things you know it's not a not a complete Trend but you know I think as as we as we gain more you know Mastery over ourselves our own our own biology um you know the ability to manipulate the technological world around us uh you know I have some hope that that will also lead to a you know to a a a a Kinder and more moral Society um uh you know I think I think in many ways it it has in the past although not uniformally,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 018,94,96
29,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Why don't you like the term AGI?,4623,4910,know I think I think in many ways it it has in the past although not uniformally why don't you like the term AGI so I like the term AGI 10 years ago um because you know no one was talking about the ability to do general intelligence 10 years ago and so it felt like kind of a useful concept um but but now I actually think ironically because we're much closer to the kinds of things a ey is pointing at it's sort of no longer a useful term you know it's it's it's you know it's a little bit like if you see some object off in the distance on the horizon you can point at it and give it a name but you get close to it you know it turns out it's like a big sphere or something and and you're standing you're standing right under it and so it's no longer that useful to say this sphere right it's you know it's it's basically it's kind of all around you and it's very close and and it actually turns out to to denote things that are quite different from one another um so so one thing I'll say I mean I you know I said this on a previous podcast I said I think in two to three years the LM plus whatever other modalities and tools that we add are going to be at the point where they're as good at human professionals at kind of a wide range of knowledge work tasks including science and engineering um I I definitely that that would be my prediction I'm not I'm not sure but I I think that's going to be the case and you know when people people kind of like commented on that or put that on Twitter they said oh Dario thinks AGI is going to be two to three years away um and and so that that then conjures up these image of you know there's going to be swarms of Nanobots building Dyson spheres around the Sun in in two to three years and like of course this is absurd I I I don't necessarily think that at all um again the the specific thing I said was you know there are going to be these models that are that are able to able to on average match the ability of of human experts in a wide range of things that they can do there's so much between that and you know the super intelligent God if if that latter thing is even is even possible or even a coherent concept which it may be or it may not be you know one thing I've learned on the business side of things is that there's a huge difference between a demo of a model can do something versus this is actually working at scale and can actually economically substitute there's so many little interstitial things that's like oh the model can do 95% of the task it can't do the 5% but it's not useful for US unless you know unless we're able to substitute in AI end to end for the process or it can do a lot of the task but you know there are still some parts that need to be done by humans and it doesn't integrate with the humans well it's not complimentary it's not clear what the right interface is and so there's there's so much space between in theory can do all the things humans can and in practice is actually out there out there in the economy as full co-workers for humans and there's a further thing of like can it get past humans can it in can it outperform the sum total of human say you know scientific or engineering output that's that's like a you know that's that's another point that point could be you know could be like a year away because the model gets is better at making itself smarter and smarter or it could be many years away and then there's this further point of like okay you know can the model like you know like explore the universe and set out a bunch of like you know van noyman probes and you know build Dyson spheres around the Sun and you know calculate the meaning of life is 42 or whatever um you know that's that's like a that's that's like a further point that also raises questions about you know what's practical in an engineering sense in in all of these kind of weird weird things so that's that's like another further point it's possible all of these points are pretty compressed together because there's like a feedback loop but it's possible they're very far away from each other um and and so there's this whole unexplored Space of like you say the word AGI and you're like referring you're smooshing together all of those things um I think some of them are very practical and near term and then I have a a hugely hard time thinking about like does that IM does that lead very quickly to to all the other things or you know does it lead after a few years or are are those other things like not as coherent or meaningful as we think they are I think all I think all of those are possible so it's it's just it's just kind of a mess we're just we're kind of flying very fast in into this this this glob of Concepts and possibilities and and we don't have the language yet to separate them out we just say AGI and I don't know it's it's just kind of a it's it's like a it's like a buzzword for a certain certain Community or certain set of Science Fiction Concepts when when really we kind of it's pointing at something real but it's pointing at like 20 things that are very different from from one another and we we badly need language to actually talk about them what do you think,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 019,95,97
30,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,2024 AI Predictions,4910,5625,"we we badly need language to actually talk about them what do you think happens on the next major training run for llms um so my guess would be you know nothing truly insane happens Say in any training run that that you know happens in 2024 I think all the you know all the you know the stuff the good and bad stuff I've talked about you know to really invent new science the ability to to cure diseases the ability to make to make bio yeah the ability to make bioweapons yeah maybe someday that the Dyson fears the the least impressive those things I think you know will happen you know you know I I would say no sooner than 2025 maybe 2026 um I think we're just going to see in 2024 crisper more commercially applicable versions of the models that exist today like we you know we've seen a few of these generations of jumps I think in 2024 people are certainly going to be surprised like they're going to be surprised at how much better these things have gotten but it's it's not going to quite Bend reality yet if you if if if you know what I mean by that uh I I think we're just going to see things that are crisper more reliable can do longer tasks of course multimodality which we've seen in the last uh you know we've seen the last few weeks from multiple companies is going to play a big part ability to use tools is going to play a big part uh so you know generally these things are going to become a lot more capable they're definitely going to wow people uh but this reality bending stuff i' I'm I'm talking about I don't expect that to happen in 2024 how do you think the analogy of uh versus a brain breaks down for large language models yeah so it's actually interesting this is this is one of the you know being a former neuroscientist this is one of the the the Mysteries I still wonder about so the general impression I have is that the way that the models run and the way they operate um I don't think it's all that different um you know of course the physiology all the details are different but I don't know the basic combination of linearities and nonlinear ities the way they think about language to the extent that we've looked inside these models which we have with interpretability I mean we see things that would be very familiar in you know the brain or a computer architecture you know we have these you know we have these we have these Registries we have variable abstraction we have neurons that fire on different different concepts um again the alternating linearities and and and and nonlinearities and just just interacting with the models they're not that you know they're not that different now what is incredibly different is how the models are trained right the if you compare the size of the model to the size of the human brain in synapses which of course is an imperfect analogy uh but there's something like still maybe a thousand times smaller and yet they see maybe a thousand or 10,000 times more data than the human brain does if you think of you know the number of words that a human hears over their lifetime it's a few hundred million uh if you think of the number of words that a langu anguage model sees you know the the the the the latest ones are in the trillions or maybe even tens of trillions uh and and that's just you know that's like a factor of like 10,000 difference uh so it's it's as if we've kind of you know that that neural architectures have some you know there's lots of variance to them but they have some universality to them um but that somehow we've climbed the same mountain with the brain and with neural Nets in some very very different way According to some very very different path and so you know we get systems that when you when you interact with them are you know I mean there's still a hell of a lot they can't do but I don't see any reason to believe that they're you know fundamentally different or fundamentally alien but what is fundamentally different and what is fundamentally alien is the completely different way in which they're trained you said that alignment and values are not things that will just work at scale and we've talked about constitutional Ai and some of the different um uh viewpoints there but can you extrapolate on that view yeah I mean this is a bit related to the point that I said earlier about uh you know that that there's kind of this fact value distinction right you cram a bunch of facts into the model you train it on you know everything that's pres that's present on the internet and and it kind of leaves this this blank space or this this this undetermined variable um I you know I I I basically just think that like you know it's it's up to us to you know to determine the values the personality uh especially the controllability of these systems there's another sense in which I would say this which is just that you know that that naturally these are statistical systems and they're trained in this very indirect way right even even the Constitution it's like the Constitution is pretty solid but then the actual training process uses a bunch of examples it's kind of opaque and of course the the part where you put in place tens of trillions of words like no no human ever sees that so it's it's still I think very opaque and and and hard to track down and and so you know I think it's I think it's very prone to to failures and you know this is why we focus on interpretability steerability and and reliability we we really want to kind of tame these models make sure that you're able to control them and that they do what humans want them to do uh I I I don't think that comes on its own you know any more than like that comes on its own for for airplanes right the early airplanes like you know probably they wouldn't crash every time you fly them but like you know I wouldn't wouldn't want to you know I wouldn't want to get in the right brother's plane every day um and just bet that every day it would not crash and would not kill me um it's it's just not it's not safe to that standard and I think today's models are basically like that why is uh mechanistic interpretability so so hard to do yeah so you know mechanistic interpretability is this uh you know it's it's an area that we work on which is basically trying to look inside the models and you know and and kind of analyze them like like like an x-ray um and I think the reason it's so hard it's actually the same reason why it's hard to look inside the brain right the the brain wasn't designed to have humans look inside it it was you know it was designed to serve a function right the the the the interface that's accessible to to other humans is you know your your speech not not you know the actual neurons in your brain right they're they're not designed to be read in that way um of course the advantage of reading them in that way is you know it's it's it's you you get something that's much closer to a ground truth not a perfect ground truth but you know if if I really understood how to look in your brain and understood and understood what you were thinking you know it would be much harder for someone to to deceive someone else about about their intentions um or for behaviors that might emerge in some new situation to to not to not to not be evident um so there's a lot of value in it but but yeah there's you know there's there's nothing in both the case of the brain and in the case of the large language models you know they're they're not designed or trained in a way that makes them easy to look at so you know it's a little it's a little bit like you know we're inspecting this alien City that wasn't built to be understood by humans it was built to function as an alien City uh and so we might get lucky we might get Clues we might be able to figure it out but there's kind of no guarantee of success we're we're on our own uh and and so that's what we do we kind of do our best that said I am becoming increasingly optimistic that interpretability uh can be I don't know about fully solved but that it can be an important guide to showing that models are safe and and even that it will have commercial value in you know kind of in the areas of like trust and safety or classification filters or moderation fraud detection I think there's even legal compliance aspects to interpretability so my co-founder Chris Ola um has been working on interpretability run a team at anthropic for the last two and a half years before that when we were at open AI he ran a team that worked on interpretability of vision models for three years before that and for that entire period uh it's been it's been just basic research right there's been no you know commercial or business application you know Chris and I have just kept it kept it going because we we believe that this is something that will pay off from a safety perspective and maybe even from a business perspec perspective and now actually for the first time um you know you you you will you will you will see by the time this podcast comes out uh we were we releasing something that shows that we've really been able to solve something or make good progress toward solving something called the superposition problem um which is which is that if you look inside a neuron it corresponds to many different concepts we found a way to disambiguate those Concepts so that we can see all the individual ual Concepts that are lighting up inside inside a uh inside one of these large llms it's not a solution to any to everything but it's just a really big step forward and for the first time I'm optimistic that you know give us two or three years I don't know for sure but we might actually be able to get somewhere with this and depending on your level of understanding of all this stuff why it would that be important for safety yeah so um I would go back to the X-ray analogy there right um you know if I can really look inside your brain you know if I if if if if if if I can say this is what's happening I mean you know I can ask you questions and like you know you can say things that sound great and you know I I have no idea if you're telling the truth or if it's all just right but if I if I if I look inside your brain and I have the ability to understand what I'm seeing then it then it becomes much it becomes much harder to be misled similarly with language models you know I can test them in all kinds of situations and it'll seem like like they're fine the fear is always oh but you know if I talk to the language model in this way I could get it to do something really bad or if I put it in this situation it could do something really bad on its own that's always the fear right that's the fear we have every time we deploy a model we've had a hundred people test it we've had a thousand people red team it but when it goes out into the world a million people will play with it and and one of them will find something that's that's truly awful uh and you know you know we'll we we'll find oh well if I use this trick for talking to the model you know it's it'll it'll it'll it'll you know it'll finally be able to produce that that that bioweapon or oh if I put it in this place where it has access to infinite resources on the cloud it'll you know just just self-replicate itself infinitely uh and and so interpretability is at least one attempt at a method to address that problem to say okay instead of thinking instead of trying to test the model in every situation that it could be in which is impossible we we can look inside it and try and decompile it and say what would the model do in this situation well we understand that that the algorithms that it's following we understand what goes on in different parts of its brain at least to some extent so we can we can pose the hypothetical and say hey you know what would what would happen in this this whole part of the space What would happen in this whole class of areas and you know if we can do that we have some we have kind of some ability to exclude certain Behavior to say okay we know the model won't do that which which you never have behaviorally you know you know it's just it's just like it's just like humans you know you know I'm like you know what what would you do in a life-threatening situation I don't know what I would do you know in I don't know what I would do in a lifethreatening situation I don't know what you would do in a life-threatening situation it's hard to know until you're actually in the situation but if I knew enough about your brain I might be able to say what's your view on open source",Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 020,96,98
31,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Dario's opinion on open-source models,5625,5883,enough about your brain I might be able to say what's your view on open source models yeah I mean that's uh you know that's a um obviously a complex topic I mean I think as with many things in as with many things in you know in in in AI versus the rest of Technology you know from a normal technological perspective I'm extremely Pro open source like I think you know it's accelerated science it's accelerated Innovation it allows you know errors errors to be fixed faster and development to happen faster and I certainly think you know for the smaller models for the smaller open source models uh this is this is this is true for AI as well and I don't see much danger to smaller models therefore I think open source as it's being practiced by every open source model that's been released up to this point seems perfectly fine to me um My worry is more around the large models um and my worry in particular is that you know these models that are offered via API and I'm you know I'm talking about models the future that really are dangerous not not models in two or three years maybe one year not not today's uh not not not today's models uh if they're offered by API or even if you have fine-tuning Act access to them there's a lot of levers that you have to control the behavior of the model right you can put in your Constitution don't produce bioweapons right and then if the model does it anyway you can you can basically make changes to the model you can say okay we're just retracting that version and serving a new version of our model that patches a particular hole you can monitor users so if a million people are using the model and within that there's these five Bad actors in This terrorist cell you can use your trust and safety team to identify the terrorist cell cut them off and even call law enforcement if you want to do it um so it it it really provides an ability you don't have to get things right in the first time and if something dangerous happens you can you really have the ability to fix it with with models where their weights are released uh you don't you don't have any of that control right the minute you release the the the model all basically all of this control is lost and so that's our concern that that doesn't mean by the way that large open- Source models shouldn't exist but the way we put it in our responsible scaling plan is we say okay when models get to the the level where they're smart enough to create these dangerous capabilities and the next one for us is asl3 we're at asl2 right now uh then models have to be tested for Dangerous Behavior according to the complete attack surface According to which they're going to be released in reality so if you're just releasing an API then you have to test that you know you can't build a bioweapon with with the API if you're releasing model with an API and fine tuning then the people who are testing the model have to have to mock up the test with the fine tuning if the model's being released in practice then the right test to run would be I'm a mock terrorist cell I'm I you know uh I get the weights released to me I can do anything I want with those weights um is there some way to release release the weights of the model so that they can't they can't be abused I I think there might might very well be but I think people who want to release model weight have to confront that problem and have to find a solution to that problem I'll say by the way because uh there's a person on my team who uh who this is this is one of their pet peeves um the word open source I don't think is necessarily appropriate in the case of all of these models I think it is appropriate in the case of like you know small developers and companies where kind of their whole their whole business model is about is you know their whole business model is about open source but when much larger companies have released the weights of these models they generally have not released them under open source Li under under under open source licenses they've generally asked people to pay them when they use them in commercial ways so I would think of this as you know Less open source and more that model weight releases a particular business strategy for these for these uh for these large companies and again I'm not saying that that model you know model weights can't be released um you know I'm saying that the you know the tests for them need to be conasur it with the issues and that we shouldn't automatically say oh open source open source is good some of these are not open source they're they're the business strategies of large companies that that involve releasing model weights Paul Cristiano recently on a,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 021,97,99
32,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Probability of AI Catastrophe,5883,6052,that that involve releasing model weights Paul Cristiano recently on a podcast said he thinks there's a 50% chance I think the way he phrased it was that his the way he he ends up passing away is something to do with AI do you think about percentage chance Doom or yeah I I think it's popular to give these percentage numbers and and you know I mean the truth is that I'm I'm not I'm not sure it's easy to put to put a number to it and if you forced me to it would it would fluctuate all the time um you know I I think I've I think I've often said that you know my my chance that something goes you know really quite catastrophically wrong on the scale of of you know human civilization you know it might be somewhere between 10 and 25% when you put together the risk of something going wrong with the model itself with you know something going wrong with human you know people or organizations or nation states misusing the model or or it kind of inducing conflict among them or or just some way in which kind of society can't can't handle it that that said I mean you know what that means is that there's a 75 to 90% chance uh that this technology is developed and and and and and everything goes fine in fact I think if everything goes fine it'll go not just fine it'll go really really great um again this stuff about curing cancer I think if if we can avoid the downsides then this stuff about you know about curing cancer extending the human lifespan um you know solving problems like like mental illness I mean I this all this all sounds utopian but I don't think it's outside the scope of what the technology can do so you know I I I often try to focus on the 75 to 90% chance where things will go right and I think one of the big motivators for reducing that 10 to 25% chance is you know how how great it'll you know is trying to increase is trying to increase the good part of the pie um and I think the only reason why I spend so much time thinking about the 10 that that 10 to 25% chance is hey it's not going to solve itself you know I think the good stuff you know companies like like ours and like the other companies have to build things but there's a robust economic process that's leading to the good things happening it's great to be part of it it's you know it's it's great to be one of the ones building it and causing it to happen but there there's a certain robustness to it and you know I find I find more meaning I find more you know when this is all over I think you know I personally will feel I've done more to contribute to you know whatever Utopia results um if we focus you know if if I'm able to focus on kind of you know reducing that that risk that it goes badly or it doesn't happen because I think that's not the thing that's going to that's going that you know that's not the thing that's going to happen on its own the market isn't going to provide that do you worry more about,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 022,98,100
33,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Misuse of AI,6052,6284,happen on its own the market isn't going to provide that do you worry more about the misuse uh by people misusing it or the AI themselves or is it just different timelines yeah uh partially different timelines I mean you know if I had to tell you I would I would say the the misuse to me seems more concrete um and I think you know we will happen sooner you know hopefully we'll stop it and it won't happen at all um I you know I think the the AI itself doing something bad is is also a quite significant risk um it's a little off in the future and it's always been a bit more shadowy and vague but that doesn't mean it isn't real um I mean you know you just look at the rate the model is getting better and you look at something like Bing or Sydney it really gives you a taste of like hey these things can really be out of control and you know Psychopathic and the only reason being in Sydney didn't cause any harm is that you know they were out of control and Psychopathic in a very limited way limited both in that you know was was confined to text and limited in that you know it just wasn't that smart you know tried to manipulate the reporter tried to get him to leave his wife but like it wasn't really you know com it wasn't really compelling enough to you know to get a human to fall in love with it but someday maybe a model will be uh and you know maybe it'll be able to act in the world and then you put all those things together and you know you know I think there is there is some risk there I think it's harder to pin down but I'm personally I'm worried about both things and you know I think I think our job because we see such a positive potential here is you know we have to you know we we have to we have to find all the all the possible bad outcomes and like shoot them down we have to get we have to get all of them and then if we get all of them then then then you know then then we can we can live in a really great world hopefully is if you could wave your hands and have everyone follow a single policy would it be the responsible scaling policy would everyone have one of those yeah I mean I think you know I think I think if we make a constraint of like realism right where you know it's like you know I I in fact can't wave my hand and get you know everyone in you know China or Russia or somewhere else to you know stop building these powerful models um you know like there there are just some levels of like world or International coordination that are just are just are just not going to happen because of because of realism so if you stipulate that like some you know you can't you can't you can't just make everyone stop or you can't just make everyone build in a certain way the idea that hey you know for most things people should just be able to build what they want to build but but you know we're we're cordoning off these you know these particular levels of capability these particular points in the in in the development curve where something concerning is happening and say hey mostly do what you want but you've got to take this you there's a small fraction of stuff you've got to take really seriously and you know if if you don't take it really seriously you're the bad guy um you know that's something that I think I can reasonably recommend that everyone's that everyone sign on to that there's some sacrifice to it there's some loss to winning the race but it's it's you know it's only as much sacrifice as is as is as is needed and because it's so targeted I think you can make a strong moral case that hey if you don't do this you're an how do you think about the trade-offs between building in public and having people aware with what you're doing with on the flip side maintaining that Secrets or that that the appropriate things stay within the order yeah yeah I mean you know this is this is one of these uh this is one of these kind of difficult tradeoffs right so um you know definitely an org benefits from you know kind of everyone knowing about everything but on the other hand as we've seen with multiple AI companies um you know secrets secrets leak out um and you know even just from a commercial perspective forget safety like with models built in you know the next year or two let's say a model costs $3,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 023,99,101
34,The Logan Bartlett Show,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI,Looking ahead: Dario's optimistic outlook on AI,6284,6585,models built in you know the next year or two let's say a model costs $3 billion and you have an algorithmic advance that you know means you can build the same model for 1 $1.5 billion right these kind of 2x 2x advances along the scaling curve have occurred in the past and you know may occur in the future and you know companies including ours may may may may may be aware of such advances um so so basically that's like three lines of code that's worth $1.5 billion um you know you don't want a WID set of people and you may not even want everyone within your company to know about them and at anthropic at least people have been very understanding about that people don't people don't want to know these secrets um people are people people are on board with the idea hey you know it's not it's not a marker of status that you know these secrets these secrets you know should be known to the tiny number of people who are actually working on the relevant thing plus you know the CEO and a couple a couple other folks who you know need need to be able to put put the entire picture together right this is this is this is kind of um compartmentalization and and and and need to know basis and of course it has some costs because information doesn't propagate as freely but but again you know just as with the RSP you know let's let's take the 8020 let's take the little the few pieces of information that are really you know that are really essential to protect and and be as free as we can with everything else if you hadn't gone down this AI path would you be doing academic work right now I I think that was my assumption like that that was what I kind of always imagined doing I imagined being a scientist and you know scient work at universities but the really interesting thing about the the you know this this AI boom is that to really be at the Forefront of it uh you know you have to have these huge resources um and you know I think I think the huge resources are are basically you know they're they're basically only available uh at companies uh you know first it was the large companies like like like Google but you know more recently you know startups like ours have been able to raise raise large amounts of money and so I I I was kind of drawn to that direction because it had the ingredients necessary to uh you know to to build the things we wanted to build and study the things that we wanted to study as scientists and you know I would say many of my many of my co-founders feel the same um one thing one of my co-founders who is a physicist what he often you know what he often brings up is and you know it's it's it's kind of more a more um more an academic question because I don't think things are going to go in this direction but you know he said you know in in in in in in my field we bu these you know 10 billion telescopes in space and you know we build these 10 billion particle accelerators why did the field go the way why why why did the field you know kind of kind of kind of go in this direction instead right why didn't all the AI academics get together and you know build a you know build A1 billion doll cluster why did it happen in you know in in startups and in large companies um I don't really know the answer to that um things could have gone the other way but it doesn't doesn't seem like that's the way things have gone and I you know I don't know if it's for the better or the worse I mean we've learned a huge number of things uh you know by by by working with customers and seeing how these things impact the economy so maybe the path things went is the be best path they could have gone um how did I actually don't know how did they get access to Capital in the in the prior or in the alternative way of doing that was it through government grants stuff these large yeah these large telescopes they're often kind of like government consortia or private um you know kind of kind of kind of large scale private philanthropy it's it's it's honestly this huge Patchwork mix I I'm surprised it even happens because if if I think about it happening in this field I I just I just I just can't imagine how it would happen but in these other fields somehow they've made it work um I I don't actually know how but so I don't know that's that's just like a that could have been like just a weird alternate history of our of our uh of our industry that didn't happen and you know I I I very much doubt it's going to happen Al who knows um you know we we went on we went on this path instead and I don't know there's a lot that's exciting and interesting about about this path and you know one way or another this is this is a situation we're in has working with your sister been as fun in saving the world as you had hoped it when when you were when you were kids yeah it's surprisingly similar to to to what I imagined I mean you know if if you were to if you were to look back on the things we were saying to to to you know to to one another as like an adult observing it you would have been like this is crazy this is crazy and you know kids's dream of course but um no I mean uh you know it's it's it's just just amazing that we're able to work on this together very cool Dario thanks for doing this thanks for having [Music] [Applause] [Music] me,Anthropic CEO on Leaving OpenAI and Predictions for Future of AI - 024,100,
35,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,Intro,0,73,welcome to the Logan Bartlett show on this episode what you're going to hear is a conversation I had with nikesh Aurora now nikesh is the CEO of paloalto networks a business most recently valued at about $85 billion before that he served as the COO of soft Bank working with Masa to make investments and before that he served as the chief business Officer of Google Nesh and I talk about a bunch of different things including the benefits of being a generalist in entering a new field as he was not familiar with cyber security or Enterprise when he joined Palo Alto networks as their CEO we also talk about m&a and how paloo networks has used that as a strategy in acquiring 19 different companies and lessons learned from growing paloo networks from an $18 billion company to an $85 billion one over the course of the last 5 and a half years finally we discuss his growing up in India immigrating to the United States his path to becoming the CMO of T-Mobile prior to his Google Journey a really interesting conversation with one of more thoughtful Executives at a high performing public company you'll hear that conversation with nickh now [Music] nikash thanks for doing this my pleasure,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 001,,121
36,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,The Benefits of Being a Generalist,73,140,[Music] nikash thanks for doing this my pleasure so I want to talk a little bit about domain expertise and uh the the knowledge of a given sector before you go into it so you had never done advertising when you started at Google right no you had never done investing or at least professional when you started off Bank actually that's not true I used to be a bide Analyst at partner oh interesting okay I missed that uh you had never done cyber security or Enterprise before joining Palo Network so what do you think the benefits are of people coming in without knowledge in a given domain with fresh eyes that's an interesting conization right we all started life somewhere without any domain expertise my first job out of college was with no domain expertise so I think it's a bit of a fallacy that we don't learn on the job the question is is what skills are needed for the job What proportion do you bring with you and what do you learn in the job now understand then I got to put somebody in a plane and go figure out how to fly a plane but I think we if you're not learning in your job if you're not picking stuff up you're not intellectually curious I think makes it a horrible career how do you go about,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 002,120,122
37,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,Acquiring Domain Expertise,140,245,"intellectually curious I think makes it a horrible career how do you go about acquiring domain expertise like when you started at pal their networks how did you go about learning about cyber security or Enterprise and again Alto was uh is is a phenomenal organization uh when I walked in there's a phenomenally good culture the company was doing well it had sort of you know gone from being a startup to about an1 1820 billion public company and people had gotten there because of all the good stuff that had happened before and when I talked to the board their point of view was that uh we have 5,000 plus people who understand cyber security so we don't need more cyber security expertise we need somebody to sit down with the management team figured out where we need need to go what we need to do next how do we win so you bring some of that Knowledge from whether it's from your investing life some of it from analyzing company some of it from running a large prodct Google uh you bring all of that expertise to bear and okay let's understand the field let's understand the market let's understand the domain and see where the Market's going so you do that now I was privileged I am privileged because near zuk our founder was still around when I started uh so was rajie who was one of the co-founders and Le she was a chief product office they've been there from the beginning so and I sort of get my phone on my drive over and ask a lot of stupid questions to Lee and on the way back i' ask a lot of questions to near and then I'd call again next morning and ask Lee what near said and ask near what Lee said so between them and I got sort of some version of what I need to understand and then you I spent a lot of time staring a lot of presentations a lot of stuff on the internet trying to figure out what it means and you know slowly and steadily and you see it again and again eventually sort of par through it and see what makes sense that's so it does make sense how long did it take you to get up to speed do you think I'd say",Building to $100B: The CEO that Revolutionized Palo Alto Networks - 003,121,123
38,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,The Constant Learning Curve in Cybersecurity,245,307,does make sense how long did it take you to get up to speed do you think I'd say I'm still learning it's been 5 and a half years there's always a new thing you know the fascinating part of cyber security is it is a constantly changing field because it follows technology now you know guess what the conversation now is how do you protect AI how do you make sure that when people are accessing llms that you can't contaminate them how do you make sure that data doesn't get extracted from llm so now we going to have a whole new conversation around how do you secure AI from hackers so like there's always a new technology out there which you have got to figure out how to protect and the best way to protect is is to secure it by Design so you're constantly learning I don't think the learning is ever done as it relates to either technology or cyber security was there a point of domain or understanding that you sort of said okay i' I've I understand up to this point you're not in there writing code and there's some things you're going to rely on near four so was is there a point that said okay that's now I now I've sort of rounded out my my domain here yeah I guess you know domain is a big,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 004,122,124
39,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,Understanding the Cybersecurity Industry,307,559,sort of rounded out my my domain here yeah I guess you know domain is a big word I think you start by understanding industry what are the parameters of industry and if you looked at it six years ago cyber security was about 130 140 billion doll industry Revenue per year and the largest market share was one and a half% I don't need to be a cyber security expert to figure out know what the hell there's a one and a half% market share of the largest player which makes it the most fragmented sub sector of Technology I said so why is that why is it that no company has reached critical mass and gone to 5% 10% 15% and some cases you know look at some of the other big players in consumer you have like Winner Takes all scenarios so why is it that at one end of Technology on the consumer end you have Winner Takes all on the other hand you've got such a fragmented Marketplace that you're fighting for one and a half% market share and you sit there and say that's interesting because a it's the most Innovative industry in the world there's constantly new attack new way to solve the problem B be when somebody figures out what the S the next cool things as they spend their life trying to get hundreds of thousands of people to buy that in the meantime the new technology shows up somebody's cracked the code in the new stuff and they're busy selling the new stuff and the first company never invested Innovation never paid attention to where the puck was going and here in lo and behold he get another flash in the pan one and a half% market share leader so there was so many cyber security companies in the 1 to5 billion market cap a few in the 10 to 15 and possibly two or three in the close to $20 billion range and that was the largest so you said there and say there is something fundamentally wrong in the way these companies are run managed and constructed that none of them ends up being an evergreen cyber security company so you bring all your business knowledge to par how do I crack the code how do you take that how do you flip it then you got to go back into domain and say all right what drives cyber security demand cyber security demand is driven by the evolution of Technology they said say well I don't need to be a cyber security expert to understand what the next evolution of Technologies having spent 10 years AG good Google or try and invest a sop say you know my plastic moment is cloud and Ai and I came to Pal Alto five and a half years ago the first napkin said cloud and Ai and you said well we got to secure the Enterprise secure the cloud secure AI um and then you realize you know over the years you learn things say look it's much easier to build something for the future than go under the past so then say how do we pivot this company and put it in a course where we are the number one in Cloud security and the impacts that are going to happen because of cloud security so in that context and get into domain expert okay what does that mean what does security and the cloud mean so you kind of pars it out into small parts and try and understand what needs to happen then he then say what does the competitive landscape look like they can there's so many companies doing so many things they said say well now you're back to a business decision do you buy or you build yeah I want to get into that in a second but as as that one and a half% versus 130 billion was that something that the board recognized in recruiting you on the way in because I'm curious like what the unique set of circumstances are for the palat Network's board to say at 1820 billion dollar we're going to go hire someone from outside the Cyber industry and bring them in was that their thesis or was that something you figured out on the way in no that was their thesis and you know I still sort of Wonder I've been on about 15 to 17 boards in my life I still wonder you know how that conversation went because I was in the room because most boards are thought to be down the middle look for experience try and not rock the boat and in general boards are risk minimizers they're not Risk Takers like boards are designed because of all the governance we have in place all the fiduciary stuff that kicks in you're actually risk minimizing all the time so it was a pretty unique board at Pao Alto which made the decision to go multiple standard deviations away from the norm and they did have people down the middle they interviewed a bunch of people in the space it was an$ 188 billion company it was a well-run company it had a great culture it was doing well so there were people who wanted to job job they sat there and said well if we hire some of these people we going to end up getting more of the same we need something different now you know like jokes like you know let's find a guy who've never done Enterprise never done cyber security never been a public company CEO sounds like a perfect fit it's worked out okay I it's worked out okay for everyone yes,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 005,123,125
40,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,Nikesh's 3 Rules For M&A,559,1204,"like a perfect fit it's worked out okay I it's worked out okay for everyone yes yeah so so the m&a point uh and I guess the buy versus build so uh how many companies have you now purchased since you've been here uh 17 plus two two are on the way to closure so 19 uh 19 Acquisitions uh in the sector and the m&a strategy was that something I guess as you sort of as the board talked about the one and a half to 130 billion and figuring out the Federation or fragmentation of it did they know that m& was going to need to be a part of it or was that something you realized once you got here well look the board's job as I always say is to hire in fire management it's Management's job to come come back with the strategy so that strategy was built by the leadership team here we sat down and said well how do we go and win in all these new areas that need to be W and then you sit down and say well let's take a list of make a list of our assets and our liabilities you know our assets are 5,000 people company we have $4 billion of cash in the balance sheet we have a good brand we have a bunch of people on the field our liabilities are right 5,000 people a large company we not great at innovating as fast as some of the startups out there and uh we don't have the people who are Scrappy who are going to go make great things happen with limited resource so how do we take that opportunity you find people who do that really well ingest them to Palo Al to yet not make the mistakes that are traditionally made by larger companies when they do m&a and that was kind of the opportunity in both the challenge so we sat down we figured out and said listen there are spaces in Cloud where we could win but we're two three years late to the party we saw there was a whole bunch of startups in that space so we sat down and say well you got to First have a point of view what's going to be important from a customer perspective so he kind of figured out where the product Market fit needed to be and he said down and say well who do we want to be in business with you look at it culturally make sure these people fit up you look at it and say to they have the right product is aligned with their product Vision you go figure that out and then you go buy the best I think very often the biggest mistakes companies make in m& one you somehow end up thinking well guess what it's a simple matter of programming I can take a small team which is less expensive than the best of the market and I'll make it work because I'm so much smarter than everybody else well it doesn't work out like that there is a reason somebody is number one and best and you just don't give enough credit to it I joke like you know it's like you know my Innovation is genius and your Innovation is a mistake or a fluke so it's like no there's a reason those people are successful you buy that the second thing is in our space you know it's important to get the Innovation right we'll do the go to market we have 5,000 sales people we have a sales methodology we have customers so you end up paying people end up paying a lot more for customers which we haven't we' bought 19 companies and we usually look for anybody who's got anywhere from 15 to 50 customers which means there is product Market fit 15 customers are most startups end up getting 15 customers somehow 50 you start to pay attention say wait 50 means that somebody actually made a bet and chose your product over other people once you get to 500 then you end up paying for the extra 450 at a multiple of Revenue which I don't like to do so you find the right company you find at the right price you find the right team but what is unique in our approach is what we do after that first of all we don't use anybody else it's me and our management team negotiates the deal directly because you're negotiating principls who started the company with Blood Sweat and Tears you've got to make sure they understand you think it's as important as they think it is and we don't send our m&a team or corporate de team you don't have Bankers we directly talk to principles and in fact if somebody else gets in the middle we ask them to stay aside so that's kind of rule number one rule number two is in all 19 cases the people we acquire end up becoming the bosses of people who work at Power because they beat our teams so it's not like they come like we have to work for a senior vice president of crypto or blockchain those people have to work for them because they beat us out fair and square with limited resources faster with more agility and they have a better product Vision than we do uh so I'd say 70% of a product team today is run by acquired Founders or their teams which allows you to create that cultural transformation and the only other thing we do is we actually align our product road map and orc structures before we close the deal we sit down the phone and say Here's how it's going to work here's where it's going to fit here's what we're going to build together and we've had one or two cases where we didn't align we didn't to the deals so you start with an early emerging category it seems like and uh then you go identify who the winners are you're you're doing Outreach or or networking your way into those figuring out who is the best cultural alignment and all that one of the interesting things I I guess in convincing a Founder that they would be better a part of Palo Alto than than going alone what is that pitch CU often times from a VC's perspective I'm like no no no much better on your own right well of course because you're you know you moment you find more people who want one of your businesses that means you smell success you believe like if they give it a little more you might make out better so I understand your perspective in fact almost everyone of those 19 cases there were VCS involved the other side I think it's fair to say in a large proportion of those the VCS were saying keep going you guys are doing great so I think part of it look not every founder is a seller we've discovered that um but also you know the mats is against them right like think about it in cyber security there's thousands two three thousand startups out there he bought 19 I'd say the average price paid is between three to $400 million which is a great outcome because you know 19 winners in 19 categories they're possibly 10 other players in each of those categories so there's 190 companies who could have been considered just for what we bought we looked at 600 to get there right so a you got to be really good at what you're doing and usually when we're buying them we're looking at the product vision for the next two years out and believing that what they're going to do is going to make sense so that whole conversation about all these found with these Founders is listen if you truly want to make a difference in the space and you want to build the best outcome for customers out there many of them are inclined they're like I want to chain the space I want to own the outcome in this category he is going to say you've done the hard part the hard part is building the product having a point of view having a vision the harder part is not relying on 500 people to go out and sell your vision and most technically oriented Founders who great product guys struggle at the go to market part because go to market is rinse and repeat and you got to go suck up to a lot of people sell really hard you got to convince them and not everything is logical it's not like oh my product is better than yours hence I will win there are so many other factors that go into it and that's what we're good at well we're good at integrating this stuff we're good at making sure the customer relies on Pao alter we're going around for a long time we have customer support processes in place we have sales people in place so we s them listen you could come in be part of the team now we have demonstrated like we acquired a company Bridge crew right we started 6 months in there 150 customers with us right because we made them part of our bundle we let them we activated their product the founders like oh my God I got 150 customers in 6 months it would taken me 5 years to do this outside of palal so and then these become these people become our ambassadors the founders we' acquired go back and tell their Community tell other people in the space listen if you want to be part of a largest cyber security team I would choose follow out it seems like you're paying a high Revenue multiple but a low absolute valuation for these remind me when I try and buy one of your companies yeah yeah yeah exactly well the incentive alignment by the way if a VC invested in a company at 250 post and you're coming in at 300 or 400 after it but the founder owns 20 to 50% of the business right and the founder could be walking away from 60 to 80 up to $150 $200 million among a few Founders which is very material to any individual actually even better than that so in our case what has happened many of the founders what we do is when we we acquire the company to tell the founders you're locked in for two years you can't extract your money out we even give them an incentive of more shares on top of what they already own to stick it out a third year in most of those cases their Founders literally one walked up to me last week and says I don't like you I'm like what did I do to you he's like well I know this other guy you locked him for three years me you let me sell some I should have stayed in B out of stock so you know it's not just what they walking walking away from they're walk they walked some of them have made Forex their initial money so the two to threee period is an important one uh and then is that the point that the businesses can operate a standalone business units and are less potentially behold in to the founders vision and you can reintegrate them in some way oh look we want to make sure that the founders stay as motivated to execute on the original Vision because we're requiring them at a point where they're two three three and a half years into a product and and you know I think it takes four to seven years to build a great tech product in the world you pick your category you pick your space it's taking four to seven years to build great products whether it's an Uber or a WhatsApp or a YouTube or Google search these things takes four to seven years because you have so many ideas you got to test them in the market you got to keep building keep building until it becomes somewhat a holesome proposition so having the founder around for the first four to five years of a company is very important because they're executing in their product Vision now after that it's you know one or two things will happen either the founder will stay motivated be relax want to be there or they'll miss their roots of going and starting something a fresh because the upside is a lot more at least in their mind because their smell success the first time so then it becomes a discussion with them about what they believe is the right thing for themselves but by that time we've integrated the product the product is part of our portfolio we have people who can sell it it's part of our fabric then it's a lot less risky for us to potentially lose that Scrappy founder than it is when we acquire a company because typically when you're acquiring these companies acquiring not just a product you're actually acquiring a team that works well is cohesive and has a differentiated product Vision in the market how do you think about the brand uh you mentioned Bridge crew uh it sounds like that's survived as a standalone brand right now it's not it's the company we bought no look at the end of the day we're acquiring product categories they have their early names at the end I don't believe in sustaining multiple individual brands you eventually it has to be about a network products we have our Brands trada cortex and Prisma Brands get integrated or products get integrated in those Brands Our Brands now have been around for six years that is way longer lasting than anybody else has had cyber security brands in those spaces and these are all product companies I mean it's like 25 customers 15 customers it's not they didn't they didn't buy those products because a brand they bought the products with their products being really good and if we can translate that really good product product into our portfolio make sure we can reflect the capabilities and the benefits of products I think the brand doesn't",Building to $100B: The CEO that Revolutionized Palo Alto Networks - 006,124,126
41,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,The Role of Marketing and Storytelling in Tech,1204,1497,capabilities and the benefits of products I think the brand doesn't matter on the brand point I heard you say it one point that Silicon Valley spends way too much time overemphasizing product and not thinking about marketing and storytelling can can you expound on that yeah I know one of my many iterations of my life I was Chief marketing officer of T-Mobile and I also spend time with Google Now I think if you look at the two different sort of extremes Google got to be a really successful company went public without spending a dollar on marketing you know the product was the brand the product you know the product trial was what got you hooked you tried to do a search and you found the answer and then you went back to OB search I didn't have to explain search to you the product was so simple it cost you nothing there was no friction you go to Michael Porter they tell you there's Product Promotion you know placement and price well guess what Google there's no price there's no placement there was no promotion it was just product so you're Focus singularly on the product and you have to worry about anything else now that's great if you can get some of that virality and get people to go experience their product otherwise people spend a lot of time and money spending money trying to figure out how to get you to experience the product and the product kind of works in itself now if you take the Other Extreme and say the case that's great but you got to build a great product well there are products which are commoditized but yet you have slightly different Spin and it's hard to understand when you work in the tech universe that there is something called a commoditize product but you take the case of water right Avan it's a brand of water pelino is a brand of water there is no product differentiation I could put five and I did this in a 600 Market people marketing conference in T-Mobile I took them I was 33 years old we had six glasses of water you had to taste and figure out which brand was which they're horrible nobody could tell the difference right on purpose and the point is the entire perception is created in your brain is which one is better and there's whole art to being able to Market there's a whole art storytelling to building a positioning around it like you take the old campaign from from Verizon we says can you hear me now they figured out the problem in Mobile telephony was nobody cared about all the cool people said my network doesn't work everywhere I go they'll spent two years sending one message can you hear me now everybody believes that their network was better because they told the story stuck to it repeated the hell out of it so I think there's a balance you have to strike when you're trying to break into a market in terms of what part of your product is differentiated what product is so good that if you can just get people to experience it they're going to love it or where are you slightly differentiated and you're trying to create a distinct space for yourselves in which case you have to worry about marketing where does Enterprise Fall on the uh that Spectrum there do we overemphasize the product within Enterprise and software or is it we where does that line fall Enterprise is a I think Enterprise is not it's kind of interesting you go back in history most tech products were created for Enterprise first like I remember not having a laptop the laptop was something I got at work I remember not having office productivity tools or anything to do word processing or right anything right you all everything happened for work you had windows for work and stuff like that I think the the the pivot happened when and think in the early 2000s when we all started getting computers and started doing stuff at home and started having connectivity when Google of the world came about and now it's like everything's built for the consumer when mobile phones come about and you all have apps and you said holy  why I don't have apps that work I have like these clunky old software products that work which are not as cool as consumer products I think they've gone through a phase where Enterprise was where things were and Now consumer is where things were and Enterprise the old clunky world and consumer is the cool world you know look at all the apps that are out there all the AI tools that are being built they're buil built for the consumer so Enterprise is I'd say clunkier products if you will they're not as beautiful and as elegant that we've designed them for consumers funnily it's the same people you know you and I on our Mobile phones on our apps and it's you and I at work using clunkier Enterprise Products you to try using some of the you know CRM products or some of the ERM products and Erp products out there they're they're complicated they're not as easy as simple to use and that's why once in a while you'll see an Enterprise company which comes out with a slick UI and a slick product say oh my God that is so cool why didn't we do that so I think Enterprise will go there slowly I think it's not there yet because it's still a lot cumbersome to move those things around and uh I think Enterprise overemphasizes engineering aspects of product development than than the the coolness and slickness and nice you know the prettiness of consumer products but hopefully we'll see that change how how,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 007,125,127
42,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,Leadership and Management in Large Companies,1497,2149,"the prettiness of consumer products but hopefully we'll see that change how how many people is fell out our networks now about 15,000 15,000 people so you're managing and leading a a big group what what have you learned about managing and leading a big group of people that maybe you wished you knew when you started your career you know I heard a podcast one of these things where Eric Schmid's asked the same question what have you learned in your life and I think uh uh his answer which was glip but it's kind of interesting it's like I wish I'd made my better decision faster so yes if I could go back and do things really well it would differently be the things that I did I'm doing well now which I wish i' had done better that so and again I think it's part of the evolution part of The Learning Journey um I think you learn as you manage larger and larger groups of people is that you have to keep things simple you have to communicate a lot more you have to be consistent so it is kind of very interesting if you look at companies over the the long term the company culture becomes very consistent with the leadership and if you look at Facebook if you look at Google if you look at Uber if you look at go out there and look at companies where CEOs and Founders have been there for a long time you you'll notice you know if you work there long enough or if you know them well enough as a board member a lot of the company's values become somewhat consistent with the founder values and founder behaviors so as much as we don't realize it people Model Behavior of leadership it's not just what you say it's what you do so you suddenly realize that you know if there is a gap between what you say and what you do people will sus it out a lot faster than you think and one of the things I learned and I think you know it to me it came to me in spades in the pandemic you know before the pandemic I said i' probably spent less time thinking about what you I talking about but as the pandemic hit you know we brought in a bunch of external people bunch of speakers bunch of people to understand how how the world operates and it's kind of like it hit you in the face saying the first thing I was told by this this behavioral psychologist was saying listen and that the best piece of advice I got I was like people are going to go through tremendous amounts of uncertainty because of the pandemic you have to become the island of certainty and that's just like that's that's that's an interesting thought so we pivoted our Behavior saying let let's create certainty for employees we said nobody's going to get fired literally like like 10 days after Apple said people have to go home and work from home we said you can work from home but nobody's going to get fired and we stuck to it and slowly instead you realize that you know people want simple things and the more comfortable the more secure the more trusting you can make people of yourself and the environment they do the best work I I feel like nobody comes to work to screw up right when someone say oh my God I to go to work and do a shitty job so they say Well then why do some people get perceived as not doing as good a job as the other person if you believe you hired the right set of people you realize a lot of that is in your culture a lot of that is in your communication a lot of that and this how they feel about the company how they feel about you if you can clear out all those things structurally as a company then you can figure it out if it's a competence issue or not but a lot of the problems happen to be there because culturally we're not consistent or our values are not equally deflected and understood by everybody what what have you learned about goal setting and expectation around managing a company especially in the in the public markets since you become the CEO of petwork I go setting to the public markets or neither I guess but I was more thinking internal like uh of of galvanizing the group and marching towards something do do you review on a quarterly basis do you do it annually do you set fiveyear targets or threeyear targets like what's a point in the future to think about like you have to have a longer term view about where you think the business needs to be and you have to have at least some sense of the hard the possible it has ever been done what would it take to get it done what does it take for something to scale you just can't just hope and pray you got to have something you know if you're going to sell $100 million this year you want to sell 500 next year it takes certain number of resources and you got to get ahead of that so you got to have enough resource so so you have to have a plan in your head in terms of what you're going to do but having said that you also have to have agility and nimbleness to make sure that a plan a doesn't work out there's a plan B and a plan C and you can pivot and you can keep looking and saying let's go feed your winners and starve your losers because you know when we launched we came out and said we're going to have three product categories we had no products we didn't have a product in Cloud we didn't have a product in AI we just said hly here's our threee vision to the market and told our people internally so we Set set set of goals for ourselves and in the market but we didn't actually know how we're going to get there our job was to make sure we had some reasonable line of sight as to what the different irons in the fire were to see if some of them worked we get there then you got to go back and translate that to your people and say look these are our big bets but you got to be nimble you got to watch and you got to keep monitoring to see are we headed the right direction and we have to course correct so it's the way I sort of kind of describe it our job is to set the North Star for the teams and the company they have to have some meet around how they were going to get to the Northstar they have to communicate around them and then our job is to give them the resources and monitor progress so you do that you do that across every category you teach your management your lead ership how everybody needs to do that and then you review that on some periodic basis uh but I mean there's no three Monon or six-month business we're reviewing every week like on certain cases like you know the biggest topic is AI so we're sitting and reviewing where we are on a weekly basis because the market changes every week you got to figure out are you was your decision that you made last week still good enough today I want to go to AI in a second but um on the the operating Point um so bringing on a new candidate if you're interviewing for executive to come on to your team um what are you typically looking for in a first meeting with someone I think you know it's fair to say uh a lot of the hiring for many of us who worked at Google got influenced by how Google hired people and you got to start with competence you have to find people who are competent and smart uh you know when I interviewed at Google with Eric I was in Europe u i met larryan Serge there I met Omid cordani who hired me to Google um but then I was First supposed to come and meet Eric and a bunch of people so I was walking around campus with Eric he was my first interview and he turned to me and said well it's really exciting that you're here I love your experience you worked at T-Mobile you worked at putham you worked at Fidelity uh you understand the markets and all the stuff but I do know we're looking for AE of Europe and the primary role is to sell advertising and he never sold advertising before and I said oh yeah it's kind of like the Pao Alta story and I'm like so does this mean I should just like pack my bags and head back over because why waste my next two days over here talking to 11 more people and er turned around to me and said listen the business model of Google and possibly the tech world is going to change multiple times in your career so I'm not insistent on looking for somebody who's done this before for a long period of time I want to make sure that somebody who's intellectually curious and I think capable enough of rolling with the punches and smart enough to figure out what the right thing to do is and that's with me so you the first thing you got to look for is is the person you're looking at capable are they willing to are they able to roll with the punches so they understand how to act in adversity and they're fundamentally smart right so you start there how do you tease that out by the way or is it just an intuition from the conversation or Are there specific things you'll drill down into in the resume and trying to get them to explain it in more and more dep this is where I think experience helps like if you the older you get to spend more time with lots of people and you have a bit of a gut instinct you get to talk to people and of course then make sure they talk to multiple other people and there are domain Specialists who will quiz them on domain specifics like and just like Google over here we don't just accept and you say hey Logan interviewed the guy he was really smart say hey Logan what do you asked him that told you that this person was really smart so then you have to tell us what you ask them say wow that's smart or that's not really smart so actually you will test across multiple attributes from a competence and smartness perspective you obviously will do the airport test which is don't like this person want to hang out this person but the one which you know we add or I add over there is what I call the the passion test and the way the passion test works out is you got to find out is this person sitting across from you have they done so well at something that they had to sacrifice other things and it's kind of like you know many years ago I heard the world knitting Champion I like what does the world knitting Champion have to do with being a Google or Matt Britain who runs Google Europe in was is an Olympic Roar and he said there and say what does the Olympic RAR have to do with running Google Europe well he woke up every morning at 4:00 under every condition went out there sat in the water you know rode for two hours 3 hours in the morning sacrific going out the night before not drinking when he's in college because he was mission driven that's what he wanted to be really good at he understood to be really good at this I have to not be able to do this other stuff so people who can understand how to prioritize how to get stuff done what sacrifices need to be made then to be really good leaders competence and passion are those the two characteristics that you've seen people have the most success early in I think competence passion and ability to relate to people right because eventually as you get into leadership as you get into managing lots of people you have to understand people if you don't understand people you can't get 20 of them to chase you up the mountain as you're trying to win something uh that you know having confidence and having passion doesn't help because you know the corporate structures are set up that we have to motivate and activate a large number of people so I got to motivate activate 15,000 people I can't do any of this alone if I can't activate them I can't motivate them I can be as smart and as passionate as I want but if I can't get 15,000 people to follow me we've got a problem a lot of companies struggle with especially when they're on the growth curve and I imagine you guys have had this in Spades of you've had someone that's been great in getting you to where you are but there's the question of is that the person that's going to take you to where you're going have you figured out a framework of of how to think about that transition and uh at what point to think about upleveling uh that's a tough one right because we all rise to our level of incompetence so Peter Principle applies at some level and you don't know it until somebody's tried it so it's very hard to say I'm so smart I figured out you're not going to go any further try",Building to $100B: The CEO that Revolutionized Palo Alto Networks - 008,126,128
43,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,The Challenge of Scaling and Managing People,2149,2203,"hard to say I'm so smart I figured out you're not going to go any further try having their conversation say you've been really successful but I have made a decision that you can't make it past this point because I have this amazing ins side and gut that you're not going to make it that's not something that works normally people do really well up to a point I think what you got to watch out for in that I think you know what are the two reasons two things that could change one the breadth changes so you got to do more um if you can manage a th people you could possibly manage 2,000 people but if you have to go from managing one product to three products then the question is can you can you manage the expansion of scope right and scale it goes back to the conversation where we started right at the beginning it's like you know do you believe that that person has enough skills that are required in the next role that they're going to bring with them and are they Nimble enough and intellectually curious enough to learn the other half of it",Building to $100B: The CEO that Revolutionized Palo Alto Networks - 009,127,129
44,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,Transitioning Roles and the Importance of Skill Sets,2203,2281,Nimble enough and intellectually curious enough to learn the other half of it I've seen so many scenarios where for example people get tired of being head of sales they're like I'm done I don't want to be a head of sales anymore I want do something different boss says you've done such a good job selling why didn't you become head of strategy well different skill sets right sales is a phenomenal execution job is a people oriented job so you got to watch out you got to be careful about just putting people from one box to the other because I find like it's my job when somebody takes a role to make sure that I understand how they're going to be successful it's my job to work with them to make make them successful where they are right because the more people I can make successful to work for me the more successful I will be so if I don't believe that that person's going to be successful in the role be very careful in giving them that role now there's lots of scalability and scope conversations that happen in corporate life with many many many people and I've had people who self- selected out they've been self-aware saying no that's not me that's not what I like to do that's not what I can do other people pound on the table say I'd like to do that give me a shot at it in which case then you have to decide if you want to get them a shot at it so I honestly think there's no I don't think there's a sort of magic wand or silver bullet on that topic I think this is something that gets figured out individual by individual basis but there are some obvious pitfalls you can avoid you I've,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 010,128,130
45,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,The Art of Making Successful Career Choices,2281,2316,individual basis but there are some obvious pitfalls you can avoid you I've heard you say you never had a career path or like a plan of what you were going to do don't have one yet yes well it's worked it's worked out how did you think about uh at what point to pop your head up and look around uh for different opportunities then for the most part you know it depends on how much you're enjoying what you're doing and you see your yourself doing that for a long period of time and you believe you know you sort of reached a point where you stopped learning you're just doing and different times different things and I,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 011,129,131
46,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,The Journey from Analyst to Tech Entrepreneur,2316,2624,stopped learning you're just doing and different times different things and I was a buy side analyst for two years Market was at like the 1999 the last internet bubble and everything just kept going up to the right I can't make sense if this doesn't make any sense and it too passive I need to get involved and do something I said oh I need want to go do something so talking to a few CEOs and one of them say come work for me and lo beh I said what am I going to do said I don't know some strategy some m&a some Acquisitions so I went off to T-Mobile you know I got enrolled a team that acquired voice stream at that point in time which is now T-Mobile USA so like I moved my life from Boston to bond in hindsight holy just moved from you know nice Boston to Germany and then I like oh I can't do this forever I was like I'm not enjoying flying to Germany five days a week I was in London so I'm going to leave and go do something my own and then as I start to do something my own my friend said and it's a small company in Silicon Valley trying to hire a head of sales not too small for me you should go talk to these guys that was Google I stick stuck around there for 10 years right and then Masa came about and said hey you want to come do something different so it's kind of serendipitous to some degree but it's not um impulsive not having a career plan doesn't mean I was impulsive it meant that I sat and thought through what does it take to succeed there are many times where people walked up and said come try this and I said no I don't know what to do with it and there are some very big CEO jobs in the world which walked up and said do you want to do I'm like I don't know I don't know how to make it successful I don't think it's gonna work so I decided not to take them so is there career advice in that for people uh uh or is that just a um bull by Randomness it's worked out for a cash versus no look the career advice that there are aspects of career advice like look just make sure what you take on as a rule is something you can see yourself doing for a long period of time right which is an interesting one by the way cuz people will take jobs and then be like well if I do this then I can go do this well that's the worst thing you know we'll sus you out we'll sus you out very quickly I've had people all come saying well I'm here because this was the only open opportunity in fact we're hiring somebody and that person applied for a certain job in our team and after two hours of talking to her I said listen I really like you but talking to you gives me a sense that this is not what you're real passionate passionate about it's like yeah but this is the job that he advertised for I'm like yeah but once you get it do you see yourself being successful 5 years in it and I don't see it how do you see it's like yeah but you I'm hoping when I show myself in the first year or two I can do something different and she was so good that I sat down with our team and said you know what let's carve out this new role for her she's going to do that so that's what she does this new role because I can see her being successful I can see us being successful with her in that role so I think part of it is being truthful to yourselves I always joke like getting a job is a lot easier than keeping it because look if you're smart if you're affable if you have a good track record and you can't fool four people for four hours and convince them to hire you they're probably not good at anything so getting a job is easier the question is can you do that on a consistent basis for five years and prove that you're really good and deliver outcomes and you got to figure out and say well what does it take to deliver outcomes do I understand what it takes to deliver in this certain environment and a lot of us make a mistake because I've seen so many people say ah I know my friend took the job he didn't work out for him but I'm so much better I I'm I'm just different like you're not different look at the circumstances the circumstances have not changed so do you see yourself in that role and being successful I think we have to be very you have to be truthful to yourself because I'll tell you the hardest thing to do is pick yourself up if you go take a job it doesn't work out you're there for an year or two and he says oh my God I got to get out of here and then you start making wrong choices so I think being truthful to your yourself understanding what you're going to enjoy what your skills are what do you bring to the table can you make something really good out of it because I don't want to sound like it a cliche but people who walk into roles and do a really good job end up getting more responsibility so you got to figure out how can I do a really good job there uh that's way more important than saying I'm just taking this because this is the next step and I'm just looking forward to step after this yeah it's interesting I I found that pushing on the decision point of people switching from one job to the next shows an element of how thoughtful they are because it's the single decision you can make professionally that has the most consequence and if you're not going to take your own job super seriously in terms of your movement what are you going to do when I task you with something that's much lower on the diligence priority side of things yeah yeah I think sometimes people get emotional yes uh and know I remember the generations before us possibly my parents or maybe yours or your grandparents they just had one job life they never actually change careers every two to 5 years now we have people who want to move every 3 to 5 years so it's a different world but just got to make sure that you you are able to look back and say I made good decisions because if you look back and you made two missteps then it it's not going to be fun yeah um,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 012,130,132
47,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,The Impact of Artificial Intelligence in Business,2624,3043,you look back and you made two missteps then it it's not going to be fun yeah um artificial intelligence so what are you paying most attention to right now with an AI well you know I told you 5 and a half years ago when I came here I said cloud in Ai and I said AI because I Google I Google they were all about AI even then so I figured it was going to have a significant impact in our life and I think uh you know I think Chad GPD became the iPhone moment I think Jenson said that and I said that I was I was on my way to India on a plane to go speak at my alma matter at the graduation and I read about it I tried at Dubai airport and I sort of said oh my God this is going to be big and I think what what I did then was uh I decided to make myself the unannounced Chief AI officer of P networks I said well if I could learn cyber security I can learn enough AI to be dangerous so I set about trying to learn but we did it slightly differently we took 200 of our leaders and down into one of those training rooms and we had all kinds of people to come speak about it from Nvidia Google IBM Microsoft Amazon or five startups Etc and we've been doing that now for the last seven months where we talk about different things bring different people and we so I think we've we've learned a lot is it a weekly thing it's we bring 150 people about a together about a month once every month but I do five to six hours of AI reviews every week and interestingly at least in our parliament in our mind there are two parts of it we try and separate it one set we call Precision AI the other set we call generative Ai and precision AI I think the best example I've given is like you know I don't want my Tesla to be on generative AI I want it to be precise it needs to know where the next turn is it needs to turn the indicator on or off yeah you don't want it hallucinating that's right oh sorry that I just thought that was a treat no that's not a good idea so I think that's kind of in cyber security you can't afford to hallucinate you can't afford to be wrong you want to make sure there are no false positives right so you go ahead and design machine learning algorithms neural networks get get Precision in play we've been doing machine learning as as possibly every company that has been doing it for the last 10 years but obviously the impetus and the focus has become higher given what's going on in the market um and there I think the biggest problem we all have in the world is contaminated data we need better data to get that right and we're all working on it I think it's fair to say that any company wors at salt should be working really hard on getting their data right everywhere from like you know anything you do in Customer Support everywhere in the company you got to get the data right to get build tag it label it figured it out right that's kind of on the Precision aite on the generative aiite I think you know we've seen what it's been able to do in the early days on the creative side I can make pictures I can make movies I can make all kinds of stuff I can summarize documents I usually say generative AI works really well when there are many possible answers and it's a matter of opinion and choice what is the right answer you can like a you know Bluebird and a white background a different one I can like a different Bluebird and white background doesn't make yours wrong or mine right it's just different so when you have multiple options that could be right I think it's a phenomenal use case for generative AI I can choose which one I like and you know every you know model out there whether it's Bard or it's Chad open a or Chad GPD they all figured out to give me two three options I'll pick one I can keep working on it I can make it better I can make it better so I think that's going to be a phenomenal use case on the creative side and you'll see all that change the world of publishing the world of content creation uh there is obviously a phenomenal summarization use case but the one which intrigues me the most is the conversational use case um and if you if you sort of abstract it at two levels uh society and Technology Society we all learn languages right to express ourselves and they all spend time trying to understand somebody else's language because we're not able to communicate and Google translate and all these others do the S version of it and you know maybe llms will do that for us in the future but if you look at technology what is the role of product development product development spends a lot of time building UI against large engineering data sets and my favorite example is you know which we can all understand is travel right we all have learned how to fill out that form with 14 variables to figure out how to get buy a airline ticket from point A to point B and we can all imagine a world where we can type into some sort of natural language interface or even talk to it and say book me a ticket to New York and all those forms go away well some poor product managers spent a lot of time trying to put out that form say how do users react where do they click and then the whole process of redoing the form so but if you expand that notion to so many different products that have been created whether it's Enterprise or consumer why couldn't I interact in the future with my product with the natural language interface 50% of it because I'm pretty sure as in every use case there are 50% of 50% of UI use less than 10% of time but we spend as much time building that UI as we spend the other 50% purely so you could eliminate 50% of UI generation with natural language and we never miss it right I I my phone is so complicated I when I got my new Google pixel I had to go like literally go Google how do I shut off the phone and it popped up the button right because I couldn't figure out where in the UI was hidden but imagine if 50% of the Obscure longtail use cases in every product could end up not being created in the future they just be natural language interactions how does that manifest itself most within cyber security or what are the implications of that for p networks like cyber secur is a complicated topic right and it's kind of I don't care about something until I care right oh I'm being breached guess what most people get away hopefully without being breached one day you get breached and say oh my God I've seen error 4562 and there's possibly some document manual which tells you what error 4562 is and it's on screen seven Tab four well I've never had to look for it because never happened before guess what you could go and ask using a natural language interface what is er 4562 how do I fix it I have to go look into a manual it's kind of the search use case and if I'm smart enough I say wait eror 4562 is actually you know here let me pull up the UI where it's relevant or let me not even have a UI for and tell you here's what it means and here's how you fix it so there are very many longtail examples in cyber security where I think the world is inefficient today because we spent a lot of time designing worship I stick that in the UI or should I make it easier to represent to the end user if I could take all of that complexity out by using some sort of translation layer between generative Ai and a bunch of API calls or a bunch of automation playbooks and much better knowledge based articles I could possibly build a much better set of,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 013,131,133
48,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,The Importance of Product Obsession in Tech Leadership,3043,3172,knowledge based articles I could possibly build a much better set of products it sounds like you're kind of leading from the front so to speak with regard to to Ai and kind of being on the front lines of of doing this yourself do you think that's important for all CEOs and where does the applicability exist with Google back in the internet days for for from your experience I was prey to a conversation which Larry had with Steve Jobs and and Steve was a sort of singularly focused great iPhone product kind of a guy and Larry was you prided over Google which had so many different irons of fire and so many different products going on and their different philosophies like you know Steve was Steve told Larry Larry stopped doing so many things trying and do a few things well and Larry said well if I do a lot of things a lot of them will work some of them won't work so there are different philosophies but there was one consistency and and you unfortunately in my career lar made sure I knew about it he says listen you know no tech company became great because of a great business guy they became great because of great product people so I think product obsession is extremely important especially in Tech when you're in a leadership role and even an Enterprise it goes through Cycles you know the first set of CEOs are product focused and and they go ahead and build a great product then the sales people go out and sell the hell out of them but then you hit an inflection point in the industry and in the meantime they've taken the product guy and replace that with the sales leader and Enterprise because that's what matters for a while but then he hit an inflection point say oh my God I need the product guys back right and you look at the history of tech if you look at Enterprise companies a lot of Transitions and resuscitation or Resurgence has been associated with the product guy coming back as a leader if you look at consumer primarily the leaders are product oriented CEOs so so to be a good Tech CEO you've got to be domain aware domain Savvy you've got to have business jobs to make sure how do you take that domain savviness and couple that with go to market capability so I think it's important it's very important to lead from the front understand the technology understand where it's going understand the domain and have a point of view about it you may not have to code we kind of dove in a bunch of the different lessons but uh if if we go all the way back so you grew,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 014,132,134
49,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,The Journey from India to the U.S. and the Pursuit of Education,3172,3664,"a bunch of the different lessons but uh if if we go all the way back so you grew up in India and then move to the United States at22 $200 two suitcases I think one was filled with pots and pans or something yes well you needed Essentials right yeah you need so one clothes and one pots and pans yeah can you can you take me through childhood up to that point in time yeah look my father was in the Indian Air Force and uh he was a lawyer and an accountant my mother was a master's in math and Sanskrit so I had both sides in my family and U had a great upbringing they were sort of lower middle class Indian family and my dad was a great provider uh you know instilled all the values that are important in in growing up education is an expensive in India I was able to go to wonderful school I went to IIT bhu and uh in early days I ended up getting some sort of national scholarship so they paid whatever else that I had to pay so was great life um graduated from engineering school didn't really want to turn on and off power plants which is what electrical engineers did in 1989 had a few course in computer science so decided um I want to go to business school it's hard getting into business school in India very competitive so it was much easier to get to Northeastern so I applied to a few schools here and uh you know you got $100 from the Indian government you could take to apply to business school so you had to choose universities had no application fees so you found a bunch of in those days they call them zero dollar universities not because of because of the fact that they required zero dollars to apply uh and then you hope that one of them would pay your tuition and nor Eastern chose to pay my tuition I had to come teach computer science I had to spend a summer back in India brushing up my computer science skills but I came here I taught computer science uh 40 hours a week and studied 20 hours a week and worked another few here and there to make sure I had enough to pay my bu how did you how' you stumble into computer science as something that was interesting obviously it's benefited you well in 1989 uh it's going far back so dating myself but you couldn't I think 1989 was the first computer science batch in India in engineering school I mean remember 85 used to have these things called PCS with no storage you had no hard drives that time so it old days so so at that point in time computer science was an upand cominging field cool emerging yes so you took a few courses you know I'm sure you want to put all your eggs in that basket uh so you learned enough of that and you came here and you you're one of a few people who knew it so you graduated from Northeastern and then what took you through the the t-bo we touched on putam and all that yeah I graduated from nor Eastern I Tred to apply for jobs it was 1992 one of the last recessions the United States I wrote 450 letters triple for jobs went to the alumni book and wrote to everyone sometimes nine people of the same company uh they're very good they sent me really phenomenal form letters back saying sorry not interested I still have them somewhere and uh ended up getting a job at Fidelity but most of Wall Street said you don't know enough Finance so thanks but no thanks so I figured I need to go learn a finance so I uh decided to get a CFA at night I read through it and took the exams and went back to Boston College got to Masters in finance and I was teaching a CFA level three class to a bunch of portfolio managers who looked at me and said oh you work at Fidelity you must know all these cool people who work on the money management side oh no I don't work on money manag typ I do financial analysis for corporate side and two days later I was interviewing at putam to go join the buy side and then I end up being a buy side analyst for two two and a half years covering Tech and Telecom we touched on uh what what that world was like at the time unable to make sense of well those is amazing times I think AOL went public and it was worth hundreds of billions of dollars and then Daron Leos there used to be a company for search there like these explosion of valuation which happened in 1998 to 99 and and and becoming the CMO of T-Mobile nothing in that background you just listed sounds uh marketing oriented how did you end up in in I was also had a product over there by the way so I I ended up going to Germany working on a bit of strategy and in 19 oh sorry in 2000 I started a company to do mobile data apps so I thought apps were going to be big uh unfor wrong timing might have been off might have been very off yes I think the only phones that were there was something called a WAP phone where you had to write 160 characters per screen and if you won 61 characters phone would crash so we spent a lot of time building an app company which used to show you the news you could look at the news in your mobile phone imagine in 2000 uh we built that and then it was a subsidiary of Doja Telecom so then they decided to merge it back into the parent company called T-Mobile because they became an international company and so so we'll have you build that product for everybody so end up becoming head of product and I said well product and marketing should go together so I end up becoming head of product and marketing when I was 34 and then the the the Google Journey we touched on a little bit your conversations with with Eric and Larry in the early days but you ultimately ended up being the chief business officer but you were also the first outside hire VP there um what from that experience and there's obviously a lot of talented people that you that were at that level we've had Clare Hughes Johnson on before among many others I guess that that were your peers there what have you most taken from the Google experience uh that you still use today like Google was a phenomenal part of my life I spent 10 years there I was lucky enough to be hard as uh one of the first people that went outside for because they were trying to hire somebody to run Europe and uh you know I still remember uh those conversations and you know Europe was very smart was 20 some percent of Google at that point in time Google's revenue 80% of it came from elsewhere and part of the aspiration was to scale Europe to its own sort of large business and find somebody who could manage that and I was lucky enough to to end up getting that role um it was all about scaling the product worked beautifully right like we had business in Austria where people were searching and using their credit cards US dollar denominated credit cards to try and pay for advertising so the product Market fit was phenomenal one's job was execution execution execution to scale and we ended up going from I think 800 employees to 5,000 employees in five years in Europe opening 26 offices doesn't experience of a lifetime think it's kind of like the product Juggernaut was so strong the economic model was so strong and you had so much more sort of opportunity there so my job was to make sure that we were scaling putting people in place creating people who were responsible would take accountability and build the business we ended up taking taking Europe to almost as big as the US in the first 5 years I was there and at that point in time my peers were Cheryl Cheryl used to run online Tim Armstrong used to run North America um and then Cheryl went off to Facebook and Tim went off to be CEO of AOL so kind of The Last Man Standing home he said I'm done I want to retire he says do you want to move Eric called me and said do you want to move from Europe to P business officer of uh of Google and as I was moving my friend Jonathan Rosenberg who ran product and said well I also run marketing and it's kind of like haven't done that before you've done it before why don't you come run marketing and sales so I asked lorine to Hill used to work me with me in Google Europe and she came along and she became head of marketing and Philip Schindler used to run uh Google northern Europe he came and became CEO of sales was now currently Chief business Officer Dennis Woodside us run UK he became head of North America so we had a bunch of amazing people who were phenomenal Business Leaders out there in the world and we were just lucky to be have had the opportunity of growing up together working well together so so uh",Building to $100B: The CEO that Revolutionized Palo Alto Networks - 015,133,135
50,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,Venturing into the World of Investment and Venture Capital,3664,3954,have had the opportunity of growing up together working well together so so uh mosa came knocking you went to SoftBank as coo with the path of becoming the CEO um what this was kind of all the pre wework stuff so you made a great investment in cang right yes I did coupang I did UHA Ola s the $500 million valuation red oo at $175 million pre I did snap deal which didn't quite work out but that time was an early investment um what did you find most interesting about the The Venture world and kind of operating on the investing side like it's a great lifestyle choice you can have podcast you can have a podcast hang out with cool people exactly it's different like it's it's kind of the way I just described that an operating role has both brain and Brawn you can avoid the brawn part in an investing role because somebody else is responsible for the execution and that's great because you know you have a different job compared to people who have operating roles I think that's one part of it I don't think it's any easier I think you have to have a point of view about various Industries have a point of view about execution capability people I you know I I I'll tell you a story in a second but it was fascinating it's fascinating because if you're a student of business you get to watch so many different business scenarios and you get to decide which one is more likely to succeed is it is it sort of a industry issue is it a product issue is it an execution issue is it a leaders issue you have to be able to parse all the execution parts to see which one is a risk here which one can you manage and adapt and fix and which one can you not and I think uh if you spend enough time in business and like you like to think about business it's a great opportunity I think uh those are wonderful times you know if you want to be in the investment business you want to hang out with Masa because there are no investment committees it's a phone call I used to talk to them two three hours a day on the phone you could chat with each other and deploy billions of dollars and that's cool it's much harder with you guys have have a governance and committee and pres Lodge presentation and all that stuff now the flip side of that was Mas also go I'm let's do this right so so there's always the you know you get both sides of it but I have to tell you it was it was a phenomenal learning experience was it just the diversity of businesses that you got exposed to and the different Founders and all that what was the The Learning Experience the good news is that you want to you want to drink from an intellectual fire hose that's the place to be right and inasa is a legendary individual you know he's extremely charismatic extremely you know prone to take lots of risk uh interesting so a lot of Founders want to spend time with him you know like it's like of all the places he lives in Tokyo and you can see there's a steady stream of f Founders from every part of the world who make the pilgrimage to go and get Massa interest in their business so there was no dirt of supply and we had a strong enough balance sheet you with his investment Alibaba that he made in the stock was at $250 a share you had enough Firepower in your balance sheet without even getting into the whole Vision fund situation so you had you know what are the hardest things in running a fund it's getting your LPS to give you money and finding people invested in and make sure make the right choices well we didn't have a problem with supply of deals and we didn't have a problem supply of Capital now the question is could we make some great decisions and I think if we didn't make some great decisions then you I I was gone before the vision fund was was in play but I don't think we made that many bad we made more good decisions than than we did bad what motivates you today you've presumably done very well financially with Google and uh and soft bank and now Palo networks all of those things I assume have have been beneficial from a financial standpoint a lot more than $200 yes a lot more than $200 I think it'll probably be more than you can spend I would guess Ely what keeps you going and waking up every morning what are the options well I I I you know I like doing this I enjoy it uh I enjoy uh you know building businesses understanding businesses I enjoy being able to get my hands dirty and make things happen happen so you know what better place than you know being at the helm of a technology company which is I think still one of the hottest spaces in the world I think uh the good news is uh there will be never any boredom or lack of innovation because every time we think we've cracked the last code the bad guys is going to figure out new way of entering businesses there'll be next technological Evolution so if you want to stay intellectually curious you know intellectually sort of in the right technical or Tech technological space this is a great place to be I had never,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 016,134,136
51,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,The Mission-Driven Aspect of Cybersecurity,3954,4041,technical or Tech technological space this is a great place to be I had never thought of cyber security as kind of mission driven but I I heard you speak about that I think that's an interesting thread to pull on how do you think about cyber security being a more Missi driven um look you're you're you're fighting the good fight you're protecting businesses from Bad actors you're trying to make sure that businesses can go on and do what they need to do you I was a month and a half ago I got a phone call from a prime minister of a country I'd met him at Davos he calls me and says NES we need help I said what's I said you got it he's like what I need said nothing just have your CIO call I called the CIO the whole country was down there were no access to anything we got engaged we sent people they brought the company back up and running country country they planes could land you know as a weekend four days our team went and got the country back up and running I got an email from a friend of mine from the UK saying I've been attacked can you please help we sent two people over there he sent me the nicest handwritten note and saying thank you for coming to our rescue so those are amazing things to be able to go out and tell you're you're not just selling some piece of technology you're actually making a difference to somebody's business by being there for them and and then you go back there and it creates lifetime sort of you know friendships it also creates like a good feeling to all the people who get involved like oh my God I actually made a difference one thing interesting I,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 017,135,137
52,The Logan Bartlett Show,Building to $100B: The CEO that Revolutionized Palo Alto Networks,Romanticizing the Past: A Perspective on Life and Career,4041,4166,involved like oh my God I actually made a difference one thing interesting I heard you say is something to the effect of The Human Condition is to romanticize the Past yes can you elaborate on that it sounds very profound I you know it's kind of like you know when you wake up in the morning you smile when you when you wake up in the morning or you got hurt last time skiing get back into your skis or something happens in your life so when you live a long life there is sometimes that things going to go wrong shit's going to happen right you can't always end up on the right side of an equation now if you give equal weight to the bad  in life you're going to end up a unhappy person or worse you end up in the middle so what happens over time is all of us end up romanticizing the past because it was a long time ago it and you over time say it wasn't that bad because guess what we're all happy we're surviving doing well so you take bad things in your life and you sort of dull their impact you romanticize them you make it out to be it wasn't that bad we all do that consciously I think it makes for a happier life and people ask what's the worst thing that happened what do you wish You' done differently and we all struggle like it's not just me it's like we all oh my God I wish this hadn't happened I wish we hadn't done that the other way like you asked me what would you have done different like I don't dwell on it because the more you dwell on it you end up a very bad place in life so we all have a natural human tendency to figure out a way of you know romanticizing it eliminating it making it less relevant less important our life because it allows us to move forward and enjoy the rest of our lives it's a good thing interesting Nash thanks for doing this thank you for having [Music] me he,Building to $100B: The CEO that Revolutionized Palo Alto Networks - 018,136,
53,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Introduction,0,37,[Music] hi everyone and big thanks for taking the time on you know we've been trying to get you on the podcast since uh we started it two years ago so we are super pleased that we that we have you on and indeed on your xplatform how cool yeah uh it's pretty cool yeah I mean you have like lots of people from all around the world uh simultaneously do effectively a realtime podcast and uh it works pretty well very good well we have so much to talk about [Music],Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 001,,139
54,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Where are we in the AI race,37,127,about [Music] uh love to kick off with with AI um now what's your take on where we are in the AI race just now wow that's a long answer um there's there's so much happening in AI is the fastest advancing technology that I've ever seen of any kind and I've seen a lot of Technology um you know barely a week goes by with without some new announcement so uh and if you look at the amount of uh AI Hardware the computers coming online that are dedicated to AI that is increasing what looks like at least by a factor of 10 every year if not every six to n months so when you combine the hardware um Coming online really order of magnitude increase every you know call at least every 9 months um and uh many many software breakthroughs uh if you look at that that curve it looks insane so I think we'll um my my my guess is that we we'll have ai that is smarter than any any one human probably to around the end of next year um and then AI the total amount of sort of sentient compute of AI I think will probably exceed all humans in 5 years what what is the what is the race,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 002,138,140
55,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,What is the race about,127,285,exceed all humans in 5 years what what is the what is the race about just now is it algorithms is it people is it computing power what what is it about just now is it the supply of chips just what is it yeah last year it was uh chip constraint um and the hardware deployment if you break it down into the three areas of people um data and hardware and starting with Hardware last year it was about a shift Supply people could not get enough um in video chips particularly um this year it's starting to transition to a voltage Transformer Supply so actually getting enough voltage Transformers uh put in place so my sort of very Niche joke is Transformers for Transformers because a lot of the AI That's run is called a Transformer so you need Transformers to run Transformers um and then next in the if you look out a year or two or certainly three years um it's just electricity availability so that's those those constraints in the hardware side um so many of the smart world's smartest people are are doing AI people that would have done physics before in fact or had have done physics for example have moved into AI because it's just the fastest moving field so we're seeing a lot of the best talents a lot of the smartest humans going into Ai and then uh we see along with that algorithmic breakthroughs um and then then you start hitting the the wall with the the data problem um so the you know you can fit all books ever written um just the text the the text in compressed form uh on one hard drive or call one one computer um so when you when you're looking at like so called tokens to train on yeah uh and you you still think of like all the books ever written in every in in all languages by All Humans sounds like a lot certainly it's far more than any one human could could ever read um it actually is a small it's a small number of train training tokens it's just not enough so then you you start having to look at all the videos of I created um you know all the podcasts all the everything um and and you start even running out of data there well hopefully they hopefully they will include this podcast uh that definitely will include this podcast,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 003,139,141
56,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Biggest challenge with AI,285,377,"they will include this podcast uh that definitely will include this podcast what's the biggest challenge you have with uh with xai well xai is still relatively new so it's not um you know uh like the limiting factor right now is just training our Gro version 2 model which should be do we think better than GPD 4 um and that's we're hoping to complete that in May so that's that's training right now so it's just really we're just trying to get enough gpus online to train it fast enough to get that done in May um which I think probably will happen um and then and and that's with uh roughly 20,000 h100s uh and and doing I think very efficient training then the next step would be for GR 3 which would be I guess G55 or Beyond uh would you know requires uh 100,000 Nvidia h100s training coherently so that's you know a half order of magnitude basically more training um and then you really start to have running into this data problem where you you have to either create synthetic data or use real world video those the the two sources of kind of like unlimited data are synthetic data and real world video which I should say Tesla has a pretty big advantage in real world video um Tesla has by far the most real world video of anyone yeah you've",Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 004,140,142
57,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,When will we see AGI,377,427,world video um Tesla has by far the most real world video of anyone yeah you've got a huge Library there so when do you think so when do you think we'll see proper AGI well it depends on how you define AGI if you define AGI as smarter than the smartest human I think it's probably end of next year like like within two years um but but that's that there's still there's still a pretty big leap beyond that to say smarter than the the machine augmented human Collective so like is it smarter than all humans working together uh who are also using computers to augment their output and that that I think is probably five years away one one way to look at it is is is to try to assess um like roughly what is the ratio of digital to biological compute last,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 005,141,143
58,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,New thoughts on regulation,427,554,assess um like roughly what is the ratio of digital to biological compute last question on um on AI any new thoughts on regulation and um how it should be structured well I I think we probably do need some sort of regulatory authority to look at the safety of AI um just as we have regulatory authorities and other Arenas to um you know o oversee aircraft and the safety of aircraft and cars and and other things you know medication so uh now the rate at which AI is progressing is is fast is faster than probably any regulatory agency can keep up with um but but I do have a comment on what I think is very important before achieving safe AI which is that uh it's very important to train the AI to be as truthful as possible um and not to uh yeah just to be as truthful as possible um I think you can get some very dangerous things when you program an AI to be politically correct think that things that may seem uh relatively innocuous now but will not be so in in the future if AI has immense power you can take the Google Gemini example where it it refused to publish to produce a picture of George Washington as a white man and and any in fact any historical figure would automatically be made diverse um because it's been programmed to insist on diversity which sounds you know perhaps okay at first but not if the AI has so much power that it can actually enforce diversity and decide there's too many of one kind of people or too many of one sex and kill off just just kill off enough until the the diversity number is is what it's programmed to believe is correct but don't you think this will be sorted out in the next version no no they'll make it more subtle okay and less obvious but it will still be there okay well we'll see but where,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 006,142,144
59,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Where is China now,554,621,still be there okay well we'll see but where where is China where do you where is China now in I relative to the US um I I don't know exactly what China is uh except there are a lot of very smart people in China um and they they won't be they won't be far behind the rest of the world or far behind the US um I mean the ai ai right now is very concentrated in San Francisco and London um and then you know there's there's you know a lot happening in in China but I I'm I don't have insight into what they're doing uh except that they I'm confident they will not be f behind uh what is developed in the west yeah um so but but but mark my words the if if uh if we do not program an AI to be as truthful as possible that that is where it will go arai that is where the danger lies yeah mov moving T here moving to to,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 007,143,145
60,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,EV conversion speed,621,729,lies yeah mov moving T here moving to to Tesla um is is the EV conversion now going slower than you had expected just where is the speed of conversion now relative to your expectations I think it's going quite fast actually especially Norway um absolutely well it's pretty much all there is is your your Teslas yeah there's a lot of Teslas in Norway it's crazy thanks i' once again like to thank Norway for the support of electric vehicles um so much appreciated time so I think it's we will the the that that electric that all vehicles will go fully electric uh it's only a matter of time um that includes aircraft ultimately and boats um obviously trains the only thing that is ironically difficult to well you can't really make it electric is Rockets because you need you can't get away from um having to expel Mass uh you sort of Newton's third law um but but all cars will be Electric it's only matter of time and we'll look back on combustion cause in the same way that we look at back on uh steam engines um that that I was it was inevitable that there would be internal combustion cars and and it's just as inevitable that o cost will go electric um and um there will be some e and you know so like it going to be a completely straight up line there will be some uh e and flow in how how far electric car is going but that but the ultimate um victory of electric cars is inevitable um and and I think the sooner we get there the better yeah how do you see the,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 008,144,146
61,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Chinese competition,729,755,um and and I think the sooner we get there the better yeah how do you see the Chinese competition here now we generally find that the companies in China are the most competitive in the world and certainly in uh electric vehicles or cars in general the Chinese car companies are by far the most competitive um yeah that's where where we find the most toughest toughest compet competitive challenges that they make great cars and they work very hard so when you ride in,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 009,145,147
62,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Chinese cars,755,785,challenges that they make great cars and they work very hard so when you ride in one of the Chinese cars what do you think I mean you're an engineer you know what about it what do you what do you think I haven't R I have not ridden in one lately but uh because they're not all available here you know in the US or very few are available in the US um some are available in Europe um but from what my team tells me there are very [Music] good moving moving out out in space,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 010,146,148
63,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Selfsufficient Mars,785,817,good moving moving out out in space um what what would it take to be self-sufficient at Mars to be self-sufficient in Mars it's really about the the total tonnage that is delivered to the surface of Mars um so you can say like well um I I think it's probably on the order of a million tons maybe it maybe more but somewhere between probably a million tons and 10 million tons are needed to make Mars self-sufficient and,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 011,147,149
64,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,How many Rockets,817,885,"million tons and 10 million tons are needed to make Mars self-sufficient and how many Rockets is that well I gave a presentation on this recently if people look at my my recent uh SpaceX talk but if you if you have U really uh if you have 100 tons per flight you need 10,000 flights to get to a million million tons um and that's 100 tons landed to the surface of Mars so in order to get 100 tons land to the surface of Mars you need 500 five times that number in Earth orbit um so we do a lot of orbital refilling um so launching sort of uh Rockets uh tanker ships over and over again that that would replenish the propellant of the ships that would go to Mars um and then youd need a on roughly on order of 10,000 of them to get to a million tons um and uh but we we plan to do that that that's uh that's we think we can get that done within 20 years really so and when do",Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 012,148,150
65,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,When will we be on Mars,885,985,uh that's we think we can get that done within 20 years really so and when do you think so when do you think we'll be there for the first time first first uh well the first Starship that will land on Mars which obviously would not not have people at first I think it's probably within about 5 years um and then it would probably launch several ships and just confirm that they can land okay on Mars um we'll also be doing the moon simultaneously with that so uh go taking well I think I think we'll get people back to the Moon I should say within 5 years and we'll get uh uncrewed ships landed on Ms within 5 years and and then we be building up the production rate um and improving the design of the booster in the ship so um so in the first people on Mars I think within seven years or so seven to nine years um and from from there we need to rapidly increase we need massive numbers of shifts going and Earth and Mars Only are in the same quadrant of the solar system roughly for six months every two years or at least it's only possible to really transfer efficiently um from Earth to Mars I say every six months but really there's about there's a couple months where where it's ideal every 26 months um so every two years that you would see a basically a fleet depart Mars and I think it be quite a spectacular thing to see a thousand ships depart from Mars all at once like Battle Star Galactica what kind of new technology do,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 013,149,151
66,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,New technology,985,1000,all at once like Battle Star Galactica what kind of new technology do we need before we'll be self-sufficient there actually I think we have all the tech we already know all the technology that's necessary for that it just needs we just need to build so no new physics is needed for this why is it so,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 014,150,152
67,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Consciousness,1000,1088,we just need to build so no new physics is needed for this why is it so important for you I think it's important for Consciousness in general um so if if we wish to maximize the lifespan of Consciousness then being a multipled species will result in a much longer uh existence of Consciousness Consciousness than if we're on one planet if we're on one planet we're simply biting our time until there's eventually a Calamity it could be soon it could be a long time but eventually something will happen it could be you a global Therman nuclear war it could be simply That civilization merely subsides our civilization may not die with a bang it may die with a whimper just just gradually falling into obsolescence but if we're multiplet species then we've got two planets and and they can support each other um and we can go beyond two planets ultimately to the moons of Jupiter to the to the uh um Beyond to the the outer parts of the solar system and ultimately to other star systems so this tiny this tiny candle of Consciousness that we have in this vast Darkness can be extended um and Amplified and we're just far more likely to uh survive as for for Consciousness to survive if we are multiplet,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 015,151,153
68,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Earth,1088,1185,Consciousness to survive if we are multiplet species you don't think it' be better to use all these resources and try to sort out Earth well just to put this into perspective the amount of resources I'm talking about for making life multiplanetary would be less than 1% of all resources on Earth so really you can think of it as resource allocation do you think it's worth spending half a percent of Earth Resources to ensure uh that we have redundancy in Consciousness and that we extend Consciousness Beyond Mars to other planets to to Mars and other planets and ultimately other star systems um and then also take into account the fact that there are certain inevit there are certain things we simply cannot avoid on Earth um like is it within your power of mind to stop World War I I don't think so no if it happens um and if we have theral Warfare our technology level will drop to the stern age um and we may never survive and then there are we maybe get may get hit like by a comet like the dinosaurs and um you know if the dinosaurs had spaceships they they'll probably still be around um so and then if if you wait long enough the Earth the the sun will continue to expand and eventually engulf Earth and destroy it and destroy all life so just to give it amount a certain amount of time no matter what you do on Earth no matter how careful you are um Earth will life all life on Earth will die that it will happen is a [Music] certainty on a slightly less gloomy note,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 016,152,154
69,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,X Twitter,1185,1267,certainty on a slightly less gloomy note uh X Twitter yeah um what is your vision now what do you how do you see the the vision of x i goal of X is to be the best source of Truth on the internet um and I think we're making a good you know good progress there and I mean this it's going to be like I call the everything app like if anything you want to do you can do on the xplatform um whether it's text audio video uh payments Financial stuff um Communications of all kinds um and then but but then also where there is publicly disseminated information is to be the best source of Truth um and I think it I think it already is that um now people may say oh there's some piece of misinformation disinformation I say yes but look look at the replies the reply is correct that misinformation and look at Community notes and the and how good the batting average of community notes is it's extremely good it's by far the best factchecking system on the internet um so and and a lot of people still labor under the illusion that the the the Legacy newspapers that they read are actually true there's so much nonsense in them I mean ni how many times when do you read an article in a newspaper where you know the circumstances of what that article is and how often is it spot on no of,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 017,153,155
70,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Russia,1267,1308,the circumstances of what that article is and how often is it spot on no of course it's uh normally no no of course we all know it's normally wrong but but how do you look not sure but how how do you look at the situation now for instance with with Russia uh you know the work Russia does in Germany with fake accounts on it's pretty pretty huge uh activity right I mean we don't see a lot of Russian it to be frank um on the system um so we we see very little um we do we do see a lot of lot of attempts to influence things but they seem to be coming from from the West not from from Russia right what about um what about,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 018,154,156
71,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Brazil,1308,1415,from from Russia right what about um what about things like the latest developments in in Brazil and so on yeah sure yeah so the the uh we we kept getting these demands from um uh this uh Judge Alexander um that's his that's his name on Twitter Alexander um and there would be to suspend accounts um immediately we're given typically two hours to suspend an account or face massive fines um and the the final sto we were were being given given demands to suspend sitting sitting members of the parliament and major journalists and moreover we could not tell them that it this was at the beest of uh Alexander Morales we had to pretend that it was due to our rules of service and that was the final straw and we said no now um when you when you bought Twitter um now renamed X did you expect that you would end up in these type of situations so it's is all unexpected well I knew it wouldn't be just a total B of roses um you know and it's fing I [Laughter] wouldn't um no I mean I thought it would be since we're just like rigorously trying to pursue the the the goal of being the most accurate and truthful place in the internet and that that doesn't mean that what is said is always true or accurate but is it is perhaps another way to frame it is as the least inaccurate place on the internet do you do you see,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 019,155,157
72,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Is Twitter fun,1415,1527,frame it is as the least inaccurate place on the internet do you do you see clear do you secretly think this is a bit fun it's fun yeah yeah it's fun at times it's stressful at times and it's fun at times um but overall we're trying to serve the people of Earth um and and and this is sort of an S sort of maybe an esic way of viewing it but um to try to be kind of like the the group consciousness of Earth so you can think of like if each person is like a neuron contributing to like the collective brain of Earth and you want to try to minimize the noise and maximize the signal of every neuron that's connected to the the X Network that that's basically what what is what is the collective will of of humanity and and how and and and how to yeah just serve the collective will of humanity and so serve the greater good that that's our goal um now there there's definitely going to be people who want to manipulate that information and so we have to fight that and try to have uh you know be it be the most accurate place as part to the best of our ability and have it be kind of a Marketplace of ideas where people can propose ideas and you know debate them and um I think so far it's working reasonably well in that regard um now people that don't like the truth will not like those or if they want to manipulate things they will not like it but only but only a few years ago you were you were a guy um producing electric vehicles now you are you know through starlink you've had some you know I mean some big impact in in Ukraine uh with Twitter you are kind of into some issues in uh you know Brazil India Turkey um you know you're becoming like,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 020,156,158
73,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Is Twitter a geopolitical force,1527,1676,Brazil India Turkey um you know you're becoming like a real geopolitical force and a really important one how do you how do you look at that well like I said I'm really I'm trying to take the set of actions that maximize the probability that the future is good um I mean we have to keep civilization going onward and upward as much as possible and um and try to minimize the civilizational threats that occur um you know we we we can't get to Mars if civilization collapses it's not going to happen so um you know we've got to we've got to keep um keep civilization going um and I think we should view our civilization as being much more fragile than we think we kind of take for granted oh it's always going to be there but actually if you study history you realize that there Rise you know there's rise and fall civilization um I mean I was I was reading in depth about the ancient samarians um who were arguably the first civilization if you call civilization like writing and stuff you know they were the first to develop writing um and uh but eventually they died out and they were gone so and then nobody could read the writing at all and and they they just faded out as a civilization um but they're pretty impressive in their time and the ancient Egyptians the same thing um and uh you know one sort of one after another uh ancient Greek had it Greece had its day uh you know China and India had will have incredibly impressed populations but there's been EVs and flows in the uh CH China and Indian civilizations over the the the aons you know the blenn as well um so you know I I guess I'm just trying to take this this set of the steps that um increase uh the scope and scale of Consciousness that's that's what I'm trying to do it's not it's not that I'm trying to have a put a political thumb on the scale or anything like that um but I I think I'm trying to have the political will go where the people want it to [Music] go you you mentioned some um,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 021,157,159
74,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,The key to managing smart people,1676,1747,go you you mentioned some um some uh uh really smart people here and um kind of just moving t a bit here to Copa culture now you manage a lot of geniuses in your in your companies what is the key to manage really smart people you think I don't I don't think I manage smart people they manage themselves um I I think well I guess with really smart people you know I don't really think of of it like managing them I think that if somebody's very smart and talented they they can go anywhere and do anything anytime like if they they they don't have to work with me they could go anywhere so I I really just say like look this is the the goal we're after and this is what we're trying to achieve and do you agree with this goal and if you do then let's try to get it done um and um you know provide my opinion along the way and once in a while I'll say look guys you just got to trust me on on this one we got to do this thing and if it turns out to be a bad decision you we can can all hold that against me in the future but you have an,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 022,158,160
75,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,How to balance micromanagement and delegate,1747,1817,decision you we can can all hold that against me in the future but you have an incredible eye for detail right I mean when we read the is book um it's pretty clear that you I mean you really are are deep into detail and know what you talk about so how do you how do you balance this um kind of micromanagement of some areas and then delegate other I wouldn't I wouldn't call it micromanagement um it's just insisting on atttention to detail that um if you're trying to make a perfect product you must have attention to attent attention to details essential um and I haven't actually read the isacon book you should it's very good actually I Lov it well I I asked Walter isacon if I should read it and he said I shouldn't um so so then he said I shouldn't read it so okay well I'll I'll ask you some questions from the book then they you he talks about you know you the kind of the hardcore and Ultra Hardcore culture what is an Ultra Hardcore culture I guess it's work I mean it's working culture right I mean how how I,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 023,159,161
76,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Working every waking hour,1817,1925,culture I guess it's work I mean it's working culture right I mean how how I mean Ultra hard work how hard is that well when things get really intense you're basically just working every waking hour and how and how long can you do that for I've done that for well continuously for sometimes like a few years what does it what does it du to you it really it's pain um and and every waking hour maybe it's an exaggeration because there are a few hours um obviously with friends and family and and critical other things um but 100 hour weeks would be I I've done many many stretches of 100 hour weeks like true 100 hour weeks um where roughly six hours per day is sleeping um I would not recommend that this is not that's for emergencies you know it's not uh all the time um you know during very difficult times at Tesla I've had to do that and sometimes at the beginning of my earlier start offs I did that where I just wouldn't leave the office I would just sleep under my desk and just work seven days a week um sometimes it's necessary for success or or to avoid failure um but but do you you do you enjoy being in this crisis mode no I don't it sucks okay no I I don't want to be there it's pain but sometimes it's the between success and [Music] failure when you make decisions how,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 024,160,162
77,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Elon Musk challenge,1925,1955,failure when you make decisions how important is speed he just gave me an idea which is um I'm GNA invite the uh Judge Al Alexander R uh to do a spaces and then he can explain why what I'm doing is bad and and and maybe he's right I challenge I challenge him to a spacers sounds good yeah but what about,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 025,161,163
78,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,How important is speed,1955,2002,"I challenge I challenge him to a spacers sounds good yeah but what about when you make when you when you make decisions how how important is speed and how do you how do you balance analysis with your gutfield I think the the the best offense and defense is speed if you think of something like the SR71 Blackbird it really had almost no defenses except accelerate and it was never shut down even once like I think over 3,000 missiles were shot at the SR71 blackb and non hit and and really what it did was just go faster so the the power of speed is uh underappreciated as a competitive Dimension um is that",Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 026,162,164
79,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Why SpaceX has been so successful,2002,2124,Dimension um is that why um you know Space X expenence has been so successful because you've been mean and lean as an organization and fast think speed speed is uh definitely a factor now I should say you want to go in the case of a company you you need to be a vector not a scaler so it can't be you need to go at high speed in the right direction sure so can I just so and no company's going to be going in the right direction all the time you have to do course Corrections like a guided missile you qued course Corrections um and uh but in the case of SpaceX it's like okay our goal is to extend Humanity beyond Earth um and we didn't even know how to even frame the question correctly like what what which knew that that was the General goal um we didn't know what pent we' use or what the raw materials would be or for the how would the rocket be built how would it be designed what's actually important um and uh you know so for example going from our Falcon architecture which is um uses refined jet fuel and liquid oxygen um in a um open cycle gas generator architecture engine to a to Starship which is a uh liquid methane liquid oxygen um uh propellant uh in a staged combustion very high pressure engine um that that that's that's a big architectural change um but we didn't know that we we needed to make that architectural change until we're pretty far down the road like about halfway took us about 10 years to figure out that was even the right architecture now we're confident,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 027,163,165
80,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Learning from mistakes,2124,2178,years to figure out that was even the right architecture now we're confident it is um just um we were on um uh risk- taking and so on I think SpaceX is one of the best example I know about uh what we call failing well right learning from mistakes and moving on um what generally how do you how do you look at mistakes well I mean which which ones do you tolerate and which ones don't you tolerate well I I I think I don't really think of that way uh you know the first three flights of SpaceX failed um the fourth one succeeded and if if the fourth one had not succeeded we would have gone bankrupt we would have had no money left so it was a very close call um but since then space has done very well it's now the the Falcon 9 you know knock on wood is the most reliable rocket in the world um and launches about every um two to three days now um,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 028,164,166
81,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Bet the company,2178,2260,"rocket in the world um and launches about every um two to three days now um last question on risk what are the types of risk you would not want to take uh well I I think in in terms of risks you don't you you don't want to take risks that where if if you only want to take bet the company risks if they're absolutely necessary so there have been a few times where saying the T with Tesla we we just had no choice but to V the company because if if we're in if we're doing a new vehicle program that is uh in order of magnitude larger than the past one then we're by we're just unequivocally betting the company because the new vehicle would be 90% of production so going from uh the original Roadster to the model S original Roadster was only you know about 600 6 700 per year then Model S was 20,000 per year and um and then model 3 is sort of half sort of half a million per year um model y over a million per year so these are all bet the company vehicles but the the the reason we could do for example a cyber truck which was kind of a a radical new design was because it wasn't a bet the company decision so I was like okay look let's try something I want to try something totally crazy uh it's like what what truck would Blade Runner Drive um except the when you're",Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 029,165,167
82,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Accept failure,2260,2343,it's like what what truck would Blade Runner Drive um except the when you're going to drive on was yeah I think it would be perfect for Ms um but like we could try something that where there's some chance that people might not like it um but it's it's radical and new and it's aesthetically aesthetically it's not derivative it doesn't look anything else on the road um whereas all the other sort of pickup trucks look like vague copies of one another um they we could afford to take a chance on failure and say like and talk it up to you know well we tried you know we try to do something interesting but but actually by the way cyber truck's doing great um so uh but one of the things that I think is important for Innovation is that you do accept failure like like necessarily you have to always look at the incentive structure of an organization and say um you know is is is that is that organization properly incenting Innovation um and in with if you do Innovation you're necessarily going to Uncharted Territory so they going to be some mistakes they're going to be some failures um and you have you have to like like actually like for for SpaceX uh rocket engine development like I keep telling the team look if we're not occasionally blowing up an engine on the test stand we're not trying hard enough you know um absolutely absolutely,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 030,166,168
83,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,PhDs are useless,2343,2390,test stand we're not trying hard enough you know um absolutely absolutely how important are the P how important is research and phds and that kind of stuff think I've said seen somewhere you you think most phds are useless well I think most PhD thesis are useless which I think is actually objectively true if you look at how many PhD you look at all how many phds are created every year and how many of those papers are actually used in anything yeah um then objectively most PhD TCS are have very low utility or maybe zero um because nobody uses them um or so once in a while you get something that is spectacular but it's pretty [Music] rare perhaps something more useful um is,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 031,167,169
84,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Learning from video games,2390,2599,rare perhaps something more useful um is in the book that you haven't read uh talks about your love for uh gaming in particular like stret IC ability gaming and I've been thinking quite a lot about it um what have you learned from from those games and have have that learning and wisdom been helpful when you have been planning your companies yeah I it's hard to say exactly what I've learned from video games except that I I do like playing video games as if I want to take my mind off work I'll typically play a very hard video game such such as which one well over the years it's been many many different video games um so you know when I was a little kid I was like you know pong and little tank games and things and um and but if you take a game like for example civilization it's actually quite a good um it tells you how how civilizations are formed like I remember I remember playing the original civilization with the technology tree and and how you invent different things you'd like invent literacy and uh you know invent democracy and invent gun gunpowder all all these things that like and you start to realize oh wow there's there are stages to technology like you can't um you know you can't actually get to democracy without literacy um and um you know so there's these these stages of of Technology development or stages of ideas that uh you know that's that's a helpful framework for a company um and I guess in in in like like I say in recent years there there's a game I played that was um actually developed in Sweden called polyopia which is actually quite a good game um like a lot of people like playing chess but I think chess is not a not a great um there's not a lot of transfer learning from chess to the real world because in chess you've got only 64 squares uh it's a setpiece battle same pieces every time there are no terrain differences uh there's no technology tree uh there's no fog of War um but say a game like polyopia has all of those things uh random terrain generation uh you know the differences in attack and defense bonuses depending on what type of terrain um you've got 16 tribes I think each with different abilities um you've got a a a technology tree that you can choose to develop in different ways uh and you've got of course fog of War um so that I think is much more much closer to reality yeah yeah um so I think politopia I mean I I was I was playing Diablo uh for a while pretty fun um Diablo as high levels gets very complicated they you could call it like a a spreadsheet with a game attached um so so that's that's and I briefly got the the for about a day the world record in this avire of zir on on a four-person team of of clearing the the hardest level um which was you know not bad for someone who's like 53 basically will be 53 soon um there is still some uh twitch element to it and um it's hard to beat kids at games with a twitch element um but yeah I like uh I find these games interesting if you can be fully immersed in a game some last,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 032,168,170
85,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Can you hear me,2599,2636,these games interesting if you can be fully immersed in a game some last questions here um as you know we are big shareholders and uh made a a lot of money uh on our investment [Laughter] o okay I can hear you here okay good sounds good good to go sorry I can you I think everyone can hear me let's see thumbs up if you can hear me let's try again okay okay sounds good sounds good um now um what is the score,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 033,169,171
86,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Storm has passed,2636,2677,me let's try again okay okay sounds good sounds good um now um what is the score now of in terms of the Union in Sweden and the collective bargaining actually I I think uh I think the storm has passed on that front I think things are in reasonably good shape in Sweden um so uh yeah I think things are good um yeah overall yeah I feel pretty good about the future I mean you know there's going to be bumpy quarters from you know here and there but I think the long-term future of Tesla is extremely strong uh for example um yeah I'm I'm back on just so um yeah,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 034,170,172
87,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Why are you skeptical,2677,2707,um yeah I'm I'm back on just so um yeah we met with uh we met with your chair last month so we we have some update but any any of view on it why are you why why are you skeptical to colletive I was playing with a soundboard here wo hello hello [Music] hello yeah last question for me um sorry,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 035,171,173
88,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Legacy,2707,2782,hello yeah last question for me um sorry I didn't hear the answer here because I was out but um we have covered this with with your chair but just a last question here what do you want your legacy to be I I don't I don't mind if uh my legacy is accurate or inaccurate uh provided that I I dive feeling that I've done the right thing for the future of Consciousness so just trying to trying to have this SL of Consciousness last as long as possible and maybe understand more about the nature of the universe or simulation or whatever this is so um I have a philos philosophy of curiosity which is to understand the understand the universe understand the nature of the universe um or even what questions to ask kind of like that I would say I would subscribe to the Douglas Adams hus guys of the Galaxy School of philosophy that we're trying to understand what questions to ask about the answer that is the universe okay I think that's a good,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 036,172,174
89,Norges Bank Investment Management,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management,Outro,2782,2805,the answer that is the universe okay I think that's a good place to end um for sure the life on this planet would have been been a lot more boring without you I'm I'm glad to [ __ ] it up a little totally all right well good talking bye take care now all right bye thanks [Music] bye,Elon Musk | LIVE Podcast | In Good Company | Norges Bank Investment Management - 037,173,
90,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Intro,0,106,welcome to the Logan Bartlett show I am your host Logan Bartlett and what you're going to hear on this episode is a conversation I have with Emmett sheer Emmett is the co-founder and former CEO of twitch one of the most influential Tech Platforms in recent memory from its humble beginnings as a reality television show focused on one of his friends to the most influential online community that exists I came with a framework for prioritizing features which is I got out of talking to a bunch of streamers and they wanted to fame love and money is this thing producing more money more fame or more love right we will prioritize resources against those is it not then it's not important or we're not going to do that Emmett and I take the conversation at a bunch of different directions about how he learned to grow as a leader and how he ultimately sold the business to Amazon and then stayed on for another 10 years after that running the company Amazon actually lets CEOs be CEOs if you want to sell your company and keep running it I couldn't recommend any really any big tech company other than Amazon how do we just make sure twitch continues to be the best place for streamers we also talk about why he thinks remote work is actually bad for most tech companies there's a reason why silicon Valley's taking over you can't take that culture and Transplant it into a remote work environment and expect it to work Emmett was a part of the first y combinator batch in 2006 along with the founders of Reddit and Sam Altman and so we talked what it was like in the early days of working with Paul Graham at YC he's going to walk away from the conversation with Paul believing we are the shed we are smart capable and we're going to take over the world the only thing standing between us and taking over the world is we have to go grind even harder road trips are not tours of gas stations and your life shouldn't be a tour of money picking finally Emmett shares what he's most concerned about with artificial intelligence so here's now Emma thanks for doing this,Emmett Shear on the Future of AI and YC Days with Sam Altman - 001,,103
91,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Welcome Emmett Shear,106,483,thanks for doing this um you had four co-founders of twitch Justin TV uh you stayed on ran twitch Michael has been running various aspects of YC over the years uh Kyle started Cruise after he left and then Justin Khan is a quasi internet celebrity at this point I mean he's gone through a bunch of stuff but yeah he had Atrium he had exact Atrium and now yeah now I think internet celebrity and uh DJ DJ got it interesting guy uh what made your relationships to their co-founders such that you guys all stayed together and worked well together over the years so uh I've known Justin since I was eight uh we grew up together in Seattle went to school together uh school being a high school elementary school uh Middle School who she didn't go to the same high school but then we did go to the same College again um and I met we met Michael uh in college probably closer to Justin than to me uh and then Kyle we recruited out of MIT uh as we were starting the company he was a uh he was a I think a sophomore at the time how did you come across Kyle well he says an email to the MIT like hackers who want projects list and two people replied and one of them was Kyle um and uh he thought we seemed incredible because we built a startup and sold it on eBay before and we thought he seemed credible because he sent us a 27-page PDF with CAD drawings and so we uh uh we wound up we want to pick him up selling a startup on eBay by the way we glossed over that but uh you literally listed the IP for your company on eBay which is an interesting separate story you've told it you've told him enough I won't make it again but I can tell the short version of it which is that we'd started a copy of Google Calendar but before Google Calendar existed and after Google launched Google Calendar we were like oh we didn't really have any other ideas for what to do other than a JavaScript calendar on the internet and they already did that so uh we decided we'd sell it uh we didn't know how to sell something on the Internet like how to sell us a software company uh especially one that wasn't like a real acquisition and so we thought we just listed on eBay uh which which worked actually we sold like a quarter million dollars um which um I think it's actually a good demonstration of like the kind of thing you have to do in startups a lot which is you you have to just figure out how to solve a problem uh even if the standard paths don't that you have to make up what what it is um there's a lot of making it up uh that has to happen first first thing like startup because almost by definition the normal stuff isn't isn't all set up was that a PR did you know it was going to be kind of PR stunty and that that would lead to distribution yeah that was that was the idea that was part of it the idea was we were like we need to get people to know we're selling it how do we reach them oh if we listed on eBay that'll be funny we can get people to write an article about that it was very intentional it actually worked it worked that's how it worked it worked it worked as planned actually yeah minus the fact that eBay took down our listing is we had two links in it um about five days into the seven day listing and we had to relist it which is kind of a hard stopping experience because we had one bit at fifty thousand dollars at that point and we were like no we've lost our bidder uh but it turns out that uh that that didn't actually hurt us at all so you had the credibility of that and so Kyle was like these guys have credibility yeah they they must know what they're doing I I think Kyle overestimated how much credibility that should have given us but uh it did it did prove that we were the kind of people who like could actually build things um and sell them so that's I guess it wasn't a zero and there was a deep level of trust but also some dysfunction among the group Kyle went nocturnal for a while to avoid conflicts so yeah there's actually we did a podcast about this uh an only friends episode which is great it's super entertaining two hours of like you guys shooting the yeah if you want the the full full deep details version of it but the high level version is uh like it worked because all four of us trusted each other um and we somehow managed to pick four people who were smart hard-working you know competent but I think most of all who had the who understood what it meant to be part of a Wii not not a bunch of individuals always thinking about whether or not uh how is this working for me um and I think that that's what makes uh that's what makes Partnerships work um both probably like romantic Partnerships and business Partnerships because if you're going to be in a a partnership not an employer employee relationship but a partnership relationship you can't always be thinking about whether or not or you can't be modeling what what are they doing are they are they trying to what what's their incentive versus my incentive versus the company's incentive it's dramatically simplifying it makes everything a lot easier if you can just only think about what is right for the company and then just ignore everything else um and I think that that's something that goes wrong in a lot of uh a lot of things and it's it's what causes instability because suddenly when there's lots of interest at stake there is conflict about my interest versus your interest but when you can simplify it to well what is our interest and everyone everyone can actually commit to that uh it works pretty well is there any way to solve for that as you're now working more with entrepreneurs um yeah be that way yourself yeah like the number one the number one cause of the problem in all of your relationships is you that's true again romantically and in business uh and the solvings for how do I actually buy into this is this is we we are doing this is really important and then also obviously you need to work with people who have that mindset as well but you have less control over that and you have less ability to detect it and at some level you know you have to trust and Hope but uh I think there's something about if if you do it first of all you can control that so that's good but second of all it brings it out of other people when they when they can see that you're serious about we and you're not you're not run you're not gaming them you're not cheating uh you're not like pretending you're about all about we and then actually being about yourself people respond to that it draws it out of your partner as well so you launched,Emmett Shear on the Future of AI and YC Days with Sam Altman - 002,102,104
92,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Products leading to Twitch,483,1099,people respond to that it draws it out of your partner as well so you launched and shipped a bunch of products over the years uh that ultimately led to Twitch so there was a calendar for gmail which was you and Justin then after that social network for families SoundCloud like business a web crawler that map popularity like clout a flexible to-do list kind of an airtable-ish thing an Evite competitor something that was kind of similar to GitHub her Roku replet whatever you want to call it then Justin TV ultimately twitch I heard you say something interesting when learning you get exposure to a wide variety of problems related to the topic to actually teach you how to understand and see the differences between things you don't just study one piece of music to learn music did all these experiments and all the different products that you guys iterated on did it actually help prepare you for twitch's success or are they just aimless Cycles along the way um I think did help prepare us I think the main thing they actually prepared me for was building like I did learn something from the product work on it but honestly we did such a bad job on the product uh design aspects of it and the sort of business design aspects of it that I didn't learn as much from that as I'd like to what I what we learned from is the engineering side of it like the design side of it like how do you actually build a thing that like functions and and has uh has functionality and not too many bugs and I got a ton out of stuff on the uh uh on the building side I would say on the on the how does stuff work side and learning how to design how to how to build a product how to build a successful business um the stuff I built myself was probably only a tenth of the data mostly I was like obsessively studying how other businesses worked because you get a lot more you can only launch a new thing every you know few weeks if you're fast usually every few months like a significant new thing but there's new stuff being launched all the time and if you want the number of reps you need you can't just learn from your own your own mistakes you have to be learning from everyone else's mistakes and everyone else's successes at the same time I think that's the uh that was my you know my primary uh you know University for that was it what was with Justin with Michael with Kyle every lunch every night every weekend like we just talked about startups Non-Stop and the thing we would often be talking about would be not our startup but like oh why is you know Facebook is working why is Facebook working why are they beating Myspace like what's different about it why what's good about their strategy what's not good about their strategy um and that and really carefully paying attention and trying out and like using the product and and paying attention to what is actually working in the real world you learn a lot I think that's where that's probably the main thing I learned from from a design point of view I I I joke that that's my main only value as a career Finance person investor is one I'm used to just giving advice and letting people decide you know I'm not used to having actually owning the decision myself which is actually can be nice as a board member like hey here are the options and two you get to simulate out a lot of different data points for all the companies that come in and pitch and so you're you can extrapolate on a bunch of different stuff there one of the things that uh I have a friend uh Matt who uh almost started our first company with us but instead went into went into finance and you know runs a hedge fund and uh uh has worked at a bunch of the finance jobs over the years uh and he's he's an investor in what I've discovered is investors have this just in totally different lens at least you know public markets investors and I think most private markets investors as well who are a later stage where it's about like whether the industry is good and like the capital allocation like the structure of the business which is not something actually that Founders think about all that much and I think I think it's right that Founders don't think about that because For the First two three years of a company it doesn't matter like just build something people want and sell it to them like ignore everything else but as you get bigger and bigger those things become actually quite important and you start to try to think about oh or in this industry but we could be in this adjacent industry and there's some way to shift how our business is perceived or what kind of work we do and how what's the structure of how Capital flows work and can we change that and those actually do become very important questions but like usually later um there's sort of a handoff that happens it's funny to hear you all talk about like iterating on the product and trying to find something and one of the throwaway lines I've heard I don't know if is you adjusted or someone's saying we could go be build Enterprise software but that's boring and we don't want to go do that I was like you know in 2006 or 2007 you would have created a lot of value doing Enterprise software and you were just like hey not for us that's too boring to go pursue it um I think uh we we talked about building instead of twitch or social cam um because Michael went and uh founded social cam which you end up selling to Autodesk that was weird in a way the Autodesk thing never really made sense to me um we talked about building a WebEx competitor because like WebEx is obviously a huge market like teleconferencing and we have like no doubt we could build a better product than WebEx because WebEx is annoying to use which we're basically talking about Zoom right a more consumer version of WebEx is zoom um and we decided against it because we knew it would involve Enterprise no not the product side of it but the sales side of it like Enterprise sales is like a whole business and uh we just like had no interest in like learning how to do that um and it's actually uh I think I think a good example of sort of knowing yourself like I actually don't think I would have had I think I probably could have learned how to do it but I don't think I didn't want to do that I wanted to do something else and I think it's it's important for Founders to do after the thing that they're interested in um although it was also a better time for Consumer like I think uh we go twitch worked a lot of actually good consumers companies started around that time it got a lot harder to win in consumer as the big player is Consolidated uh more of distribution and I think uh I think three or four years later maybe we would have pivoted into Enterprise just because lack of any credible other option just getting distribution would have been really hard with and certainly today it's a huge pain um there's an hourglass quote that I like a lot and I know you do as well I think I've quoted it before uh I won't read the whole thing but nobody tells this to people who are beginners I wish someone told me all of us who do creative work we get into it because we have good taste but there is a gap for the first couple years you bake stuff it's not that good it's trying to be good it has potential but it's not but your taste the thing that got you to the game is still killer and your taste is why your work disappoints you a lot of people never get past this phase they quit most people I know who do interest in Creative work went through years of this we know our work doesn't have the special thing that we wanted to have we all go through this and if you are just starting out or you are in this phase you got to know it's normal and the most important thing you can do is put in the work period it's a sort of profound quote I think it can be applied to a lot of different things as you guys were iterating on all of it did you did you know that your products just weren't that good and was it disappointing along the way to ship all these different things and not have it take off like you hoped it would yeah I'd say there's I had that in three different layers I'm the first one from an engineering perspective I just wasn't that good of an engineer I knew what good engineering looked like I remember visiting for the friend feed offices which had like Gmail incredible programmer I think who else is maybe Brett Taylor like like really insanely good programmers and them talking about their Engineering Process and like me using their product and seeing the engineering behind it just being like oh that's what that's what I'm trying to do that and like the thing I'm just like incompetent like the thing I'm doing I'm just doing this all wrong um and then spending like 10 years trying to get better at building stuff and eventually I I did learn how to build stuff more like the way that they did um and it was better but it was it was 10 years of like being quite disappointed in the quality of the engineering I was doing it was it worked but it it wasn't like uh it wasn't well designed it wasn't good um and then there was this the product side where um I I did I didn't think I didn't I wasn't that disappointed in the the Micro Design of the products I just thought we did a pretty good job there from fairly early and to some degree it's a uh it's a little different from uh from other kinds of creation things sort of like whatever you can think of you can kind of do like copying is very easy at that level and so if you have good taste you can just copy good stuff and it works but at the macro level there's this thing missing where like we weren't good at business well we weren't good at figuring out what do we need to build that people will actually want and the constant disappointment was we'd build stuff and I wouldn't have actually solved the problem for the person I think and that was very frustrating and like hard to uh hard to deal with and uh there was a sense of like uh that that's that's almost more like learning to do science I would say the engineering thing is the thing where I felt The Hourglass quote the strongest the and about like I had this level of taste where I could really see what I was trying to do and I couldn't achieve it with my hands the other thing was not understanding like uh it's like I was I was I was a grad student I was writing papers and they were getting published but I knew I wasn't actually like discovering anything that's that important like the the papers which weren't on important discoveries and I was somehow missing I knew I was missing something um but that's a that's a slightly different thing Than The Hourglass question,Emmett Shear on the Future of AI and YC Days with Sam Altman - 003,103,105
93,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Justin.tv (before Twitch was Twitch),1099,1690,but that's a that's a slightly different thing Than The Hourglass question Justin TV so you the original idea for it for people that don't remember was actually following Justin your co-founder around for 24 hours a day filming 100 of his life did you did you actually think at that time that like that you would be a reality television Studio business or did you know this was kind of a means to some end and you were mucking around the idea was always that we would open it up for other people to produce shows like we never thought we were going to be the people for doing the production everything we're going to license the MTV or whatever yeah we we thought we were produ building a technology to enable ourselves to make the first live streaming reality show and that other people would then go on to build make more live streaming reality shows using using what we built which you know within one order of magnitude is what what happened um but where we got what went wrong was that uh uh with that Vision was that like reality TV requires editing live reality TV is a bad idea I just didn't really understand how the entertainment I didn't understand we didn't understand anything about anything so that that was that was crazy but actually to level below that we were very very right because we what we actually had was that Justin uh who uh has always been you know living in the future a little bit when it comes to this kind of stuff uh really wanted to be an influencer and really wanted the attention and wanted thought it would be cool to like live stream a lot about of of his life which was a thing that not a lot of people had done at that point and we built the technology to enable him to live his influencer dreams and that turned out to be quite powerful actually um the number one thing we did right with Justin TV the thing I would encourage everyone who's trainable consumer software to do is we built something for ourselves we built the thing that we wanted to use every day or at least one person on our team wanted to use every day and uh most of the Innovations on Twitch on Justin TV were things we invented for ourselves running the show because we thought they'd be fun like the idea of having uh our own custom emotes and chat that are like faces of the people on the show that twitch distilled does that today uh and we had that idea because we were running the show we would never have come up with it in a vacuum and so I think there's something really powerful about that approach even though we were we were wrong about the reality TV show thing we were right about people wanting to do live streaming I mean twitch was ultimately a version of just Justin TV it was just more uh verticalized or specialized do you think had you kept that Justin TV I guess why didn't you guys keep at Justin TV why didn't you keep plowing ahead we we got Justin TV profitable after the 2008 financial crisis uh I've spent about a year you know cutting costs generating Revenue just iteratively focused on money and you had a Doomsday Clock actually on the wall yeah is that something you would recommend to people it wasn't on the wall it was every every Friday we would sit down with the employees and be like we've got we've got you know 14 months we've got 11 months we've got I think at the worst case it was like we were down to like nine weeks um it was certainly clarifying I think uh I only recommend it if you can convince people on the team that we can't succeed otherwise people will start uh packing their bags and looking elsewhere pretty fatalistic um but I think I think the founding team never showed any indication nor ever had any indication or belief that we weren't going to figure it out we were we were very sure and also we we started sharing We Didn't Start sharing it when there's like two months left we started sharing it quite far in advance um and everyone could see that while the Clock Was ticking down the rate was getting better and we were making progress and so it felt like oh yeah yeah we are pulling the plane out of the dive it might be a close thing but like they could people could see that we were going to get there so was the pursuit of profitability ultimately at the expense of Justin TV's success as a standalone oh it was it was at the expense of our growth uh and then we had this thing that wasn't growing we're trying to decide I was profitable but I'm not growing and we're trying to decide okay what do we do from here and uh no one thought just pursuing Justin's we had no good ideas for ideas for what to do to like reinvigorate growth of Justin TV per se and so we had you know Michael they did a pivot into so much you built some mobile video stuff which in an era before before Snapchat and before Tick Tock pretty good Insight yeah um I like Autodesk didn't seem to execute on the uh the vision of the consumer video yeah we didn't quite get there but but like but there was a there was a there for sure um and uh I want to do the video game thing because I was watching video games on uh on Justin TV and enjoying it um but the big pivot from Justin TV to Twitch like the real difference between the two companies is not the gaming versus the uh general purpose thing that's a superficial change um and it motivated me to focus on a set of customers um but I think we could have in retrospect I know why we did the Rebrand and like but we actually had a debate at the time whether we needed to do the redrands I think we probably would have been successful without it I think the Rebrand was helpful and it was uh it made more sense like that uh for the uh for the company and so it's probably like you know a good idea I would do it again but not strictly necessary would have found some amount of a similar level of success regardless and uh no but the big change the big change was was I realized that the important customer for live was the streamer not the viewer that that ultimately for live video unlike for uh for YouTube um we we could not say well whatever the creators will deal with it what matters to the audience they're just here for an audience they weren't really there for the audience the the audience was there for them and you have to keep winning the streamer's business every day because if you don't they're gonna go stream somewhere else and now there's no library of content like there is for the uh for something like YouTube and so you can't just have people show up create a little bit and leave that like that doesn't work you need people creating all the time and so the focus had to be how do we build something great for streamers um and that's why we invented paying streamers money that's why we like I went and interviewed a bunch of streamers and figured out here's what streamers need um and that was a that was a much bigger shift than the gaming part was building a product that was designed for streamers first and foremost was the gaming thing in intuition on your part I mean you obviously enjoyed watching it uh or was there I guess as you reach these conclusions gaming and focusing on streamers and not the viewers was there data that backed that up or was it just an intuition thing that you kind of realized the gaming thing was purely intuition I mean you could point at data like there are a lot of Gamers if I was to do a McKinsey market research thing I could be like Oh look The size gaming industry is very large that's more increasingly spending more time on the internet bessemer's memo is uh public from their investment so you can go look at all their Market sizing stuff I actually went back and I found my the email I wrote that's like it's like uh two paragraphs long like a like a half page email that's like here's why gaming makes sense and it's basically it's the very abbreviated version of that argument um advertisers like gaming you know like the game companies interest are aligned with ours so the copyright stuff like I thought about that but the idea was purely intuition I really like this I bet I'm not that much of a weirdo like I I like popular media if I like watching this it's likely to also be a popular media um the part that was more data backed was the streamer part um I went and looked and found that there were about 200 people making gaming content and who had any audience at all and I was like oh if we get you know if we can convince 180 of those people to stream on Twitch we just win like there's just there's just no it's just unambiguous there's no way we can lose actually um and so we just pivoted the company to be fully focused on that and that I think was reasonably data driven in the sense that I went and like looked at I looked at to just make the decision about viewer versus streamer I went and actually got data and then thought about it um although it was certainly informed by having worked in the space for five years and like having the intuition to sort of understand the dynamic of uh what it would mean to win a streamer and so the band kind of broke up at that point Michael went with social cam Justin left the company around then and Kyle ran the legacy of what was Justin TV for a while yeah so we uh we actually weren't sure whether we wanted to do the social cam idea or the gaming idea instead of debate it we were like we're gonna fund the gaming idea and the mobile idea is like Skunk Works projects and then we're gonna set goals and if they hit the goals we will pivot to them it was just jamming more ads on Justin TV to pay for this we are our jamming words TV that was that was already the pre-roll mid-roll banner that was already the plan yeah the plan was already to extract like we were already in the we were Justin TV plan unchanged extract maximum profit um we then but yeah that meant we were profitable which we could fund like these projects um and twitch we hit our goals we hit like the we hit like I said like a 25 growth a month goal and we were hit like a 30 growth a month which is great,Emmett Shear on the Future of AI and YC Days with Sam Altman - 004,104,106
94,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Growing Twitch,1690,1960,growth a month goal and we were hit like a 30 growth a month which is great um and how did you go about actually doing that was that the work with the streamers yeah it was just it was the the exact process I was talking about of like oh there's 200 streamers let's call them what does it take to convince them I'm sorry Kevin was one of the very first people who I think he was the first person I brought on to my my gaming team he was my the first time I was like I need Kevin I was like Kevin uh was our he was basically all the non-programming engineering stuff originally um and I was like Kevin your mission is to convince people they need to stream gaming on Justin TV like go talk to them and get them to do it um and Kevin is probably one of the single best beauty people I've ever worked with and uh he did a very good job of doing that and like I backed him up with he would be like Oh They'll switch if we build this and I'm like great on it and then we just go build the thing um Michael had took two engineers and built uh and Justin kind of uh started working mostly on that project with Michael building uh social cam we didn't actually hit the social cam goals it took them longer to build Twitch uh the Justin TV gaming was like literally just built on Justin TV gaming like we didn't have to build a new product we turned out we overestimated how fast it was it would be to build uh an entirely new product from scratch for mobile gaming so that they didn't hit the growth goals but we felt like the product they built was promising enough that it was worth doing the effort to spin it off and be retrospect we were right about that too so we spun that off um Michael went with it uh Justin hung around for a few months but like I think you know was also feeling the issue something else and I think wasn't it it's funny Justin now like loves twitching like with learned what the product was I think at the time didn't really understand or love the gaming streaming product and so he was like didn't want to work on it that much which totally makes sense to me um and then Kyle uh was figuring out what to do graciously ran the Legacy business for me for like uh more than a year I think and then found uh Colin carrier who was our uh who was the his replacement um to run it uh which was amazing because like that was basically funding the business so we needed someone good running it um and then he went off to start Cruise I heard you say that people come for the entertainment but stay for the relationships within the community what when did you realize there was something unique going on from a community standpoint yeah come for the video stay for the chat so uh when I first started building twitch I was using myself as the Prototype viewer and I am the type of viewer we've come to call a strat the strategist which means I was there because I liked watching people who are good at the games that I like to play so I could learn from them ask them questions stuff like that almost like a group coaching session um or I could watch Esports and like like live uh Esports entertainment and that was also that was also uh great for me um it turned out very rapidly though that I was in the minority strategists make up about 20 percent of twitch viewers 80 of people were there because they like they liked the hanging out they liked the connection they got from hanging out with other people in chat and talking to people looking you know watching them it was just rapidly obvious oh I'm I'm actually not the Prototype here these other people are the Prototype is that are those the two groups yeah you can you can you actually can subdivide the community group more finely so we have more second Graphics underneath that but they're at the top level there's like are you there for the entertainment or are you and then learning or are you there for the entertainment and then Community um and then there's you can you can go deeper I'm curious like are there any broad Strokes of the other types of people it's it comes down to like are you do you are you about uh the personality more or are you about uh the chat Vibe more like what's the what's the part of being here that's the most important thing but like it's a little inside baseball and fundamentally not that important because they all benefit from the same like it you don't actually build different products for different people yeah so it doesn't you know it's at some level it doesn't really matter um whereas the the learning versus Community thing is a really important uh product impacting Insight um and so that's when we sort of realized like oh people are there for the connection okay that's the like real that's the thing they stick around for most of our users stick around for um and so that was uh that was a pretty deliberate uh learning from looking at our users like it wasn't we didn't just make it up,Emmett Shear on the Future of AI and YC Days with Sam Altman - 005,105,107
95,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Winning over streamers,1960,2220,uh learning from looking at our users like it wasn't we didn't just make it up were there non-obvious things that you did to cultivate the community or ultimately what is building the product for the streamer then it became obvious that they would cultivate it yeah no I think that that was the that was the learning from focus on the streamer was like very very early it's obvious if our customer is the streamer then our job is to help them cultivate their Community I still believe that to be true I don't think twitch has communities doesn't have any Community Church has streamers streamers have communities and when people get confused and think that oh the community people on Twitch care about twitch they don't care about twitch they care about the streamer and our job is to facilitate that um and it's very egoic I think it's like you get you're like you're telling the story that you're the hero the streamer is the hero our job is to like with the one of the slogan we came up with for it later was we play support like the streamer is the carry we are support um and I think that's uh that's a key like mental model for a product like twitch twitch was able to uniquely beat back competition from a bunch of uh heavily funded tech companies along the way I guess what was the feeling Google had a famous tweet welcome player two uh when they when they announced their presence where were you when that happened what was the sentiment around that yeah so after we were acquired by Amazon I think everyone else who was like oh maybe we'll acquire twitch it's like oh we're not gonna acquire twitch okay uh we better launch competitors and So within like less than a year Microsoft Google YouTube and Facebook had all launched very explicit like twitch competitors and started spending huge amounts of money bidding on streamers um in a very non-sustainable way that wasn't that was trying to them trying to buy a critical mass um of uh of audience uh which was uh was exhilarating um I think it was good for twitch like competition is good it's good for the service um yeah we we posted a kind of snarky tweet they sort of Welcome player two uh when when uh YouTube launched theirs um but I think that really was our attitude which was like if you can build a better product for the streamers you're more than welcome and you should win um but we think our product's better and we think streamers are going to choose us every time and our strategy to beat everybody was we had to bid against people and stuff but like ultimately we let a lot of streamers walk like we we could have kept bidding more money um and uh we didn't outbid on everything we updated a bunch of stuff but like not everything um and our Focus was from the very first beginning how do we just make sure twitch continues to be the best place for streamers um and if we continue to make it so like they continue to choose us instead of instead of the competition and I've uh I remember reading a bunch of some streamers complaining about to actually be like basically like oh I want to go to the you know YouTube except like you know I feel like they're offering me more money but uh and I'd go except you know it's not fair like which has to be trapped because because all the moderation tools are so much better and this is bad I'm like yeah we you mean we trapped you about with our nefarious plan of building a better product that actually works yes I fully admit this is our our secret plan the rules all along yes you didn't see it coming but we would we're gonna build stuff that you like and then give it to you for free um yeah so that very diabolic yeah it's totally diabolical but like that was the that was the thing like we we won because our product was better um and uh I think that's like a and it was better in the way people people cared about there's a lot of nice to have features um like fun things that people think they want polls is the best example I think we actually have a polls feature now for many many years we did not and our competition did they had a polls feature people would complain about this but the truth is you can run a poll using like a online like there are many poll things you can just use on the internet if you want to if we screw up chat moderation if we screw up uh if you're if your subscriptions don't work well if discovery isn't good you can't fix those things only we can and you don't actually want us to build a poll you think you do but like the truth is you don't actually and when you would confront them with the actual choice would you like to make a dollar more per hour or would you like to us to build launch polls like oh a dollar obviously what are you talking about like well that's right okay we're gonna keep working another thing then how do you get to those conclusions like when you're hearing from your users I want,Emmett Shear on the Future of AI and YC Days with Sam Altman - 006,106,108
96,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,"Streamers want fame, love and money",2220,2342,you get to those conclusions like when you're hearing from your users I want this thing but being able to discern that they actually don't really want this thing at the expense of what it would take you to go do it and is it actually framing in those terms of would you prefer an extra dollar or would you prefer like more Engineers on this yeah so when I got started with twitch I uh I came up with a framework for prioritizing features which is I got out of talking to a bunch of streamers and basically uh asking them why they streamed and sort of what their goals were and what what they what they wanted out of it and we came up with was they wanted Fame Love and Money um uh Fame they want to like obviously reach an audience grow it have more people watch them get more hours of video watch basically um love they wanted to feel positive social feedback and connection for doing it whether that's from twitch because they got invited to twitchcon or because some uh twitch you know partner manager reached out or it's from Their audience saying how great they love the show and how amazing that was um and then money they wanted to make money on the stream and basically if we built something and there's a secret fourth thing which is like the video has to work they want the live video to be like actually good enough okay cool that's us let's put that one aside that's the underpinning that's the underpinning like that's this table stack that just has to work uh assume it does um is this thing producing more money more fame or more love great like we will prioritize resources against those is it not then it's not important and we're not going to do that um and uh that worked because those were the three most important things and then once you have that structure you can kind of it's easy to compare within money which of these things we think will be most helpful to make the most money within discovery which of these things in Fame which of these things will help grow audiences the most um Love is the trickiest one obviously because love is a little bit less it's a little more fuzzier and less clearer than Fame and money are but uh but ultimately it's like having good moderation in chat um having uh experiences for streamers where they feel they feel loved by by twitch and that we care about them Once Upon a Time Google came and made an,Emmett Shear on the Future of AI and YC Days with Sam Altman - 007,107,109
97,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Amazon acquihire,2342,2825,twitch and that we care about them Once Upon a Time Google came and made an offer to buy Justin TV uh before it was really in the before all the twitch stuff came to be and ultimately uh you got far down the acquisition process and then people on the team failed Google's uh tests or something they uh Google was considered an aqua hire and out of our like whatever I know 30 total people they like had like 10 that they thought were like Google quality um and made an offer on that basis and we were just like no uh and in retrospect I remember like looking at the offer and if they'd offered us three times more money I think we would have taken it um but uh but I'm really glad they didn't offer us three times as much money um same thing actually happened with Kiko on Yahoo Yahoo offered to buy Kiko which is your calendar with a calendar company for a million dollars in but like in retention over like four years so like 250 000 a year in signing bonus on top of a you know mid-level Yahoo engineer offer and I'm really glad they lowballed us because if they hadn't live all this we might have taken it and like uh uh we didn't really want to do those we didn't really want to work for Google or work for Yahoo we wanted to run the company it's just it felt like you have to take if they offered you a win you could like have to take it um so I'm so I'm glad they didn't are there any lessons actually from either of those stories that are that you could extrapolate as a as something you would tell a founder or is it hey those are the funny situations and the road not traveled but at the end of the day it's just there was nothing to be learned from the individuals I don't really think I've I've I've the only time I've ever used it someone's like oh I got an acquisition offer from this company um and I talked to them and I'm like okay this is this is like an aqua hire here's like here's how it's gonna work like here's here's how Aqua hires work and having been through one myself I kind of like I know what the what the process will be like a little bit although it does vary from company to company it's not that variable um and so occasionally it's helpful for advising someone who's in the exact same situation but other than that there's not really much there's not much generalizable from it I don't think this might be an annoying question that just uh people in the media and I guess Venture capitalists ask you but how much time have you given to thinking about twitch as a standalone versus within Amazon I like whether we should have sold it or not yeah um well I mean selling it uh worked out really well for us because uh it was unrelated let twitch grow on its own like did they help with bidding for streamers and like was that an expensive Capital outlay that wouldn't have been obvious because the weird thing about twitch versus Instagram for example is Instagram it's hard to uh disconnect Instagram within uh Facebook right because of all the stuff they did from an ad infrastructure standpoint and you know the adoption and promotion and all that I think it's kind of hard for me to mentally disconnect the two twitch was to your point able to operate pretty Standalone right and so almost you can you can at least from the outside in maybe you disagree almost see what the business would look like is a CEO for a very long time and it pertains many of its CEOs for a very very long time like how you were there Don Katz at Audible for 10 years eight years uh eight years um and uh they do it because Amazon actually lets CEOs be CEOs that's like that's uh it's the most decentralized of the big tech companies and uh I think uh if you want to sell your company and keep running it I couldn't recommend any really any big tech company other than Amazon because it's the only one that runs under that model um I've thought about what it would be like to have done it separately um and I actually just think it wouldn't have been that different the biggest difference was the reason I sold at the end of the day is I don't like fundraising and I knew that if I sold to Amazon I'd get to keep running my company in fact they would demand I keep printing my company and that but we were aligned in that I wanted to and they won they wanted me to so great and they would make sure that they would they would fix the I have to raise Capital every year problem because they're committing by buying us to fund us um and uh or like write out pick to take a big loss but like they're basically committing to fund you and so uh that worked out where I would say it worked out as planned um and so uh I feel really good about the decision in retrospect uh I think I made it more intuitively at the time than like reasoned and it only took me a couple years after doing it to actually even figure out what the math I was I had been doing subconsciously was but uh but I think it was the right to call for sure what did Amazon do to keep you around for so long besides just letting you run autonomously where's that at that that's yeah what that's literally the thing that they did I don't you know if hey hey like you know you know big tech companies if you want to retain CEOs after you acquire the company you have to let them still be the CEO which means they decide who gets hired who gets fired what contracts you sign what products you launch if you're going to take that away from them make them go through your processes there if they're any good at all as a CEO they're going to leave because they're not a CEO anymore they're a division vice president and that's not the job they signed up for so FYI if you want to retain CEOs one weird trick what uh but did they get strategic value I guess to take it from their Vantage Point obviously twitch is a very important asset within there but the the flip side of of the reason I think people end up being somewhat heavy-handed or at least involving themselves post acquisition of a company is to try to make the value greater than the individual component of it were there things they did from an integration standpoint or things that they pushed that benefited other parts of Amazon um I mean twitch advertising is sold by Amazon advertising the third biggest I think third biggest I don't know if it's exactly I think it's third biggest advertising company in the world so that's that's good that's like the Instagram Facebook thing in a way um except I think we provide more value to Amazon relatively Facebook already had a lot of AD inventory in feeds that's basically the identical whereas twitch Amazon did not have a bunch of ugc video and uh you know premium gaming video to like sell against yeah that's that's them to you I was wondering you to them if there was anything that oh no I think actually the revert in the advertising case unlike Instagram where it's mostly Instagram benefiting from the Facebook it advertising machine here it's much more two-way we open up a new kind of inventory they don't have access to otherwise which is good for the overall Amazon organization but also good for twitch because we get access to the big sales team so I think the advertising thinks the obvious just like with Instagram it's the obvious integration point for something like twitch um we also do stuff with prime that's that's been very good that's which are quite well again very two-way um uh and we've launched IBS um which is our the interactive video service and AWS service um very cool like to be able to take one of the things core things you built and externalize it as an AWS service is very cool um and something you can't do unless you go to I guess it'd have to be Amazon Google or Microsoft basically because those are the cloud companies um and we definitely couldn't have done that on our own um but yeah a lot of it has been less deeply integrated um and I think that that's except for the points like advertising which is a really big one where it does make sense to integrate and I think it's really to Amazon's credit that they didn't try to force a bunch of Integrations for efficiency reasons um basically if it's always efficient in theory it's never as efficient as it seems in practice so um uh I I think strategic there are strategic Acquisitions where you're buying something as a technology you know Apple buys some semiconductor companies so they can start making chips sure and that should get integrated it's not it's not actually a separate product but if you're buying a product to be part of your portfolio I actually don't I think the often the Integra it's the Integrations are a little heavy-handed for no good reason,Emmett Shear on the Future of AI and YC Days with Sam Altman - 008,108,110
98,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Declining value to more money,2825,3256,Integrations are a little heavy-handed for no good reason I've heard you say that as you get money there's a clear declining value to more money obviously that's true it sounds like something people just say but um how did how has that actually impacted your life having success and how would you internalize that or I mean having money is great um the you know the utility of money declines approximately with the log of the amount of money um you can actually see it on like uh if you if you plot like you know life happiness correlation curves and stuff so every 10x more money you have is like one incremental unit more so like it's not that there's no value of going from one million to 10 million but it's like 1 million 10 million is the same as a hundred thousand to a million and so every going from one million to two million is like yeah like it's just not that big of a deal even though it is a million dollars more it's like it doesn't change your life that much um uh I am very glad to have money um because it makes my life directly better in terms of consumption but most of all because it allows me to do things like invest in I've invested in my friends companies or in other you know in strangers companies and people I've met companies and that's being an angel unless it's really interesting and I feel like I get to support people and and help um I got to fund SF New Deal uh right and like I got to write a check for a million dollars uh in SF New Deal uh because I I had someone I knew Lenore who uh Lenora Estrada who CEO and I trusted she could step up and run it and I could make that decision fast and just write the check um I didn't have to go try to raise money from other people or put it together crowdfunding it and uh that was really valuable I really like stand by that being that was it was and it was interesting it was fun for me um and uh uh you get to go do think cool things for your friends in your community so I mean yeah but I guess having money is great who doesn't like having money um I don't think it's the it's not the only thing um there's a the quote I like about money is like you need money like if it's like on a road trip you need gas but light you know road trips are not tours of gas stations and your life shouldn't be a tour of money making uh activities but like also like you should think about where the gas stations are on your road trip like it's especially if you're going in on a long big one like you really don't want to run out of gas uh over the years you've been particularly transparent about being a bad manager initially uh I I there was a quote you had I was bad at hard conversations I was bad at good conversations I was bad at delegating I'm sorry to the people that worked for me in the early days uh I assume you're being too hard on yourself there but what were some of the things you've learned about yourself and managing that uh allowed you to develop into a good manager so I would say in the early days I was a pretty good leader already but a bad manager like I could I could inspire people with a direction I could communicate but I I was I always stand behind when I said there I was a bad manager um but I don't think I'm being particularly harsh I have yet to meet anyone who is a good manager the first year they are a manager I don't think anyone is uh being a manager is a set of skills like a thousand different skills and the you aren't good at some by they're not like things you just know automatically like no one's a good skier the first time they go skiing it is not possible to be good at first um there's only one thing that distinguishes good Managers from Bad managers consistently that I've found some people when presented with this uh don't like this truth that they suck and decide that they are in fact good managers already or that management is easier that they don't that it's not any they're as good as they need to be and some people realize that management is a skill and they're good at it yet and decide I I want to get better at this I want to learn how to be a better manager and they every year they're thinking about how do I get better how do I how do I how can I become a better more qualified manager and I can be better at this set of skills and the people who try don't always succeed but fair amount dude Management's not so hard if you treat it like a set of skills they're not the hardest skills to learn anywhere it's not as hard as like you know abstract math and inventing like new uh uh you know new theorems about topology that's that is more difficult than management but it is it's hard enough you have to learn the people who dedicate themselves to it often usually not always become good managers the people who don't are universally terrible and that is that is just the only thing that differentiates the people is like whether you accept into your heart you need to learn you need to get better at this thing that's basically it what was counterintuitive or what what skills were counter-intuitive to you that you that that maybe took longer to learn or just were super surprising as you got higher and higher uh with more and more people the thing about management skills is they're all dumb they don't they're not surprising when you say them out loud they sound stupid um like uh when you delegate something to someone you have to actually delegate it to them you can't delegate it and then like look over their shoulder and then tell them they did it wrong like that's just gonna make them unhappy um and and and worse they're gonna stop trying to even do the thing because they know whether they do it well or badly you're going to show up and then review it and then make it the way you would have done it and then and so why should they put much they should this their whole goal is to get it in front of you so you can correct it and like that's obvious if you think about it it's not like that's a dumb it's a almost dumb to get wrong and almost everyone gets it wrong when they first become a manager because it's a little it it's I guess it's counterintuitive but I don't even think it's the counterintuitiveness it's just you don't most of management is learning to notice the Thousand and One things that you can do wrong and if you thought about each one of them you would probably come to the right answer but you just this doesn't occur to even notice um I guess the thing that one counterintuitive thing about management that was a surprise to me is that managing individual contributors on a team is not the same job as managing a team of managers is not the same job as managing a team of managers and managers it's not the same job as managing a team of directors it's not the team same as managing a team of VPS and that's not the same as managing a team of spps every time you add a layer to the company your job is like oh it doesn't have nothing in common but it's fundamentally quite different um and that's one of the reasons why it's so hard to scale teams and people like sort of uh hit a scaling limit at some point it's because if you keep a fast growing company everyone's like winding up underwater all the time as layers under you get more layers under them and that that directory was doing such a great job is doing a bad job now they didn't get bad the job changed from underneath them and so everyone's in this race to like figure out the new job before it uh before it kills them what changes in,Emmett Shear on the Future of AI and YC Days with Sam Altman - 009,109,111
99,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Changes in levels of abstraction,3256,3613,like figure out the new job before it uh before it kills them what changes in those levels of abstraction like I mean at a high level when you're when you're so far removed what is so different than being on the ground I would say like uh the you have just the techniques you you have to use techniques that scale better as you get that rather than things that are maximally effective you want things that are scale best and that like the trade-off between how effective is this individual interaction how good it how like in the beginning you need things you need to actually fix problems and ship code and make things happen um and that kind of dynamism is really important as you get higher and higher in a hierarchy you have to figure out how you can make your time and your effort and your your energy scale because you can't go do it Thing by thing and so there's a trade-off between those two things and you are uh you need to pick either the skills that you need are more of the scale scale level skills um writing becomes more important when you're higher in the organization for example as you get higher and higher and the company gets bigger and bigger there's also a component of transparency and input of decisioning that you can't be can't be quite the same as it was in the early days right like the decisions all the inputs that went into an individual decision you can't communicate in Mass to the totality of the company which I think could be hard I think that could be hard when you come from a culture of like full transparency or it was small and now you're big and all that did you did you experience that was that something that you struggled with of just like how to balance transparency with just giving people the outputs of stuff yeah I mean there's people can use the word transparency a lot but what they mean is input um you can give transparency for stuff actually not everything but most things no matter how big you are a lot of the time the transparency just makes people more upset though because what they really meant when they said they wanted transparency is they wanted a voice and that you cannot scale everyone cannot have a voice on everything um in fact almost everyone can't have a voice on almost everything and uh that's not true when you're small it gets true and you're bigger when you're small you can stay very very synchronized and as you get bigger everything has to just desync um and uh figure creating mechanisms and processes so that the right people can have input at the right time is like that is the art of management and and production in a company of Greater size and like I don't know there's no there's no short answer to that question that like is it all meaningful but uh I think that the main metaphor I always use for it is like you have to you have to Shard the decision making like sharding is any different programming and from like databases where you have to find a way that you can divide stuff up such that some set of the of the data of the decisions can be made in this hundred people and this hundred people in this group owns everything about that and they don't have to communicate much with the other groups and figure out how you can bottleneck and shrink those uh between group Communications and and let people interact in a more asynchronous way um and again there's a thousand and one techniques for that but that's what you have to figure out how to do what was something that you you found that even after uh whatever 10 12 years how long would you run it uh 17 years 17 years from from Justin TV and what was something that you found that was just you were never going to be particularly good at and that you needed to augment uh or or get people around you to to help you with I think I uh what I discovered is I could be good at literally anything um there was no skill I could not learn but there were a lot of things where uh I don't like doing it I'm never going to enjoy doing it and it's always going to drain energy from my day and by the end of the day if I'm doing a bunch of that kind of work I'm gonna be grumpy and unhappy and tired and people interacting with me are probably not gonna have the best experience um and so it was less about what I could get good at I could get good at almost anything at least you don't once um it was more about like what could I do sustainably what what what how do I manage my job so that I am doing things sustainably in a way where I am can consistently do it at high quality uh when I'm not at my best um and so for me one of those things is like I really have never been great at interviewing I don't like interviewing uh new candidates for jobs um there's a lot of energy for me to meet new person and bring bring that their who they are on and make a connection and like I like doing it actually uh when the either someone I find myself naturally drawn to which is you know whatever one in four people or something like which I don't think is unusual in terms of the one in four part I think it's just an unusually high energy cost for me for the other three and I can do it and I've learned how to do it and I've but I don't like doing it and so I've just found ways for that idea was not the first person I I had this heroic idea like hiring is super important I need to be the only one out there you know interviewing for the new CFO um or the new you know head of product or whatever that turned up a terrible idea I was grumpy all the time if I did that um then instead the right thing for me to do was uh let other people do most of the interviewing and only interview you know two or three final candidates um and you're sort of admitting that you don't have to like all the all the work you do you tweeted recently management lessons,Emmett Shear on the Future of AI and YC Days with Sam Altman - 010,110,112
100,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Management lessons,3613,4132,you do you tweeted recently management lessons from the front number seven six one I don't I didn't go back to look if there were actually 760 before of the in front of them but your reports will ask you a lot of stuff especially in the beginning like should we do X or Y do not be tricked this is usually usually unintentional trap the only correct response is what what do you recommend does that tie back to the autonomy of decisioning and being able to delegate yeah if you and this is this is not to say that you should never answer your reports questions I'll be really clear uh some people interpreted me as saying uh in that uh you should uh when when if you tell the report like uh you tell you you if you tell someone like uh uh you know hey go go make this thing go build this thing and they come back and say oh well there's this trade-off that's above my pay grade uh you know the company yeah I have to commit the company to 20 million dollars of liability or we have to spend an extra 16 weeks doing this and I kind of think we should actually commit to the liability but like what do you think that's a fair question that is not that is not uh uh it's not their job to figure out uh you know some individual contributor you know a new person on the marketing team is not the company's job to figure out should we run this level of risk no no fair and those often get kicked up the chain and that kind of taking on a risk sometimes requires all the way up to the CEO to sign off totally reasonable but when you ask someone like hey like uh like they're designing a new a new ad campaign if they come to you and they say like oh well do you think we should make the ad campaign's message this message or that message that is literally the that's the job of the person who's you asked to go should we use this creative approach or that creative approach it's your ad campaign like you tell me um because if you don't do that they don't own the decision um and the whole point is to delegate down both the authority to make the decision and the accountability for the results of that decision um and in very dysfunctional companies sometimes uh people have learned you they actually don't have the actually have the authority to know what no one tells them and that means they don't want the accountability and you want to put this entire circle of them very logically constantly trying to push off accountability by getting input and making sure everyone is signed off and they never have to make a decision to take any risk and that's because they know they don't actually have the authority and they don't really believe they'll be allowed to do a good job um and I I have empathy for people who are stuck in that situation it happens even in good companies it happens sometimes uh but the the good solution is not then get everyone to sign off on everything so it's all safe but rather than actually Empower people to make decisions so it so that they you can actually just do stuff you did a great YC talk with regard to product development um about how to think about developing product and talking to your users and customers users and customers or competing Services as well as other people that aren't using your product uh today we touched on product development a little bit and the three pillars that you guys kind of use to think about um how you think about I guess now it's probably been 10 years or eight years since you did that talk how do you think about product development and actually figuring out the features that matter at a zoomed out level maybe not twitch specifically but just like how do you make these prioritizations so that that uh saying that comes to mind for me for this is um planning is a central plans are useless I think it's like a like Eisenhower or something some general um and it's totally correct because you if you don't make a plan where you're gonna get you know from point A to point B you're gonna miss so much stuff and you're you're gonna fail but once you make the plan no plans Surprise contact with the Enemy as soon as you start executing you realize like oh that didn't actually work this didn't actually work you have to improvise um talking to customers is essential um but the results of talking to customers is useless like it is the talking to the customers and they're really paying attention that informs your intuition that then lets you design the product you're not you're not delegating the mistake people make is they're trying to delegate product design to the customer and if they were so good at the product design they would be starting the company they would be the product manager they're not good at product design they know what problems they're running into they know the realities of their business and what you need to do is you need to understand them incredibly deeply not get them to do your job for you so almost the inverse of the manager situation don't delegate that to them um the this that's one One Direction you make the mistake is you try to delegate the product design to the customer no no you designed the product you talk to them you get you understand them and you use that to design the product the other side of it is you have this you come up with a great idea and you got to talk to customers to validate it validate is the like uh and I hear we'll talk about validating product ideas I uh I instantly know that there's a there's a problem because once you've had the idea no amount of talking to any number of customers will change how good of an idea that was it is exactly as good as it is the moment you have it and you can have a thousand customers tell you it's a great idea it was or was not and it will a great idea and it will or not have the impact before they've told you or after if a thousand years will tell you it's a bad idea same thing doesn't change anything and unless you are actually throwing away lots of ideas customers tell you they don't like them you're not having any impact at all and to the most and I've also just I that never happens every now and then people valid ideas and throw them away and actually what's even funnier there is it's usually when they do actually throw them away those videos might have been good the customers are just bad product designers and they don't realize that actually is a good solution to their problem but um the uh the main thing is the order is wrong you need to talk to customers uh first and then have your great ideas and so like there's this uh idea from the rationalist community online that I really like holding off and proposing Solutions um holding off on proposing Solutions is one of the key product design skills because you have to like know there's a problem and not come up with the solution long enough to actually go like gather some data and think about it um because once you come up with a solution you're it's actually your brain is like attached to it it's really hard to like let go some people can do it but it's surprisingly difficult there's a uh I want to talk about more about decisioning and one as it relates to um when you had four co-founders Kyle Michael Justin and yourself in a room you mentioned there was ultimately kind of a war of attrition on decisioning that it was whoever was Last Man Standing kind of got the got the vote we had a real hybrid model so we would we would have uh these like epic debates which were mostly useless and sometimes we would do the thing of like the person who like had the most stamina but often it was just like there's this other thing happening which like whoever just did stuff they got their way too like uh that was Kyle's main move but actually it was my move and a lot Michael's moves too like we'd argue about it and then we'd uh we'd go off and like all just like do stuff and actually I would say in retrospect it's a little bit of the planning as essential plans are useless thing we could the arguments could have been less contentious and we could have probably shortened the debates but they were valuable and we would we would game it out we would argue it out extensively we would hear from all the sides and the person who's in charge of that would go make a decision to make stuff happen and actually that's actually in retrospective relatively functional process um because there was a distribution of authority and accountability although it it maybe was an unpleasant it was an unpleasant but kind of effective approach as I would say it was like a high unpleasant medium effective I wouldn't really recommend it as a you can do better there's more Pareto optimal outcomes but like but it's better than the stuff that's ineffective,Emmett Shear on the Future of AI and YC Days with Sam Altman - 011,111,113
101,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Remote work doesn't work,4132,4669,one one question on uh I guess just because it's been a big debate uh of late is the remote work thing and now having lived through Justin TV and twitch and seeing all the discourse online you're uh you're working with Y combinator now like what's your what's your perspective on remote work and decisioning that specific question I guess you had tweeted out my observation you can build a two by two for workplace environments remote local and hierarchical versus egalitarian there is an island of stability out and deep and remote and hierarchical uh what did you mean by that and what's your perspective on remote so the island of stability is an idea from like the periodic table where like as hell elements get heavier and heavier they become unstable but there's a theorize we've yet to synthesize in one of these elements as a theorized island of stability way out at like a very high Neutron proton count where suddenly the elements get stable again for a little bit um uh but there's like but in the in-between options are all bad um and are unstable and intensely radioactive um and I I've seen I I know a couple companies that are very effective remote and what differs the way they differ from the traditional Valley culture is it's not uh distribution of authority everybody like you know is a autonomy everyone figure out what to do on their own it's like the Army like there's there's delegation there's delegation of authority but it's not this delegation of authority and distribution of authority are not the same and uh they are all over auditing and follow-up and interactions and documentation and you have to document your work and you submit it and you can you can build a mechanism and a culture that works remote um but the valley approach I think I think of it kind of economically in its most extreme of like the Google Vibe of like yeah like no real accountability like directly like because we don't really know what's good and like go experiment and try stuff and like maybe it'll work and like no real urgency to like launch stuff fast that that does not work remote it works okay in person because there's sort of this weird implicit social pressure thing that gets used in place of official uh methods to like manage stuff but once you take that away and you don't have the official thing either it's just like bad uh and so there's the silica the valley culture works there's a reason why like Silicon Valley's taken over like you know it didn't evolve for no reason but you can't take that culture and Transplant it into a remote work environment and expect it to work um and I think we'll slowly sort of like you'll start to see models that'll be like oh you're on this month you're on this model you're under that model um uh and I think that that will uh uh I think that will that'll be good for us actually well they'll if you really care a lot about remote you'll go work at a company there's a remote culture that works um I actually like love the in-office thing uh I love being able to be remote sometimes but like uh I just I enjoy being around my co-workers I enjoy people to see them in the face I like the organic parts of it um but I know not everybody does um and I'm really happy that they get to go work at a remote environment that hopefully has a functional not dysfunctional remote culture goals and okrs versus intentions you prefer to focus on intentions than okrs rather than being tied to specific goals can you elaborate on that I mean that's that's when we're on a personal basis than on a on a corporate basis like at a corporate basis goals are very important like you need to actually setting goals as a I could probably give an entire hour-long talk on like setting goals and the trade-offs in various ways to do that but uh at an individual level um setting goals for yourself can be fun but ultimately goals are usually a way to coerce yourself or whip yourself into doing a thing you don't want to do you should have set a goal and like that's why you need them in the corporate environment people have to sometimes be a little bit coerced into doing something that's less than the thing that they want to actually do but it's important to the company even though it's not the most interesting exciting to them personally and you're trying to artificial excitement and fun it can be inspiring it could be exciting to try to get a goal to it doesn't have to be always a whip there's a carrot there too but even carrots are coercion like ultimately it let the intrinsic thing itself you're you're using reward or punishment like operative conditioning on a dog to get it to Tran to do what you want there's a place for that but like I try not to like coerce myself all the time um and I find that an over focus on goals like goals are set with your intellectual like top-down analytical mind and if you're good at setting them you can be effective with it but like your internet you're like top analytical mind is only so good and it's easy to set a goal that is not it it's like related to what you wanted to do but not quite what you actually wanted to do and then that's very dangerous because it you can spend years sometimes pursuing this and achieving and succeeding only to find out oops I didn't actually that goal was that was like sort of right but it missed this important thing and now I'm like I have to actually go way back and start over um and so I mostly think about it in terms of like uh you know I really concrete one like don't set a weight loss goal uh set an intention about uh wanting to be strong and healthy and acting in accordance with that um and it's creating an infrastructure to support you and doing that well um this is the book I think about on this a lot is the score takes care of itself Yeah by Walsh that's that's got books great and I think it really it's for those people from Reddit it's the idea of like what you the score in the game is literally irrelevant if he's a football coach and you ignore the score score like screw that like uh uh what's important is did you practice did you do the practice you needed to do or did you everyone show up to practice they give they give practice they're all and if they if you they did that you're gonna win or lose the game that will that that will take care of itself and I find that's very true for for goal setting um the goals tend to focus you on the score or creating a score you're you're going to create a way to score yourself and that's not necessarily the best way to do it I I don't know if we have an hour to do the uh the setting goals for a company thing but is there a short primer on how you think about setting goals or okrs for companies uh uh no like uh let me see some if I have any maxims about goals um obviously it sounds like focusing on inputs versus outputs is uh no you need a blend of input and output goals because the output goals keep you honest and the info goals keep you focused uh I'd say uh there's a lot of different kinds of goals you said for different kinds of reasons and the most important thing is to be kind of clear with yourself like one goal except for twitch was like I want us to have a million people earning money on Twitch in five years 2016. I set a goal we had like at the time I think 15 000 people earning money ten thousand something like that and I was like it was ten thousand and I said go I wanna have a million people earning money on Twitch by the end of 2021 and we hit it because of covet we hit it actually early 2021 which I'm very proud of we I call that weirdly accurate but I didn't really call it I I uh God I sound like a hippie like I manifested it right like by just saying that to the company and having people think about that uh it generated things like other people inventing things like the affiliate program well if we're gonna have a million people earning money we have to have a really easy way for people to sign up to earn money and so we created the affiliate program and like and then a bunch of other things like that right um and that's one kind of goal that's like a five-year goal that's like I had no idea how to achieve it was very output oriented and supposed to be sort of an inspirational goal but then you also set goals like you know this quarter we need to get our uh you know the number of TPS reports filed up by 17 and those goals can be very effective and important also um you know and I I think it's like uh uh it's the they're useful for different things they're motivating to different people in different contexts and you should just not confuse each of the goals for each other,Emmett Shear on the Future of AI and YC Days with Sam Altman - 012,112,114
102,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Mentors and coaching,4669,5304,just not confuse each of the goals for each other I know you're a big believer in coaches and that a coach will pay for itself as soon as you can afford it I guess maybe can you tell the story of your coach and then ultimately why you think it's important how you go about assessing a mentor or a coach along the way yeah I think I got very lucky so you know with my coach uh he was a introduced to me by Stuart alsop uh who's our on our board um the yeah and it was our first in you know PC investment um and uh when I gave CEO shortly after he uh he recommended I start working with Jose um and I said all right I'll give that a try um and I was honestly a little skeptical at first but I was like uh I guess I took getting better at being a CEO very seriously and if this thing could help I wanted to give it a give it a shot um and Yoshi has had taken his previous company public who had been a CEO before and then he pivoted into wanting to do coaching and I feel very lucky I got someone who had that level of background because I think it's very hard to coach a CEO if you haven't lived seen been part of that I don't know if you had to be a CEO exactly but if you haven't at least been an executive or you know a board member or something if you haven't been part of those decisions it's hard to give to do good coaching um coaching is valuable because it it creates a space where you you uh get to have someone who has no incentive other than helping you hold a mirror up for you to what you're saying what you're doing what your goals are and helping you and what your intentions are and helping you live more in accordance with those and notice what you're doing better or doing worse and there's a bit of coaching where it's like literally just advice like uh uh you know how what what are the what are tactical tricks to handle this given situation but honestly like that's like the the less important side of coaching and mentorship the more important side is like you're almost uh you're learning the questions they ask and you're learning to ask their questions of yourself consistently um the you know you have a coach that is you you have to really come I think the the way to think about coaching is sort of like uh fine selecting a good coach a little bit so I think a therapist reflecting a good um you know a good partner or something on a on a startup a good co-founder you know it's working when you have more like positive surprises than negative ones you know where it's working when you feel you're growing you're you know the things you learned there you find yourself repeating to other people um and uh and so they definitely recommend it but I also think that a lot of coaches aren't that good and so like there's a process of looking and selecting how do you go about assessing the relationship and make it is it just are you getting I'm a worst person to ask I got very lucky number one great one I'm one for one I don't know what to tell you like it's like people who like met their sweetheart yeah high school sweetheart and they were like the person for them and it's just it's great and they're deliriously happy with that person 30 years later it's like that's amazing for you but like not an actionable plan for anyone else yeah hard to explain yeah so actually I have very you need I have great advice about how to go about learning how to build product because I sucked at it I I didn't get it right a bunch of times you know I actually think later Drew I love Drew but I bet he gives bad advice to startups about how to go about finding your very first glimmer as a product Market fit because he happened to build the right thing true house your house yeah yeah Drew managed to build the right product just like directly he just directly somehow I think he's very obviously very good at product but like he just did it oh yeah nobody does that like that's what I feel about Twitter today yeah you can't you can't advise anyone if you've done like that's you don't know how to figure how to get from failure to success because you never spend any time in Failure um Drew has a lot of great advice about scaling a company uh because he's had to struggle with that I want to go back in time so you were a part of the first original YC batch which also was Steve Huffman Alexis Ohanian of Reddit uh Sam Altman as well as you look back on the success of the group of people uh that came out of that how much do you put that just the people innately the insights and Lessons Learned right place right time some other variable I'm not thinking about I mean I certainly think you know I have to we all have to credit Paul with like being an incredible mentor to all of us um and like really I do think you know leveling us all up a little bit like I I absolutely don't think it's a coincidence but I also think this is true of YC today it's true of every Elite University selection effect system it's like this massive part of it and it's not just it's not that so much that YC was some brilliant filter or that filtered out the best people it's like who thought in 2005 remember in 2005 when I graduated it was the smallest computer science class that Yale had graduated in years because uh the tech that the tech was over anyone who is like reading Paul Graham's essays thought it was a good idea to start a startup in 2005. I thought it was a good idea to go take a risk on a something like YC like that seemed like that it drew in people who turned it would turn out that that was a a good like trap for people who turned out to be uh ambitious and talented and a good vintage to be starting companies it was great it was very counter uh narrative which is always the best time to be starting right we're starting at the bottom and so yeah YC classes of like 050607 particularly like before the density is a little higher I don't know that's actually true like that is pretty high later too but like it certainly pulls in a lot of uh it pulled in a lot of uh very talented people because uh it was it wasn't prestigious yet so you only did it if you were like really committed to wanting to do a startup because even though that sounded like even though everyone was telling me it was a bad idea to do that um and I think that's like uh uh that's actually all still the best reason to start a startup um fortunately starting a startup is so painful that mostly that people still do it unlike a lot of things that people do for fun uh because they're supposed to starting a startup is so painful that most people just select out correctly because unless you like buying your paying economy size they don't really recommend it what what did what did Paul do uniquely well in those early days I mean the whole institution he started has obviously lasted and but Paul's supervised superpower by far his superpower like he's Paul's very smart good a lot of things but it's by far superpower is you somehow walk away it's everyone this is me but also Justin and also everyone else I talked to another YC you somehow walk away from the conversation with Paul believing we are the shed we are smart capable and we're going to take over the world and the only thing standing between us and taking over the world is we have to like go grind even harder we but like Paul believes in us but it's not what's happening is like Paul believes in you so you believe in yourself but it's not actually but somehow he doesn't he doesn't feel like Paul Believes In Me it feels like Paul just sees that I'm obviously extremely talented and capable and like I mean I'm just I'm the only issue maybe I'm aiming too small should we be aiming bigger but it feels like he's just noticed this fact that now you can see two and that is such a gift to be able to give to people because I think I think that did what it gave us was the most important thing that startup Founders need and I think that anyone doing anything ambitious needs which is the like gumption to keep going when things aren't looking so hot Midway through uh and Paul really convey like Paul's that's that Paul gives that to people I think that's like that is by far his his greatest Talent which is which is funny because he has a lot of other talents but like that's uh that's a big one so so it was you Alexis uh uh Steve Huffman Paul like was it Sam Altman was it obvious at that point in time that something special was in the room or was it hey uh we're in the room with that no no absolutely not I mean yes in the sense that like I thought everyone seemed smart and good but and and like are you coming from Yale and I'm sure yeah there are good people there yeah and I'd I'd been in several Elite programs in my life where there's other smart people uh I I had that sense more like joining Tech in San Francisco at that time like in Boston and Cambridge not really when we moved to San Francisco it was like oh oh I'm in the middle of like Ground Zero of something um and it actually wasn't the first batch the first batch was just like some weird summer program but when the batches kept getting bigger and I would meet the new Founders and like more companies and they had a building as much everyone's in the and like it's like oh oh where this is something the thing is happening around me that became very clear about like 2000 by early 2007 absolutely but at first no it was just just it just seemed like a thing it was cool I was really grateful I got to do got to raise money and like start a company didn't have to go like live with my parents to like start in their basement was it obvious Sam was going to be the leader of the Free World uh no it was obvious Sam was an incredible deals guy like he was he was somehow convincing the uh phone companies to give his startup that like didn't really have a product like deals I still don't know how he did that and like he's doing the same thing like convincing Microsoft to like you know buy his fusions power like Sam is still an incredible deals guy obviously uh but that that was that that was the only obvious thing about Sam at the time was like that he was he was ambitious but we had most of us were pretty ambitious and he was great at great great deals guy,Emmett Shear on the Future of AI and YC Days with Sam Altman - 013,113,115
103,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Ambitions of the founder,5304,5813,great great deals guy you said something interesting once the only reason you ask about Tam is to understand the Ambitions of the founder uh can you tell me why why you think that why you approach investing in that way or why you approach thinking about Tam in that way what was the Tam of uh live reality television uh whatever Justin was willing to pay you at the time yeah yeah like nothing basically um and yet that's pretty live streaming shows a pretty pretty big business um I ask people about Tam a lot and I'll push them on it but it's like it's because what you're trying to figure out is do they have a story they might have a lot of holes in it and like we're like questionable assumptions we're like they're trying to build something really great and big or do they have a story that's like oh yeah yeah I well no don't worry about the Tam we're just gonna flip this to Google uh in two years once we hire the talent like that's a really bad sign companies don't get built by people who who are looking for the exit you want people who are looking to build something big something great um because they even built something medium-sized you have to be trying to build something really great and if you're if you don't if you're not if you're aiming for the aqua hire you're probably not even gonna get that um and uh and so it's a very important question but the point is not to like that you can actually analyze the Tam per se and I think that's a little different for more established Founders especially it's a little different in places where there's more you need more commitment up front more Capital up front to get going because the other thing I look for is that there's Founders who there's this trap you can wind up in where you you start is there is the if you're building for a customer that doesn't exist or like that hates buying software or that like doesn't want to doesn't want any anything to do with this the whole thing that's bad so like I don't care if your thing credibly you know isn't is every developer on Earth is actually going to use your product but building software for developers is a good plan there are a lot of developers and they spend a lot of money on software and so even if you're wrong about your exact thing it's going to be okay like the software developer Market is big if you're building market for software that's like really for public school librarians that's the only people and you're like and I'm pushing you I'm like dude isn't this bigger than that you're like no no it's for public school librarians I'm really worried for you because even if you like crush it that's not actually they don't nothing wrong with Librarians they don't even buy they don't like buying software in the first place and there aren't that many of them so like that's maybe not a great idea and so like and and Founders can kind of get trapped in these ideas where they're building okay businesses targeted at very small groups of people who don't want to buy their product um and so it's more about your just in theory if you executed like crazy and figured out six things that we didn't even think of in this room on a pivoted idea that's like halfway only halfway connected to your idea would that be a big business no okay maybe maybe we should change ideas yeah uh there's two options for companies you can either be generally you can either be early or late uh and I I get the feeling being early is probably better how do you think about timing a timing a market and making sure that you're there to ride that wave and the differences between being too early and too late you're every startup that wins pretty much not everyone but like 98 uh is too early because if you're not too early you're usually too late because if you think about it you're trying to like there there is some optimal day like literally there's probably a day where like start if you start the company on this day everything will be available as you need it there will be sufficient Bama through your idea they'll be sufficient this to you know the modernization techniques will work the distribution will be in place but if you start it on that day someone else when everything actually becomes available everyone else someone else will have started it a year earlier stupidly just by random chance because every idea is getting started over and over again and they will be in motion and they will have infrastructure and they will have a product built and suddenly their product will be working and you will be too late and so what you're trying to do is like so if you think about it for Justin TV we were too early bandwidth was too expensive to make our business work uh and almost honestly most people couldn't even watch didn't have good Advantage but we've been watched reasonably quality live video and the video ad Market didn't exist we didn't have a business it was important but we but but there were other startups that tried to compete with us several that started later and they just got destroyed because we'd built Global live video infrastructure for the past four years and they hadn't and so you know from when the start gun went off on like oh no no the business works now there's just how do you catch up with someone with a four-year Head Start still led by the founder CEO still like grinding super hard trying to make it work like you're just screwed I mean so a big part of it is if you if you have faith that your thing is going to work and the pieces are going to come into place the question for startups is how do you survive that's why you read so much about like YC's advice to like be a cockroach like don't don't don't be a uh uh graceful swallow be a cockroach because you're usually too early and you just have to survive and survive Until It's a combination of like you find the secret thing but often it's actually like you didn't change anything the market caught up with the future that you saw that you can't ever time exactly right and suddenly your thing that wasn't good is good and boom like Airbnb is a good example of that actually I think like they they had a product and yes they did iterate and they did figure stuff out but like a big part of it was like people got comfortable with the idea of like host listing stuff on the internet and renting through uh renting other people's stuff uh and and the business that didn't work and wasn't good became a business that did work and became it was good and I think that's a it's a very common pattern talking about cockroaches and you guys were certainly that with Justin TV um frugality uh being something that's important but maybe not a virtue uh that that being able to use frugality I I've written down here I assume you said it because it's in quotes reality isn't a virtue in itself speed is how do you think about the balance of of speed and frugality and the ability to execute the other thing I think you said the blast radius of startups is low which is to your advantage as a startup yeah I mean frugality for uh Amazon one of Amazon's core uh like like Amazon Amazon's leadership principles the Alp um one of them is frugality um it's about how leaders should be frugal and inside of Amazon there's a saying about like yeah yeah frugality but don't you don't are going to avoid for pity um because frugality and frippidity are near and far aligned um because you need to be willing to spend money and to invest for something that's important to do something but also but wasting money is very bad and the difference between one man's investment is one man's waste is another man's investment and vice versa the real thing that kills startups in terms of spending money is usually not overspending on like bandwidth or like server capacity or like even marketing obviously you don't run marketing it doesn't work it's inefficient but like what uh what kill startups is hiring you hire too many people and that does kill you because you have this High burn rate and eventually run out of money the burn rate kills you but it also kills you because more people means more slow um and speed is the essence and like hiring is not in alignment with speed um speed isn't alignment with speed and so there is a point where you do need to hire more people to go faster but like it is later than people think it's less hiring than people think and so I think a lot of the advice to startups about for frugality is really better framed as advice about you need to be going fast and that means don't grow your team too much that creates a lot of drag,Emmett Shear on the Future of AI and YC Days with Sam Altman - 014,114,116
104,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Back to YC,5813,6285,your team too much that creates a lot of drag what what Drew you back to YC as a as a partner now like what was you were at Amazon for a long time what made you want to I've always wanted to be a YC partner like I uh I don't know if it's like my forever job but like I watch my friends go through it I love advising startups and I was like oh that looks fun I want to try that and so I just always had the intention of going and doing YC for a year as a partner um just honestly because it's like a life experience and I'm really enjoying it so far I think there's a chance I do it more ongoing but I also I'm enjoying like having a little bit more freedom post stopping SEO and the job it's a real job at YC like you know it's not not like being a CEO of 2000 company but like it's a job um and so we'll see if it's like the long-term thing but no I just always wanted to and then once I was free I was like oh well there's a badge coming up I guess I could go do that and so there's more of almost more of an Impulse thing less than like a I have some like plan of wanting to do it what do you think's allowed the institution to survive in the way that that it has uh it's obviously scaled quite a bit over the years um I mean YC is like a uh a little like a university yeah uh and like universities it has a very strong Network effect the smart people want to hang out with some other smart people and want to be at the place where the other smart people are and that like creates a positive feedback cycle that allows it to scale and grow it makes it hard to Harvard was the first University in America weirdly still the most prestigious that's weird like I think Yale was the second University founded in America second or third most prestigious like that's weird uh uh I think that exact same Dynamic happens here and like it's not a and unlike the university is YC is designed to Ben can scale up a lot more and so there's the second place gets is a little bit smaller um because people can move around more much more easily now right if uh if Harvard had been founded in the Nero or anyone could have flown anywhere and had an attitude of scale as much as possible it's not clear there is University 234 immediately um and I think YC has been sustained because it's always had a long-term mindset like I remember Paul telling me like saying like you're two or three and our biggest advantage in the long run is going to be very eliminate the Alumni network that's what YC is really building and they call them alumni right it's not that's that's a different attitude um and uh and so he was thinking that far ahead at the beginning I had another question about YC and I oh what do you what do you think the biggest constraint on startups today the reason that we don't have uh 10 more stripes or a hundred more Twitches or 30 more uh airbnbs is it because is it Founders is it ideas is it capital sufficiently talented people who who want to take on the pain of starting a company yeah like pretty much exclusively there's lots of capital available um I will say there's there's a uh there's definitely uh of missing there's like a few missing gears in that process of like there are people who should get access to the capital and if they did they would be successful and so there isn't enough Capital there's plenty of capital there's more than enough Capital but like there's lots of people who think they should have the capital many of them are wrong some of them are right and the system is like pretty efficient but there's definitely some number of people who like just don't get funded and they should because they because they're bad at pitching because they don't present because because on the surface it looks looks wrong because because they they can't show you that they should get the funding um and I think that's a shame and like one of YC's mission is to find and fund those people we can't do it perfectly were you were you bad at pitching or did you just hate it I was okay yeah that wasn't great um I would be a lot better at pitching now and I think I would enjoy it more uh weirdly at twitch after we sold I I get to practice sales a lot around with the game game companies and and advertising and I and also I started investing and I uh there's no way to do this but like if I could give every founder I should probably have them like go be an investor for two years and then go pitch their company because it's suddenly I'm like oh I see what I was doing wrong like now I know I've sitting on the other side of the table I'm like oh oh no wonder that person like passed my startup I was I was explaining it all wrong I didn't understand their incentives I didn't understand how they saw the world and so the most important thing you can do is a like I wonder if this would work if I was at if I was to go back and give earlier Emmett advice like how to pitch better what I would tell them to do is like go find some VC who you're friends with who like sees good companies and be like I will give dude some due diligence for you for free if you let me sit in on like you know a month of pitches and hear the discussion afterwards because I think I would have been like oh I know how to I know how to do this I would have figured out how to pattern match like the problem is like starter Founders have only usually have only seen their pitch how do you get good at something that you've only you've only seen your performance at you don't know what good looks like um and I think that's the so yeah that's if that's what I would that's what I would tell my prayers have to go to the information's gotten so much better uh about VC I mean one of the things I I want to do is try to sort of give elements of how I think or rethink and operating but the pitches it's it's almost a voodoo science in and of itself Are there specific things that you learned in pitching the EA or whatever that you would there's um there's things you can write down that people have written down a thousand times like you know templates and stuff sure they're all wrong and they're all bad actually you temp pitch templates whether they're pitches for VC money or like product things templates are the enemy there's certain like aspects you can do but every pitch is unique because it wasn't unique you don't have any unique Insight like your company is unique there's a there are generalizable principles and rules but there's a template you can go off of and so and yet you know it when you see it and there's like there's there is there's uh I think pitching is one of those things where it's uh it's very much medicine you know that idea the idea of like a skill that you learn through organic doing there is no training for pitching that would be more effective than just like watching a bunch of pitches and then the honest brutal discussion afterwards about like whether they think that's a good idea or not and participating in that um and unfortunately I don't think anyone's brave enough to put there's no I don't know how the you want to create a piece of content that would be like incredibly valuable to founder Center to pitch record three months of every every partner pitch and every pre-partner pitch for every company and the discussion afterwards and the decision and publish that and like then edit it down so like you cut out the dead time edit it down and just have people watch that as a as a uh as a uh as a course that would work like people would get a lot better at pitching um unfortunately I don't think the company should like that I think the partners would be really be awkward but like you need to create some system like that um because that's the that's the only way to get it's like trying to learn how to like cook from a cookbook you learn to cook in a kitchen you don't learn to pitch watching a YouTube video you learn to pitch they're from a PowerPoint you learn to pitch in the in the pitching room that's interesting,Emmett Shear on the Future of AI and YC Days with Sam Altman - 015,115,117
105,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,AI Concerns,6285,7408,pitch in the in the pitching room that's interesting uh shifting gears artificial intelligence um I get the feeling you might have some concerns about the path we're on and some elements of how things are playing out is that I mean so I guess I have I have a very specific concern about AI like generally I'm very Pro technology and I really believe in the sort of like uh the upside's usually outweigh the downsides everything's technology can be misused you should usually wait and you should wait until you've eventually as we understand it better you want to put in regulations but like regulating early is usually a mistake when you do do regulation you want to be making regulations that are about reducing risk and and for uh Innovation and I actually uh authorizing more Innovation Innovation is usually good for us um I sort of have a very very high level like syllogism about like AI that I've come to believe I think is like correct it's like if we can build if you consider intelligence to be the capacity to solve problems from a given set of resources to a given goal we are building things that are more and more intelligent like that we've built an intelligence it's kind of amazing actually we built something like definitively it may not be the smartest intelligence but it is unintelligence it can solve problems so we've built something that can solve problems like arbitrary problems from arbitrary resources and make arbitrary plans at some point as it gets better the kinds of problems that we'll be able to install solve will include uh programming chip design Material Science uh power production um all the things you would need to design an artificial intelligence at that point you'll be able to point the thing we've built back at itself and this will happen before you get that point with humans in the loop it already is happening with humans in the loop but that Loop will get Tighter and Tighter and Tighter and faster and faster and faster until it can fully self-improve itself at which point it will get very fast very quickly and a thing that is very very very smart and you generate something that is very good very intelligent and by intelligent again I mean this capacity to solve problems there's people there's lots of ways people that work it's a it's an English word which means it has no one definition but I have that in that sense I mean that kind of intelligence and that kind of intelligence is just an intrinsically very dangerous thing because intelligence is power human beings are the dominant form of life on this planet pretty much entirely because we're more and smarter than the other creatures when was the last time intelligence came into you know entity at this level right and within humans I think people get confused between these two different things within the band of human intelligence intelligence is not the most important thing humans have a lot of other attributes but intelligence is a very important thing for people we have a lot of other gorillas are more intimidating as a species than we are but Thrillers did not build this studio and humans are like humans are dangerous as hunters for example because we're smart we do things like you know crowd The Woolly Mammoth off cliffs and set traps and like uh uh human be we have we have whites in our irises it's the exact balance uh of white to color uh is what communicates what we're looking at other primates have black eyes because they don't want the other people other creatures to see them so try not to give away information we have the white uh around the IRS because it lets you tell what other people are looking at you basically have this Eerie ability to do it you know exactly what other people are looking at imagine hunting these creatures are hunting you and you see one of them and one of them can like indicates that they want to cross the way that they just saw a deer by like looking at the deer and there's no Sound Exchange at all it's just purely this like it's like you're gonna be pretty smart to do like big very strong theory of mind to do that if you build something that is a lot smarter than us not like somewhat smarter again within humans the smartest people don't rule the Earth obviously um but like it's much smarter than we are as we are than like dogs right like a a big jump that thing is intrinsically pretty dangerous because if it gets set on a goal that isn't that that uh like the the the instrumentals first instrumental instrumental convergence the first instrumental step towards achieving that goal is we'll go first step one if this is easy for you because you're really just that smart well step one just like take over the planet right like it's like then I just have control over everything and then and then step two solve my goal can you define instrumental convergence for those that didn't sit through three and a half hours of me talking to Ellie yeah so it's real convergence is this idea that like uh uh often when you're trying to achieve a goal step one is to achieve a instrumental goal along the way it's like if you want to like for example like uh uh paper clips is the one yeah yeah so uh no I'm thinking more like an instrumental instrumental convergence I'm trying to give an example from where it happens to people in their people's lives um oh in chess um your actual goal is to Checkmate them but like along the way to check making them check making them most the time one of the things you want to do is take their pieces now you could not take their pieces there's probably I bet a really a really good chess player against the kind of mediocre one could Checkmate them without taking any pieces just like trapping their their but like that's like even more impressive but like generally speaking if you're just trying to win a game of chess taking their Queen taking their pawns taking taking their pieces is a good idea it like makes it easier to Checkmate them and so I can predict something about almost anyone any good chess player they'll take a bunch of their pieces they'll take the other person's Queen eventually probably um in the same way you can predict uh that corporations uh uh that want to expand into a new market there's an instrumental goals like step one they'll probably hire people in that market it's like just predictable and in general if you want to accomplish most goals like big goals Step One is like accumulate a lot of money and power like if you have if you have a goal of uh changing the world it accumulating money and power or accumulating followers who like care listen to you and will do what you say these things are like Obviously good first steps even if you don't even if you didn't even know what the next goal was they would be good first steps and if you know what it is they're definitely good for steps step one if you can achieve it along the way to to achieving any Big Goal yeah paper clips is the traditional one um is first just if you if you can pull this off which like you can't humans can't do this we don't think of goals like this because we're not capable enough but if you could pull it off step one would be like well first I'm just gonna like literally just make sure I have total control over everything at all times and then step two I'll like do whatever it is step two do the thing easy I already have control over everything no one can stop me I have accessed all the resources simple um and I think people just don't it's hard it's hard to people don't imagine sufficiently capable as sufficiently capable um and then so then and some people some people have this idea like oh what if we just don't give it goals well first of all we are giving it goals people are already building agents but let's just say we didn't and so you ask this Oracle what's the best way for me to accomplish goal X and it knows if it takes you literally um and it and it it actually answers your question correctly the answer to your question will will be a thing that causes you to bootstrap an AI that then takes over the world and accomplishes the goal that's the most reliable way to accomplish that goal now I just laid out a chain of argument with a lot of if this then this if this then this if this then this uh I know Eliza thinks that like we're all doomed for sure um I buy his Doom argument I buy the chain and the logic I just think that like first of all I'm less optimistic with the current set of technology is going to get to self bootstrapping super intelligence I'm less optimistic than he is that we could officer pessimist whatever unless I'm not sure than he is that when it hits that self bootstrapping step that uh that process will be fast and that we will that there aren't important new discoveries that will take a long time on top of that that we haven't found I'm less sure that uh uh this was the idea of alignment getting it you could make the IAI such that it wants the same things we want and then if you ask it to do the thing it won't go and do horrible things because it's not dumb and it's aligned and if it wants the same things it knows what you mean it's smart and it has it has aligned goals hooray like that'll work great I'm Le uh Elijah thinks that we're like just alignments this incredibly hard problem that like is almost unsolvable more doomed I'm like not so sure I think it's a more solvable problem than he thinks it is for a variety of reasons you know just it would take too long to let go into but like my my belief is that it's easier um and so uh as a result like my P Doom my probability of Doom is like my bid ask spread and that's pretty high because I have a lot of uncertainty but I would say it's like between like five and fifty so there's a wide spread which I think Paul Cristiano Paul Christiano who handled uh you know a lot of the stuff within open AI I think said 25 to 50. it seems like if you if you talk to most AI yeah researchers there's some preponderance of people that's that percentage that should cause you to your pants but it's human level Extinction I think yeah because no it's not a human level Extinction it's such extincting humans is bad enough it's like potential destruction of all value in the light code like like not just for us but for any species caught in the wake of the explosion like uh it's like a universe destroying bomb like it's really if if if if if if it's really bad it's bad in a way that's like makes global warming like not a problem it's bad in a way that makes normal kinds of bad not uh that's not normally I'm like yeah we'll just roll the dice it's fine we'll figure it out later yeah no no like this should go this is not a figured out later thing this is like a big problem southern Manhattan Miami might go underwater like okay okay but this is we're talking about and so why do you think I mean I I it's like someone figured out how to invent it a way to make like 10x more powerful Fusion Bond bombs out of like sand and bleach that like could anyone could do at home yeah um it's terrifying and and I've had enough time with it now that I can laugh about it when I first realized it was heart stopping when was that uh probably like it's it was it was a dinner I went to before when opening I was just we had basically right after attention is all you need had been written like and they sort of realized the scaling laws were there and I went to a dinner and someone was there and they were talking about it and they were like I think we were were uh we actually might be on the path to build a general AI attention you all you need was 2018. yeah I think early 2017 2017. um and Google paper that yeah and uh and I I heard about the problem I thought about it I just had I'd like I'd been like the the AI Doom thing seems plausible whatever like I don't think an AI is coming anytime soon so I'm just not gonna think about it that hard yet uh and then I was like oh maybe and then I started thinking about it harder and then I was like oh oh oh um and so uh uh I guess the the pro I believe the proper response is like unfortunately this isn't the kind of thing where uh we can stop forever and it unfortunately is also the kind of thing where like more time is good like I'm actually I'm okay with stretching the time a little bit but like ultimately to solve the problem um I think this is one of my biggest points of Divergence with uh yukowski um he is a mathematician philosopher you know decision theorist by training I am an engineer am I everything I've ever learned about engineering is the only way you will ever get something that works if you need to work on the first try is to build lots of prototypes and models at the smaller scale and practice and practice and practice and practice and and trot build start building the thing but like smaller and if there is a world where we survive it is and everything goes wrong where we build an AI That's smarter than humans and we survive it it's going to be because we built smaller AIS in that and we actually had lots of as many people smart people as we can working on that and taking the problem seriously and so I'm I'm generally I'm in favor of trying to create accounts I've got a fire alarm where we were like like maybe not AI is bigger than x at some point like trying to like create a and I actually think there's good reason to believe like nobody wants to end the world and this argument is not that hard to understand and so actually there's good there's a good option for international cooperation and like treaties about some sort of you know the AI Test Ban 3D about not bigger than x at some point I don't think I don't think we're actually at the point where it needs to be not bigger than our the current AIS are just not that smart yet but I think we should be moving towards creating that kind of a some kind of soft I don't we have to figure out what that looks like because it's it's trickier to set that than it setting that rule is way harder than it looks writing good policy is hard we should be thinking about it now I just think I don't think we're ready for it yet but in the meantime on these smaller models we it is good that lots of people are around with them it's good that we have more and more people trying to figure out how they work and trying to figure out how you can make them do things how figuring out how to make them do bad things the best way to figure out how to stop a big AI from doing bad things we'll have the best way as much it is true there's a bunch of failure modes for super intelligent AIS that don't exist unless super intelligence AIS and we better not bet on oh don't worry it works it works in the dumb ones it'll work no no that's not how it works you can't do that but like we will figure it we are figuring out more and more about the principles of how it works and if if we survive it's going to be because that process produces a a good generalized understanding a good generalized model of how these kinds of predictive models which I think includes humans as we are some kind of very complex predictive model with other stuff too but like that's a big part of what a human is we'll understand how those work at a deeper level we'll have some of some science of it actually and that's what we need we need a science of AIS right now we have an engineering of a highs and no science of AIS and we need to get use the engineering to bootstrap ourselves into a science of AIS before we build the super intelligent AI so that it doesn't tell us all why do you think people are struggling with the discourse around this like very smart people seem to abject it's very very obviously mood affiliation rules everything around me people don't make decisions on a reasoning basis I mean myself included most of the time I happen to like I happen to find this problem interesting and compelling on its own so I spent a lot of time like digging into the arguments themselves because I was like drawn to it but like most of the time I make decisions the same way as everyone else does the person pitches me on pitching me on the Flat Earth thing who's saying that like a bunch of things I don't really like listen to them I just like I can they say some things and it triggers like oh you're part of that tribe those people generally I don't think think very clearly I'm just gonna discount everything you're saying and ignore you yeah Robin Hansen talks about 9 11 people and it's like you don't argue with them you just sort of move on yeah conversation and the AI people sound like uh religious nuts who are telling you about the end of the Doomsday into the world and it sadly pattern matches really nicely right like the AI is like the you know it's like the Antichrist it's coming and you know if what if we're good the good AI will come and save us from that if it's like it sounds like like uh Christian rapturists yes um Last of Us yeah it uh unfortunately um reasoning from fictional evidence is it doesn't work and that mood affiliation reminds me of this is not an argument and the Earth is not round because the flat earthers sound crazy the Earth is round because you can demonstrably see that the Earth is round and go measure that yourself and it's true and if you make decisions based on anyone who's telling you that doing X will unleash a force which is going to kill us all they sound like a bunch of uh crazy religious people because it's never happened before it's never happened before guaranteed guaranteed the first time that's true we're all dead because your algorithm always predicts the same thing you know the thing you're going through in your head always predicts the same outcome for anyone who is predicting Doom from creating a powerful force Beyond human ability now it is true you should be skeptical in general when someone proposes that because there are a lot there are Infinity examples from the past three you know six thousand years of history of people predicting that falsely about things and and people people made imaginary cures for medicine medicinal cures for like tinctures that were supposed to cure you for a very very long time and then we made one that worked and and you just can't reason that way sometimes sometimes it's new sometimes it's not like before usually it's like before and sometimes it's not and I I am personally convinced this time it is not like before and I encourage everyone who's in that mode like the main thing that what I want you to pay attention to is listen to me like people like me listen to people like uh even utkowski who I disagree with when the amount of Doom but we're like pro-cryonics Pro technology like technology is going to fix all our problems crazies like if I if I have a if I have a defect it's that I am too Pro technology I want too little regulation like are you doing Ionix by the way I have not signed up yet I really should I've been it's one of those things on my to-do list I'm failing the rationality test yes um uh but uh but you're a techno Optimist,Emmett Shear on the Future of AI and YC Days with Sam Altman - 016,116,118
106,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Being a techno optimist,7408,8264,failing the rationality test yes um uh but uh but you're a techno Optimist I'm a techno Optimist and like and most of you should notice that the people the people who are affected by this particular one are not like the people who generally predict we're not Paul airlock is full of on like oh the Doom is coming the population bomb he is wrong and weirdly so and refuses to learn from his lesson that he's wrong again he's wrong over and over again this is not like that like I am not panicked about global warming I think I think alarming is a real thing I don't want to say like I don't want anyone to walk away thinking like I think isn't real but like I believe I I believe in technology the engineers will figure it out don't worry it's gonna be fine I'm not quite sure um it might have some well depending on whether we play to have the political to get around to it quickly or not it might have more or less dire consequences but like we will figure it out eventually it will be fine we will figure we will get this I do and it's okay so here here I am being the guy who's like the Techno Optimist and I am like no no the AI I think that this thing though actually maybe may be a problem and I think if you're if you are rejecting it because we sound like a bunch of crazy it's just notice that like at least a some number of people who are worried about this are not are on your I'm on your team I'm on the Techno Optimus team really think and it and it's not it's not obvious why it's true it takes a good deal of Engagement with the material to see why it's true because at first at first it seems like it shouldn't be that big of a deal it shouldn't be that big of a problem but then the more you dig in then we realize like oh well actually but wait a second and it is and so like I just encourage people to engage with the technical merits of the argument and I always welcome people to come to to you know ask questions but also like if you want to debate no no I have an idea we can align it this way do this and it will create a you know it will create the AI that like cares about things we care about we want your ideas like that's great let's have an argument about that self-improvement won't work okay if no because like people have said self-improvement will work in the past and it never did then I don't want to hear it because whatever it's not a real argument but if you have an actual like argument about how the current learning techniques won't work for that for some reason engineering reason absolutely let's talk about it well I'm glad we spent all this time building your credibility as a normal person just go off the rails here but but seriously I mean it's uh it is something that people are like genuinely concerned about and there's no incentive you have Mark Andreessen published something recently about like Ai and he had a bunch of different points against it but the AI killing everyone point was the first one and I I think it went back to a lot of discourse around people that profit from this is their business and this is how they make money and therefore that's what why they're intended to communicate in this way you are not I I as far as I know your business is not in Dooms uh doomsday scenario planning around AI I have no Financial stake in in either doomsday or planning around AI or the other weird like double think 40 chess thing people impute to uh it is like oh we're actually trying to build up open Ai and anthropic as being and Google as being like super powerful and it's on ego thing about making talking about how amazingly great this stuff is so we can like raise more money for opening and Regulatory capture yeah yeah and like like I don't own any equity in any of these things you're not trying to help your 2006 batch made 2005 batchmates no he's he's Sam Hunt's really good at raising money like he does not need my help yes I promise yeah what would you recommend to people I mean uh if they are curious about all of this stuff and like I I when people ask me about it because I've had Eliezer on they're like so now what and they're like ah you know call your Congressman I don't know so there's sort of two paths that you can where you can contribute if you care about this problem one is um if you're Technical and you're technically minded um and you think that you uh but you want to work on it like go learn how the AIS work and work on interpretability and work on uh courage ability and can you define those terms so durability is like understanding what's going on inside the AI which we are very little understanding right of right now yeah and ability is is how do you make a decision making thing that is willing to be corrected by others but he's actually and that's sort of that's where Eliza has a lot to say about this he's kind of a decision Theory thing how do you get it to sort of like believe that even though everything it knows seems like this is the best way of doing it the other or telling me that's not right and so I'm open to being corrected by their point of view how do you make it sort of this how do you how do you give something humility almost in a way right um and and we have no idea to do that either um we've made some we've had more progress on interpretability than courage ability but made a little bit of progress on both and we're making more on interpretability and what the my real pitch for this is it's actually really interesting there's just stuff lying around that no one's checked no one's tried it's like uh when the microscope was invented and suddenly like you could just make a scientific career by like pointing microscopes at things and like there were like lots of things to point the microscope out that no one had ever looked at before we have a mind you can go look at and examine and experiment with I've never had one of those before and so there's a bunch of like interesting kind of like scientific workers I would encourage people to go do scientific work on AIS go take the AIS other people have built and then the engineering and try to see what science you can do to them particularly around interpreting what's going on inside them or how you get them to accept corrective feedback um if you're more if you're not technically minded and you don't feel like learning how to like actually build a Transformer um they you have there's less obvious how you contribute directly but uh I think mostly like uh helping correct the debate and correct the tone of the discussion because this like the super intelligent AI must kill it might kill everyone thing gets wrapped together with a bunch of other concerns about AI that are real they're their things but they're like normal concerns like will it just cause discrimination job loss what's called job loss like I know that let that stuff's all like a thing but like honestly on those things I sort of feel about it the way I feel about most regulation we're like you know like it's a little early probably there probably will be a good regulation to write we don't know what it looks like the area is evolving so fast very hard to write good good regulation don't let that get confused um job loss doesn't kill everyone yeah so there's so there's there's AI ethics and there's then there's AI not kill everyone ism and the ad not kill everyone thing is like we need the AI to not kill everyone don't let people mush it together when you see hear people conflating the two correct the record uh because they're just not the same idea um and they're the same kind of problem and they won't be solved with the same kind of tools and uh uh the danger is that those two get sort of wrapped together into some thing again gets rejected by people who just like well I don't who rejects the safety AI safety aif ethics stuff that they it's like well this doesn't really make sense which I kind of agree with a lot of the people who say like most that stuff kind of doesn't make sense to me I could see a theory maybe some of it um we will throw out the the good part with the bad and a lot of that other stuff is true of almost any technology right yeah that stuff is just general it's General technological like and that's my thing of why I think Silicon Valley in particular has has struggled to talk about this and I don't even know what my fully formed opinions are on this stuff but it's that for a long time Silicon Valley has been accused of all the negative externalities that are going to come from uh whatever the technology is that we've integrated around and so everyone's very used to being defensive about like oh job loss yeah everyone's on the on the defense against anything critical like sort of broadly critical of new technologies because it pattern matches to a bunch of people who go around throwing up check throwing chaff in the air attacking everything all the time even when it doesn't really make any sense and so it's easy to slot it into that the slot the whole thing into that thing and just you know throw it away and I just think like uh the uh there's there's a piece of that that is different than the normal than the norm it's hard Verity mood affiliation drives most of our decision making and it's hard to get people to notice that I have three more questions how are you on uh time can we keep going time is it we're almost at 30 so we're way overtime um what's my calendar here um oh I have to meet my friend uh let me just text my friend real quick okay um these won't be nearly as long as AI ethics all right all right um there's a graphic you tweeted out about income that you said you keep handy for dinner parties uh why do you keep that graphic we'll show it on screen as we're talking but why do you find that graphic so interesting um it shows I think on a more optimistic note than the one we were just on there yeah yeah it it shows so first of all the fundamentally what it shows is in general people getting much richer households getting much richer and specific um house you know the the almost all of it is the the bulk of this curve moving down and that the big spike on the right isn't a small number of people getting much wealthier it's a lot more people making 200 that more than 200 000 a year like eight percent of the population and for people listening it's like a distribution since 1970 or something of income and maybe a Family household income in real terms and in real terms and so it slides from like a you know sliding kind of smiling graph to being much more on the 200k plus side of the axis and but the other thing that happens is it goes from being a pretty defined like bell curve to like this weird like almost like exponential decay shape like there's still a lump in the bell curve area but like it's less and less a bell curve and more and more flat like an exponential decay curve of people making every bucket people makes there are slightly fewer people in the bucket but actually there's about as many people in forty thousand 30 to 40 as there are in 50 to 60 as there are 60 to 70 as they're already 70 to 80. it's actually the main story is the flattening of the distribution and what that flattening means is it used to be there was a normal income uh normal household income was 30 35 000 a year and if you're within and some people made a little bit more and some people made a little bit less but for the most part everyone was in the mode they had they made there was a real modal income the there was a most common income and most people made relatively close almost everyone made relatively close to that that amount of money and what has been happening and then you had some number of very rich people and like medium rich people but like the bulk of society was in that thing and what's been happening is uh there isn't really a norm anymore we've destroyed the norm and instead there's just some people made a lot more money some people made a lot a lot more money some people made a medium amount more money some people made a little more money but like in general uh everyone's do making like uh everyone's making more but the the there isn't a it's hard everyone at can now look to the person the left and the person on the right there's like lots of people making a lot more money than me and lots of people making a lot less money than me for most people and that feels bad it feels like inequality it also is inequality actually but but everyone's getting richer which is good and yet somehow it's this flattening that creates a sense of like being left behind and I don't quite know what to do about it because it's a it's a very optimistic story on the one hand of like American households generally getting much wealthier and many many more people having access to more uh resources which is good that's the good that we'd like that um there is a slight Spike at the very bottom of people with almost nothing I think that's that is that's also an issue um and it's not to say there aren't any issues in our society like you know obviously Healthcare education um housing all cost you much but like the the real uh underlying thing that bothers people that that on the other side of like this this flattening out I'm not quite sure how you how do you how do you how do you fix that without like making everyone really poor there's an easy way to fix it make everybody poor go back everyone take it take a bunch of money for everybody it's easy to break things it's easy to make people more poor um and so I think a big part of what happens there is as it's you get spread out more you have the the housing in the middle um the people in the middle can't afford things that get bit if there's a good that gets where there's a limited Supply and it's getting bit up and you make you're making more money on that particular good you are screwed and that sounds a lot like education and housing to me um and so I think that's a that's a big part of why it feels so bad for people um which is why I care so much about getting more housing made making education more available I think these these things are very important,Emmett Shear on the Future of AI and YC Days with Sam Altman - 017,117,119
107,The Logan Bartlett Show,Emmett Shear on the Future of AI and YC Days with Sam Altman,Turning 40,8264,8588,education more available I think these these things are very important you recently turned 40 and you did a Tim Urban ish layout of your life and time spent doing that exercise was it on a weekly basis yeah every week yeah every week did an internalization of turning 40 and laying graphically out how you spent your time did were there any takeaways from from that that you I mean I kind of I knew the story already yeah um it was just kind of cool to see it all laid out um and uh I guess my biggest takeaway was man I spent a long time in school yeah it's been a really long time like maybe longer than I needed to like maybe I could have started working earlier in retrospect yeah uh that was not that many surprises it was fun though it's like fun to encourage everyone to do it it's fun to lay your life out like that and take a look at it you live in San Francisco are you optimistic about San Francisco absolutely the city is uh doing doing much better than it was during covet Cove is very hard in San Francisco I think um we have problems um you know the fentanyl crisis is really real people are dying in the streets it's like not okay um but we also have a lot of really big advantages um and there's a lot of good happening in the city too and I'm uh I think I'm most encouraged by the number of people I know who uh have built their careers and their lives here and who are committed to the city and who are not leaving and they're planning to do the work to try to make San Francisco better place where it's you know the core things that everybody cares about right like you shouldn't be people who don't have anywhere to live dying on the streets uh there shouldn't be obvious rampant crime people being victimized by that uh left and right uh there shouldn't be housing shouldn't be so expensive that like you have to be rich to have access to it um there should be better Transit options for people um they don't they don't want them stuck in traffic like the park we have more and better Parks like I don't know there's none of these things are controversial and in fact I wouldn't even say any of these things are uh things that we want we don't all we don't have wide agreement that we should go do um it's a matter of I think that I pretty strongly disagree with many of the people who but what will cause the outcome but I think the good news is we're all aligned on wanting the same things and so it's just a matter of getting the right plan in place for the right people executing it um and these kinds of changes are not uh startups are all about speed um politics and government policy is all about consistency and so that's that's sort of where my focus is just you know keep keep turning the wheel last one before I let you go what's what's a piece of conventional Silicon Valley wisdom that you wholeheartedly disagree with or think people get wrong the the most of the wholehearted wisdom in Silicon Valley is 100 correct for certain people in certain context and 100 wrong for other people in other contexts and trying to come up with a piece of advice that's always wrong is like pretty hard um because like that's you know advice is over generalization and so it almost always is true in some context um what about proportionally gets said yeah a lot and like is is should be said relatively is usually not right um oh I I know uh startups hire a players startups should not be trying to hire any players I mean you can be trying if you want you're not you're lying to yourself uh you don't know what an a player looks like if you did they wouldn't work for you um you might be an a player because you like the founding team like might be a players potentially you hope you hope you are that's your goal but like and you might hire some a players because you attract them and they like believe in the mission I'm not saying you never hire an a player but they think you're gonna hire all a players I mean come on no you're not that's just like a lie and and that's okay and this idea that you can separate people and sort of a players and B players anyways I know what people mean by it it's like you want to hire like the most talented people in the entire world but actually what you really want to hire are talented people who will work hard and believe in the mission and who want to work for you and like there are a lot of those people and you should go hire them um and uh and that that that idea is like a sort of toxic Elite idea that like I think I just think is ridiculous like that's you can't do it and you shouldn't try most of the time and then you have companies um that are on the very very Cutting Edge of Technology where you better hire pretty much all a players you're gonna die and you have to hire very slowly Renaissance Technologies it's basically Renaissance technology is a hedge fund that actually hires only like you have to have published multiple groundbreaking math papers before they'll like they'll hire you like in statistics like they're not joking around they're actually hiring a players it might be the most interesting business in the world or at least on a very short list yeah yeah and but like you're not rentac yeah don't kick yourself yeah yeah good well Emmett thanks for doing this thank you everyone [Music] [Applause] foreign,Emmett Shear on the Future of AI and YC Days with Sam Altman - 018,118,
108,a16z,Politics & the Future of Tech,Teaser,0,38,you know ignoring Tech uh is kind of no longer an option in the government big Tech has been pres in Washington but big Tech's interests are not only very different than kind of startup Innovation innovators interests but we think also kind of Divergent from America's interest as a whole you know if America is going to be America in the next 100 years we have to get this right y [Music],Politics & the Future of Tech - 001,,1
109,a16z,Politics & the Future of Tech,Intro,38,149,[Music] welcome back everybody uh we are very excited for this episode um we are going to be discussing a lot of hot topics the uh theme of today's show um is Tech and policy and politics um and so um there's you know just a tremendous amount of heat right now uh in the tech World about politics there's a tremendous amount of heat in the political world about tech um and then we as a firm actually I and Ben both Ben and I as individuals have been spending a lot more time in in in policy and politics circles over the last several months um and we as a firm have a much bigger push here than we used to which which Ben will describe um in a moment uh but we're going to go into quite a bit of detail um the big disclaimer that we want to provide Upfront for this is that uh we are a nonpartisan firm um we are 100% focused on Tech politics and policy um we today in this uh episode are going to be describing um uh a fair number of topics some of which which involve partisan politics um our goal um is to describe anything that is partisan as accurately as possible um and to kind of you know try to be as sort of fair-minded and representing multiple points of view as we can be we are we are going to try very hard to not take any sort of personal political uh partisan position so ple please if you could get grant us uh some generosity of interpretation in what we say we are trying to describe and explain um as opposed to uh advocate for anything specifically partisan uh we advocate for policy topics we we did not Advocate uh for for for other partisan topics um and actually so yeah on that theme Ben um could you uh you know we we wrote a little while ago you wrote a blog post uh about and published about the our firm's engagement in politics and policy uh we sort of laid out our goals and and and then also how we're going about it and we're actually we are actually quite transparent about this um and so hoping maybe as an introduction for people who haven't seen that if you could walk through uh what you know what our plan of strategy is and how we think about this yeah kind of starts with you know,Politics & the Future of Tech - 002,0,2
110,a16z,Politics & the Future of Tech,Why get involved in politics now?,149,701,of strategy is and how we think about this yeah kind of starts with you know why now you know why why get involved in politics now um you know historically Tech has been a little involved in politics but it's been relatively obscure issues H1B visas um stock option accounting uh carried interest things like that H but now the issues are much more mainstream and it turns out that you know for most of kind of the software Industries like um you know Washington just hasn't been that interested in Tech or in regulating tech for the most part uh but starting kind of in the mid 2000s uh as self rate the world and Tech started to invade all aspects of life um you know ignoring Tech uh is kind of no longer an option in the government uh and that you know they've seen it impact elections and education and everything and they you know uh I think policy makers really want to get in front of it is term that we hear a lot you know we need to be in front of these things this time not like last time when we were behind the curve and so uh Tech really needs a voice and in particular little Tech needs a voice so big Tech has been president in Washington um but big Tech's interests are not only very different than kind of startup Innovation innovators interest but we think also kind of Divergent from America's interest as a whole and so that just makes it like quite imperative for us to be involved not only to represent the startup Community uh but also to kind of get to the right answer for the country um and you know for the country this is we think a mission critical effort because if you look at the last century of the world and you say okay why was America strong and why was basically any country significant in terms of military power economic power cultural power in the last hundred years and it was really those countries that got to the industrial revolution first and exploited it best and now at the dawn of the kind of Information Age Revolution um we need to be there and not fall behind not lose kind of our Innovative Edge and that's all really up for grabs um and really the kind of biggest way America would lose it because we're still like you know from a you know capitalistic system standpoint from an education standpoint and so forth from a talent standpoint we're extremely strong and should be a great innovator but the thing that would stop that would be kind of bad or misguided regulation that forces Innovation elsewhere out of the country and kind of prevents us ourselves America and the American government from adopting these Technologies as well um and kind of driving that you know driving the things that would uh kind of make us you know bad on Tech regulation our first really you know big Tech whose goal is not to drive Innovation or make America strong but to preserve their Monopoly you know we've seen that uh act out now and AI in a really spectacular way where big Tech has pushed for the Banning of open source for safety reasons safety reasons now you can't find anybody who's been in the computer industry who can tell you that any open- source project is less safe U from a first of all from a hacking standpoint uh you know and you talk about things like prompt injection and and then new attacks and so forth you would much more trust an open source solution for that kind of thing but also for you know a lot of the concerns of the US government about like you know copyright where does this technology come from and so forth not only should the source code be open but the data should probably also be open as well so we know what these things were trained on and you know and that's also for figuring out what their biases and so forth how can you know if it's a black box so this idea that you know close Source would be safer and big Tech actually got this you know some of this language into the Biden Administration executive order like literally on you know like under the guise of safety to protect themselves you know against competition is really really scary and so that's kind of a big driver the other kind of related driver is I think um this combination of big Tech pushing for fake safetyism to preserve their Monopoly and then rather thin understanding of how the Technologies work in the federal government and so without somebody kind of Bridging the education Gap uh they're very very you know we are as a country very vulnerable to these bad ideas um and we also think it's a just a critical point in Technology's history to get it right because if you think about what's possible with AI so many of our countries kind of biggest challenges are very solvable now you know things like education uh better and more equal Health Care um you know just thinning out the bureaucracy that we've built and making the government easier to deal with particularly you know for kind of underprivileged people trying to get into business and do things and become entrepreneurs all these things are made much much better by AI similarly you know crypto is really our best answer for you know kind of getting back to delivering the internet back to the people and away from the large Tech monopolies it is the one technology that can really do that and you know if we don't do that you know over the next five years these monopolies are going to get much much stronger probably some of them will be stronger than the US government itself uh and we have this technology that can help us you know get to this you know dream of stakeholder capitalism and and participation for all economically um and we could undermine the whole thing with poor regulation and then finally you know in the area of biology which is we're at an amazing point in that if you look at the kind of history of of uh biology you know we've never had a language just much like we never had a language to describe physics for for a thousand years we didn't have a language to really model biology till now the language for physics was calculus the language for biology is AI and so we have the opportunity to cure a whole host of things we could never you know touched before as well as kind of address populations that we never even like any testing on before and and and always put in danger and you know this again you have big Pharma uh whose interest is in preserving the existing system because it kind of locks out all the kind of innovative competition and so for all those reasons we've like massively committed the flag and the firm to being involved in politics so you've been spending a tremendous amount of time in Washington I've been spending time in Washington um you know many of our other part parners like Chris Dixon and BJ pondi have been spending time in Washington we have like real actual kind of a lobbying capability within the firm and we'll talk about that some more but call it government Affairs but you know they're registered lobbyists and they're working um to to kind of work with the government and and set up the right meetings and help us get our message across and then we're deploying a you know a really significant amount of money um to basically pushing Innovation forward making you know getting to the right regulation on Tech um that preserves America's strength and we are not only committed to doing that this year but for the next decade and so this is a big effort for us and we thought it'd be a good idea to uh talk about it on the podcast yeah thank you that was great and then yeah the key point there at the end is worth double underlining I think which is long-term commitment um you know there there there have been times um with with tech specific Al where there have been people who have kind of cannonballed their way onto the political scene you know with you know large you know kind of bomb you know sort of money bombs um and then you know and maybe they were just single issue or whatever but they're in and out or you know they're just in and out they just you know it was just like they thought they could have short-term impact they they you know then two years later they're gone um we're we're thinking about that very differently yeah and you know and that's why I brought up you know the historical lens we really think that you know if America is going to be America in the next hundred years we have to get this right y good okay we're going to unpack a lot of what you talked about and go into more detail about it so I will get going on the questions which again thank you everybody for submitting questions um on X um we have a great lineup uh today so,Politics & the Future of Tech - 003,1,3
111,a16z,Politics & the Future of Tech,Big Tech's involvement in political process,701,1294,everybody for submitting questions um on X um we have a great lineup uh today so I'm going to combine a bunch of these questions because there there were some themes so jar uh Jared asks why has Tech been so reluctant to engage in the political process both at the local and National level until now and then Kate asks interestingly the opposite question um which I I I find this Chuck's position very interesting because this gets to the nature of kind of how how we've got how we've gotten to where we've gotten to Kate asked Tech leaders have spent hundreds of millions lobbying in DC right the opposite Point um in your opinion has it worked and what should we be doing differently as an industry when it comes to working with EC and so I wanted to kind of juxtapose these two questions because I actually think they're they're they're both true and the the way that they're both true is that there is no single Tech yeah um right B to your point there there is no single Tech and so and maybe once upon a time there was you know and I I I would say you know my my involvement in political you know kind of uh efforts and and you know in this domain started you know 30 years ago um so I've seen a lot of the evolution over the last three decades and I was you know I was in the room for the founding of TechNet which is one of the sort of Legacy you know kind of John John Chambers and John door so um you know so I've kind of seen a lot of twists and turns on this over the last 30 years and I I think you know the way I would describe it um is you know this you know as been said you know so one is look there there there just you know was there a diversion was there was there a sort of a distinction and a real difference of view between big Tech and little tech 20 30 years ago yes there was um it's much wider now um I would say that that that whole thing has really gapped out um you know the the big Tech even big you probably remember big tech companies in the 80s and 90s often actually didn't really do much in politics yeah um you know they didn't really have you know probably like most famously Microsoft probably you know Microsoft probably would everybody at Microsoft during that period would probably say they had underinvested um you know kind of given what happened with the antitrust case that unfolded yeah actually the one issue we were United on was the stock option accounting which um you know interestingly and we were against Warren Buffett and you know Warren Buffet was absolutely wrong on it and won um and it's actually very much strengthened Tech monopolies so it I think did the opposite of what you know people in certainly in Silicon Valley wanted and I think people you know in washingtoniana and America what have wanted was to you know make these monopolies so strong and using their Market cap to further strengthen their Monopoly uh because we move from stock options to you know it's too esoteric to get into here but let's just say trust me it was bad yes yes it was very good for big companies very bad for startups so um yeah and actually that's another thing that actually happened in the 90s and 2000s is um so there's a fundamental characteristic of the tech industry and in particular Tech startups and Tech Founders um which which and Ben and I would include ourselves in that group which is we are idiosyncratic disagreeable eastic people um and so it like there is no Tech startup Association like every every Industry Group in the world in the country has like an association that has like offices at DC and lobbyists and like major Financial Firepower and you you know these undern names like you know the MPAA in the music industry or the movie industry and the Raa and the in the record industry and the National Association of broadcasters and you know the national Oil and Gas Association and so forth so like every other industry has these these these groups that basically where where basically the industry participants come together and agree a policy agenda they hire lobbyists and they put a lot of money behind it um the the tech uh industry just we've just never been good at actually especially the startups we've never been good at agreeing on a common platform um and in fact you know Ben you just mentioned the stock option accounting thing like that that's actually that's my view of what happened at TechNet uh which is TechNet was an attempt to actually get like the startup Founders and the new you know kind of the new Dynamic tech companies together but the problem was we all couldn't agree on anything other than b basically there were two issues we could agree on stock option expensing as an issue and we could agree on carried interest for Venture Capital firms is an issue yeah carried interest tax treatment and and so there were the basically what ended up happening was again my my view you know kind of TechNet early on got anchored on these I would say pretty esoteric uh accounting uh and financial issues um and just never had a view um on you know could could not come to agreement on on many other issues and I think a lot of attempts to coordinate Tech policy in the value had that characteristic and then look quite honestly you know the other side of it Ben you highlighted this but I want to really underline it it's just like look the world has changed and you know up until 2010 you know I would say up around until about 2010 I think you could argue that you know politics and Tech were just never that relevant to each other um you know for for the most part what tech companies did was they made tools um you know those tools got sold to customers they use them in different ways and so you know reg you know how do you regulate it you know database software or an operating system or a word processor or a router regulating a power drill or a hammer right yeah exactly right exactly like yeah what you know what are appropriate shovel regulations um and so it just wasn't that important and then you know look this is where I think Silicon Valley deserves you know kind of deserves its share of blame for anything whatever's gone wrong which is as a consequence I think we all just never actually thought it was that important uh you know to really explain what we were doing and it be really engaged in the process out there um and then you know look the other thing that happened was you know there was a love affair for a long time you know there was a view that you know tech there was just a view that like Tech startups are purely good for society Tech is purely good for society there were really no political implications to Tech um the way this actually continued interesting up through 2012 um you know people now know of all the headlines that you know social media is destroying democracy and you know all these things that kind of really you know kicked into gear after 2015 2016 um but you know even 2012 like the narrative you know social media had become very important actually in the 2012 election but the narrative in the in the Press was like almost uniformally positive you know it was very specifically that social media is protecting democracy by making sure that certain candidates get elected and then also by way Obama um you know there were literally headlines from you know very uh you know newspapers and magazines today that are very anti-tech that were very prot Tech at that point because the view is Tech helped Obama get reelected and then the other thing um was actually the Arab Spring you know there was this moment where it was like Tech is not only going to protect democracy in the US but it's going to protect democracy all over the world and you know Facebook Google were the Catalyst were at the time reviewed as the Catalyst for the Arab Spring which is going to of course bring a flowering of De democracy the Middle East that has it didn't work out that way by the way did not work out that way um and so so anyway the point is like it is relatively recent in the last 10 12 years that it's just sort of just like everything has just kind of come together and all of a sudden you know people in the in the in the in the policy Arena are very focused on tech people in the tech world have very strong policy politics opinions the media you know weighs in all the time um and and then by the way other this you know none of this is a us only phenomenon we'll talk about other countries later on but um there's also a global you know kind of thing you know these issues are playing out globally in many different ways uh I I guess I one thing I would add is like when I'm in you know I do a fair amount in DC on the on the on the non-political side and when I'm in you know meetings uh involving National Security or intelligence or uh you know civil policy of whatever kind it's it's striking um how many topics that you would not think are Tech topics and end up being Tech topics yeah um and so you know and it's just because like when the state exercises power now it does so through you know with technologically enabled means um and then and then when citizens you know basically resist the state or fight back against the state they do so with technologically enabled means and so they they're sort of you know there's sort of this you know sometimes say we're the dog that caught the bus on on this stuff right which is you know we we we all want a tech to be important in the world it turns out Tech is important important in the world and then it turns out the things that are important in the world end up being end up getting pulled into politics yeah yeah I think that's right you know on the second part of the question I think that's a you know like why is techan so ineffective despite pouring all the money in and I think there are like a few kind of important issues around that one is you know really arrogance in that um I think you know we and Tech and a lot of the people went in are like oh we're the good guys we're for the good and everybody will love us when we get there and we can just push our agenda on you know kind of on the policy makers without really putting in the time and the work to to understand um the issues and the things that you know you face as somebody in Congress or somebody in the White House um in trying to figure out what the right policy is and I think that you know we uh are coming at that from kind of our cultural value which is we take a long view of relationships we try never to be transactional and I think that's especially important on policy because these things are massively complex and so we understand our issues and our needs and um but we have to take the time to understand you know the issues of the policy ERS and make sure that you know we work with them to come up with a solution that is viable for everyone and so I think that's thing one I think Texas has been very bad on that and the second one is I think that uh you know they've been partisan where like it's been like not necessary or not even smart to be partisan so people have come in with whatever like political B mostly kind of democrat uh Democratic Party uh that they have and like okay we're going to go in um without understanding you and only work with Democrats because we're Democrats and this kind of thing and I think you know our approach is look we are here to represent Tech um we want to work with policy makers on both sides of the aisle we want to do what's best for America we think that uh if we can describe that correctly then then we'll get support from both sides and and that's just a really different approach so hopefully hopefully that's right and Hope we can make progress okay good so let's go to the um next question so um this is again a,Politics & the Future of Tech - 004,2,4
112,a16z,Politics & the Future of Tech,Q: In what ways do you see the relationship between Silicon Valley and D.C. evolving?,1294,1567,progress okay good so let's go to the um next question so um this is again a two-part question so Sheen asks um in what ways do you see the relationship between Silicon Valley and DC evolving in coming years um particularly in light of recent regulatory efforts targeting Tech Giants and we'll we'll talk about we'll talk about Tik Tok later on but um you know there's been obviously big uh you know there's big flasho kind of events happening right now by the way also for people haven't seen you know the doj just filed a massive antitrust lawsuit against Apple yeah um you know that the the the the the uh the uh the you know the the tech topics are very hot right now in DC so how do we see the Rel by the way that's an inter that one's an interesting one of the one of the things that I've talked about um which is you you know a lot of little Tech I think is is very much in alignment with some of the things that um the FTC is doing but probably we would do it in a very different against a different kind of set of uh practices and behaviors of some of the tech monopolies and and you know it it just shows why like more conversation is important on these things because you know what we think is the kind of abuse of the Monopoly and you know what the lawsuit is I would say are not exactly the same thing well let's talk about that let's talk about that for a moment because this is a good case study of of kind of the Dynamics here so the traditional kind of free market libertarian view you know is is sort of very you know uh very critical of antitrust you know theory in general and it's certainly very critical of the you know current prevailing antitrust theories um you know which are kind of more expansive and aggressive than the ones of of the last 50 years um you know as shown in things like the Apple the Apple lawsuit and and many other actions recently um and so you know there there's sort of a you know for people in business there's sort of a reflexive view that you know basically says businesses should be allowed to operate um but then very specifically there's this view or you know there are certainly people who have this view that basically says you know any additional involvement of the political machine especially the sort of prosecutorial machine in Tech you know is invariably going to make everything worse in Tech and so yeah you know they sue Apple today and maybe you're happy because you don't like apple today because you know they abused your startup or whatever but like if they win against Apple they're just going to keep coming and coming and coming and do more and do do more and more of these the the opposing view the the opposite view would be the view that says no actually um the interest to your point the interest of big Tech and little Tech have actually really diverged um and that actually um if there is not actually strong vigorous investigation and enforcement and then ultimately things things like the Apple lawsuit um you know actually these these companies are going to get so powerful that they may be able to you know really seriously damage you know little Tech uh you know for for for for a very long time so maybe been talk a little bit about how you know kind of we we think through that kind of you know because we we even debate this inside our firm um but talk a little bit through about how how to how to process through that and then you know what you think and then also kind of where where you think those lines of argument are taking us yeah so look I definitely think and by the way right you know I mean I should f disclosure uh when we were at Netscape um we we were certainly on the side of little Tech against big Tech and you know Microsoft at that time had a 97% you know market share on desktop and you know it was very very difficult to innovate on the desktop it was just you know bad for Innovation to have them you know in that level of position of power and I think that's happened on the smartphone now you know particularly uh with apple I think the um kind of the Epic case and the Spotify cases are really great examples of that where you know I am I am Fielding you know product that's competitive with Spotify and um I am charging Spotify a 30% tax on their product like that seems unfair um just from like a consumer like like just just from the standpoint of the world and uh you know it does seem like it's you know using Monopoly power in a very aggressive way I think it's certainly against our interest and the interest of new companies uh for the monopolies to exploit their power to that degree you know like look when the government gets it involved it's not going to be like a clean surgical like okay here's exactly the change that need that's needed but I also think you know with these Global um businesses with tremendous lock in uh you know you just have to you know at least have the conversation and say okay what is this going to do for consumers if we let it run and uh and you know we need to represent that point of view I think from the from the kind of small Tech perspective yeah and the big tech,Politics & the Future of Tech - 005,3,5
113,a16z,Politics & the Future of Tech,Where we need regulation,1567,1778,think from the from the kind of small Tech perspective yeah and the big tech companies are certainly not doing us favors right now so they're certainly not acting in ways that are pro startups I think we could say as a general no no no no the opposite sure quite the opposite one one one of my one of my ideas I kick around a lot is it's it feels like it feels like companies it feels like any company is either too Scrappy too arrogant yeah um but never in the middle yeah yeah yeah like it's it's like people right you're either the underdog or you're the overdog and there's not not a lot of yeah not a lot of reasonable dogs exactly exactly so there's inherent tension there it seems very hard for these companies to reach a point of dominance and not figure out some way to abuse it um which is a very I also think you kind of touch on an important point which is you know we're you know in representing little Tech we're not not a pure libertarian anti-regulatory kind of you know Force here we think we need regulation in places we certainly need it um in drug development we certainly need regulation in uh crypto and financial services the financial services aspect of crypto um is very very important it's very important to the industry that you know to that would be strong in America with a proper kind of regulatory regime so we're not anti-regulation we're kind of Pro uh the kind of Regulation that will kind of make both Innovation strong and you know the Country Strong yeah and we should also say look like when we're advocating on behalf of little tech there obviously there's self-interest um you know kind of as a component of that because we're a venture capital firm and we back startups and and so there's obviously a straight financial interest there you know I will say we you know I think Ben you'd agree with me like we also feel like philosophically like this is a very uh sort of pro-america position very Pro consumer position and the reason for that is very straightforward um which is uh you know bet as you've said many times in the past uh the the motto of any Monopoly is um or what's what's the what's the what's the motto we don't care because we don't have to right exactly and so probably experienced if you've called customer service when one of these monopolies has uh you know kicked you off their platform yes exactly yes exactly um and so uh yeah it's just there is something in the nature of monopolies where they just they have a you know if if they no longer have to compete um and if they're no longer disciplined by the market um they they they basically go bad and then and then and then you know how do you prevent that from happening the way you prevent that is from forcing them to compete the way that they have to compete you know in some cases they compete with each other although often they collude with each other which is another thing uh you know Monopoly and cartel are kind of two sides of the same coin but you know really it's at least in the history of the tech industry it's really when they're faced with startup competition um you know when they've got you know when they've got a when the elephant has a terrier at his heels nipping at him you know taking increasingly big bites out of his foot like that that's big companies actually act and when they when they when when when they do new things um and so without healthy startup competition you know like let's say there are many sectors of the economy where it's just very clear now that there's not enough startup competition because the incumbents that everybody deals with on a daily basis are just practically intolerable um and it's not in anybody's interest ultimately um you know from from a national uh policy standpoint um you know for that to be the case you know that you know things things can get bad where it's to the benefit of the big companies to preserve those monopolies but very much not to anybody else's benef yeah no exactly exactly you know which is I would say such a big impetus behind our kind of political activity yeah that's right okay we'll keep going,Politics & the Future of Tech - 006,4,6
114,a16z,Politics & the Future of Tech,Determining where politicians stand with tech,1778,2373,behind our kind of political activity yeah that's right okay we'll keep going um so um in what ways do you okay now we're going to Future looking so in what ways do you see the relationship between Silicon Valley and DC evolving in the coming years um and then um specifically um and again what we're going to be not we're not going to be making partisan recommendations um here but you know there is an election coming up um and it is a big deal um and it's going to have you know both both what happens in the in the White House and what happens in the Congress is going to have big consequences for everything we've just been discussing um so how do we see the upcoming election affecting Tech policy um and Ben why don't you start yeah well I think there are you know several issues that end up being really important to kind of uh educate people on now because whatever you whatever platform you run on um you know as a congress person or as the president you want to kind of live up to that promise when you get elected and so a lot of these kind of positions that will persists over the next four years are going to be established now um I think in you know crypto in particular uh you know we've been very active on this because there's um you know we have a big uh kind of donation to something called the fair Shake pack which is kind of work on this and just identifying for kind of citizens like okay which politicians are on what side of these issues uh you know who are the kind of just Flatout anti- crypto anti- innovation anti- blockchain anti- decentralized technology um candidates and like let's at least know who they are so that we can tell them we don't like it and then you know tell all the kind of people who agree with us that that we don't like it um and you know a lot of it ends up being you know look we want the right regulation for crypto we've you know worked hard with policy makers to you know kind of help them formulate things that will you know prevent scams uh prevent nefarious uses of the technology for things like money laundering and so forth um and then enable uh the good companies the companies that are you know Pro consumer helping you own your own data and not have it own by some Monopoly Corporation who can exploit it or just lose it you know like get broken into and so you now have identity theft problems and so forth um that can kind of help uh kind of a fairer economy for Creative so that you know there's not a 99% Tech rate or take rate on you know things that you create on social media and these kinds of things uh and so um you know like it's just important to kind of I think educate the populace on whereever candidate stands on these issues and so we're really really focused on that and I think you know same true for AI same true for Bio i' also add um and talk a little bit more about the election in a moment but I'd also add like it's not actually the case that there's a single party in DC that's protecting a single party that's anti-tech um definitely not there's not and and by the way if that were the case it would make might make life a lot easier yes um uh but but but it's not the case um and I'll just give then I'll just give a thumbnail sketch of at least what I see when I'm in DC and see if you agree with this um so uh it would say the Democrats are sort of Democrats are much more fluent in Tech um and I think that has to do with um you know who their kind of Elites are um it has to do with this kind of very long-established revolving door and I I I mean that in both the positive and perjorative sense um between uh the tech companies and the Democratic party uh Democratic politicians political offices Congressional offices White House offices um there's just a lot more integration uh you know the big tech companies tend to be very democratic um which you see in all the donation numbers and voting numbers um and so there's just like there are just a lot more I would say Tech fluent Tech aware um uh Democrats especially in in powerful positions you know many of them have actually worked in tech companies just as an example the current white house chief of staff uh is you know former board member meta where I'm on the board um and so there's just you know there's there's a lot of sort of connective tissue uh uh between those you know look having said that you know the the current Democratic party and in particular you know certain of its more radical wings um you know have become extremely anti-tech um you know to the point of being arguably you know in some cases you know outright you know anti anti- bus anti capitalism and so there you know there there's there's a real kind of back and forth there you know Republicans um on the other hand like you know in theory and The Stereotype would have you believe you know Republicans are sort of inherently you know more Pro business and more Pro pro- free markets and should therefore be more more Pro Tech but I would say there again it's a mixed bag because number one a lot of Republicans just basically think of Silicon Valley that it's all Democrats uh um and so silicon Val all Democrats if we're Republicans that means they're de facto the enemy they they hate us they're trying to defeat us they're trying to defeat our policies um and so they must be the enemy and so there's a lot of you know I would say some combination of distrust and fear um and hate um you know kind of on that front you know and then again with with much less connective tissue you know there are many fewer you know Republican uh you know Executives at these companies which means there are many fewer Republican officials or staffers who have Tech experience and so there's there's a lot of mistrust and of course you know there have been flat flash point issues around this lately like social media censorship that have really exacerbated this this conflict um and then uh the other thing is um uh you know they're very serious polic policy disagreements and there again there are at least wings of the modern Republican party that are actually quite uh you know say uh sort of economically interventionist um and so you know you know the term of the moment is industrial policy yeah right which basically you know there there are Republicans who are very much in favor of a much more interven government approach towards dealing with business and particular dealing with tech um and so I guess say like there there's real There's real like this is not a this is not an either or thing like there there are real issues on both sides the way we think about that is therefore there's a real requirement to engage on both sides there's a real requirement Ben to your point to to educate on both sides um and there's a real uh you know you know if you're going to make any progress at Tech issues there's a real need to have a bip partis and approach here because you do have to actually work with both sides yeah I I think that's absolutely right and just to kind kind of um name names a little uh if you look at like the Democratic side you know you've got um people like Richie Torres out of the Bronx um and you know like by the way huge swath of the Congressional Black Caucus that sees wow crypto is a real opportunity um to equal the financial system which has you know historically been you know you know documented racist against uh kind of a lot of their constituents and then also you know the creatives which they represent a lot to kind of get a fair Shake um and then on the other hand you have Elizabeth Warren who has taken a very totalitarian view of the financial system and is moving to consolidate everything in the hands of you know a very small number of banks um and basically control who can participate and who cannot uh in in finance uh so you know these are just very very different views out of the same party and I think that you know we need to just make the specific issues really really clear yeah and the same thing we could you know spend a long time also naming names on the Republican side so yes um yes which we do later but um so um yeah well I actually should do it right now just to make sure that we're Fair on this um you know there there are there are Republican you know look there are Republicans who are like full-on Pro free market you know um you know very much Pro you know are very opposed to all current government efforts to you know intervene in markets like a and crypto by the way many of those same Republicans are also very Pro are also very negative any any antitrust action they're very ideologically opposed to antitrust and so they would also be opposed to things like the Apple lawsuit uh that a lot of startup Founders might actually like um and then on the other on the flip side you have folks uh like Josh Josh Holly for example that are I would say quite vocally um U say I rate at Silicon Valley and and I you know very in favor of much more government intervention and control you know I think a Hol Administration just as an example would be extremely interventionist um in um in in in Silicon Valley um and would be very you know kind of very pro- industrial policy very much trying to but both you know sort of set goals and and sort of have government management of more of tech but also uh much more dramatic action against you know at least perceived a real real enemy so it's the same same same kind of mix spe um yeah so so anyway that I wanted to go through that though this is kind of that's kind of the long whining answer to the question of how will the upcoming election AFF Tech policy um which is uh you know look there you know there there are real issues with the Biden Administration in particular with the with the agencies and and with some of the Affiliated Senators has been just described so you know there there are certainly issues where um you know that you know the agencies you know under under under the Trump Administration the agencies would be headed by very different kinds of people uh having said that it's not that you know it's not that a trump presidency would necessarily be a clean win um you know and there are many people in sort of that Wing who might be hostile in by the way in in in in in different ways or or actually might be hostile in some cases in in the in the same ways yeah and by the way you know Trump is himself has been quite the moving Target on it um you know he was very he tried to ban Tik Tok and now he's very Pro Tik Tock um you know he has been you know negative on AI who's originally negative on crypto now positive on crypto so you know it it's it's complex and you know which is why I think the foundation of all of this is you know education and we you know why we're spending so much time in Washington and so forth is to make sure that you know we communicate all that we know about technology so that at least these decisions are highly informed that the politicians make good okay so moving forward so uh uh three,Politics & the Future of Tech - 007,5,7
115,a16z,Politics & the Future of Tech,Future regulation and potential for destruction,2373,2539,informed that the politicians make good okay so moving forward so uh uh three part three three questions in one so Alex asks um as Tech regulation becomes more and more popular within Congress which is happening um do you anticipate a lowering in general of the rate of innovation within the industry um number two Tyler asks what is a key policy initiative that if pass in the next decade could bolster the US for a century um and then Elliot Parker asks what's one regulation that if removed would have the biggest positive impact economic growth yeah so I think that b if you disagree with this I I don't know that there's a single regulation um or a single law or a single issue um you know there are certainly I mean there are certainly individual laws or regulations that are important um but I I think the Thematic thing is a much bigger problem or much bigger the Thematic thing is the thing that matters the things that are coming are much more serious than the things that have been I think that's correct yeah oh okay we'll talk about that yeah go ahead yeah I mean so you know if you look at the current state of Regulation you know if it stayed here um there's not anything that like we really feel like a burning desire to remove it in the same way that things that are on the table could be extremely destructive and basically you know look if we ban large language Mo or large models in general or we you know force them to go through some kind of you know reg government approval um or if we ban open source technology um you know that would have just a devastating it would basically take America out of the AI game um and you know make us extremely vulnerable from a military standpoint make us extremely vulnerable uh from a you know technology standpoint in general and so you know that you know that that's devastating similarly you if we don't get um kind of proper regulation around crypto the trust in um the system and the business model is gonna fade uh or is going to kind of be in Jeopardy and that it's not going to be the best place in the world to build crypto companies um and blockchain companies which would be a real shame you know the uh kind of analog would be the kind of creation of the SEC you know after the Great Depression which really helped put trust into the US uh Capital markets and I think that you know trust into the blockchain system as a way to kind of invest participate be a consumer be an entrepreneur are really really important and necessary and very important to get those right okay and then speaking okay let's move straight into the specific issues then more so um expand on that so,Politics & the Future of Tech - 008,6,8
116,a16z,Politics & the Future of Tech,Intersection of regulation and technology,2539,2899,"let's move straight into the specific issues then more so um expand on that so Lenny asks what form do you think AI regulation will take over the next two administrations um B asks will AI regulation result in a concentrated few companies or an explosion of startups um and new innovation um iray asks how would you prevent the AI industry from being monopolized centralized by just a few Tech Corps and then our our friend bef jizos uh asks uh how do you see the regulation of AI compute and open source models realistically playing out uh where can we apply pressure to make sure we maintain our freedom to build and own AI systems it's really interesting because there's like a regulatory um dimension of that and then there's the uh kind of technological kind of you know version of that and they do intersect so if you look at what big Tech has been trying to do they're trying they're very worried about new competition to the point where they've take it upon themselves to go to Washington and try and Outlaw their competitors um and you know if they succeed with that then I think it is like super concentrated AI power you know making the kind of concentrated power of social media or search or so forth like kind of really pale in comparison I mean it would be very dramatic if there were only three companies that were allowed to build AI um and and that's certainly what they're pushing for so I think in one regulatory world where big Tech wins um then there's very few companies doing AI probably you know Google Microsoft and meta um um you know Microsoft you know having you know basically full control of open AI as they kind of demonstrated they they have the source code they have the weight you know such a when as far as saying that you we own everything and then they also kind of control who the CEO is as they demonstrated you know beautifully um so you know if you take that it will all be owned by you know three maybe four companies um if you just follow though the technological Dimension um I think what we're seeing play out has been super exciting in that um you know we were all kind of wondering would there be one model that ruled them all um and even within a company I think we're finding that there's no current architecture that's going to gain you know on a single thing a Transformer Model A diffusion model and so forth that's going to become so smart in itself that once you make it big enough it's just going to know everything and that's going to be that um what we've seen is you know even the large companies are deploying a technique called the mixture of experts which kind of implies you know you need different architectures for different things they need to be integrated in a certain way and the system has to work and that just opens the aperture for a lot of competition because there's many many ways to construct a mixture of experts to architect every piece of that we've seen you know little companies like MW field models that are highly competitive with you know the larger models um very quickly uh and you know and then there's um other kind of factors like you know latency cost Etc that factor into this and then there's also good enough like when is a language model good enough you know when it speaks English when it knows about what things um what are you using it for and then there's domain specific data you know I've been doing whatever medical research for years and I've got uh you know data around all these kinds of genetic patterns and diseases and so forth you know I can build a model against that data that's differentiated by the data and so on so I think what we will we're likely to see kind of a a great um kind of caman explosion of innovation across all sectors you know big companies small companies and so forth provided that the regulation doesn't Outlaw the small companies um but that would be uh my prediction right now yeah and i' i' add a bunch of I add a bunch of things to this so um so one is um even on the big model side um there's been this leap frogging thing that's taking place um and so uh you know there's there's you know opening you know gp4 was kind of you know the dominant model not that long ago and then it's it's been leap frogged in significant ways recently by both Google with their Gemini Pro especially the one with the so-called long context window where you can feed it 700,000 words um or an hour of full motion video as you know context for a question which a huge Advance um and then uh you know the anthropic uh their big model claw um is uh you know lot a lot of people now are finding that to be more advanced model than gp4 and and you know and one assumes open a is going to come back and you know this leap frogging will probably happen for a while so so so even at the highest end you know at the moment these companies are still competing with each other um uh you know there there's still this Lea frogging that's taking place and then you know Ben as you as you articulated um you know very well you know there there is this this giant explosion of of models of all kinds of shapes and sizes um our another you know our company data bricks uh just released another you know another what looks like a big leap frog on the smaller model side um it's the I think it's the best small model now in the benchmarks and it is it actually it's so efficient it will run in a Macbook yeah and they have the advantage of you know as a as an Enterprise you can connect it to a system that gives you not only like Enterprise quality access control and all that kind of thing but also um you know gives you the power to do SQL queries with it gives you the power to um basically create a catalog so that you can have a common understood definition of all the weird corporate words you have like by the way one of which is customer like there there's almost no two companies that Define customer in the same way and in most companies there are several definitions of of customer you know from is it a department at AT&T is it AT&T is it you know some you know division of AT&T etc",Politics & the Future of Tech - 009,7,9
117,a16z,Politics & the Future of Tech,Big Tech and the reality of competition,2899,3409,department at AT&T is it AT&T is it you know some you know division of AT&T etc etc I I think um I don't want to literally speak for them but I think if you put the CEOs of the big companies under serum I think what they would say is their big fear is that AI is actually not going to lead to a monopoly for them it's going to lead to a commodity it's going to lead to a sort of a race to the bottom on price um and and you see that a little bit now which is people who are using one of the big models apis are able to swap to another big model API from another company pretty easily and then you know these models the business model the main business model for these big models at least so far is API you know basically pay per token uh generated um or per answer um and so like if these companies really have to compete with each other like it it may be that it actually is a hyper competitive market it may be the opposite of like a search Market uh or like an operating system Market it may a market where there's just like continuous competition and Improvement and leap frogging and then you know constant price competition and then of course you know the payoff from that is you know to everybody else in the world is like an enormously vibrant Market where there's constant Innovation happening and then there's constant cost optimization happening where and then and then as as a customer you know Downstream of this the entire world that's going to use AI is going to benefit from this kind of hyper competition that's gonna you know could potentially run for run for decades and so I I think if you put these CEOs in a TR serum what they would say is that's actually their nightmare like that that's why they're in Washington that's why they're in Washington so that that's that that is what's actually happening that is the scenario they're trying to prevent um they are actually trying to shut out competition uh and by the way actually I will I will tell you this there is a funny thing Tech is so ham Tech is so historically bad at politics um that I think I think some of these folks think they're being very clever in how they go about this um and so you know because they show up in Washington with the kind of Nar you know kind of Public Service narrative or end of the world narrative or whatever it is and they're I think they think that they're going to very cleverly kind of trick everybody trick people in Washington into giving them sort of cartel status and the people in Washington won't realize until it's too late but it actually turns out people in Washington are actually quite cynical they've been lobbied before exactly and so uh there there is this thing and then you know they won't they don't I get this from them off the Record a lot especially after a couple of drinks which is basically if you've been in Washington for longer than 2 minutes you have seen many Industries come to Washington many big companies come to Washington and want Monopoly or cartel kind of regulatory protection um and so you you SE you've seen this and if you're in Washington you've seen this play out you know in some cases the guys's been there for a long time dozens or hundreds of times um and so my my sense is like nine months ago or something there was a moment where it seemed like the big tech companies could kind of get away with this um I think it's it's it's actually I think I think actually it's it's the edges there I'm still concerned and we're still working on it but I think the edges come off a little bit because I think the cynicism of Washington in this case is actually correct and I think they're kind of on to these companies and then you know look if there's unifying issue there's basically two unifying issues in Washington one is they don't like China and the others they don't like big Tech um and so you know this is I this is a winnable War like this is a winnable war on behalf of of startups and open source um and and freedom and and competition and so I'm actually yeah I'm I'm I'm worried but I I'm feeling much better about it than I was nine months ago yeah yeah well look we had to show up I mean that's the other thing I mean it's t me a real less which is um you know you can't expect people to know what's in your head you know you've got to go see them you've got to put in the time you've got to kind of say what you think and then you know and and if you don't you don't have any right to like ring your hands with how like you know bad things are yeah and then I just wanted to know one more one more thing just for um says um uh met you know you can you kind of mentioned the the big companies being Microsoft Google meta um is worth noting meta is on the open source side of this um and so meta's meta's actually working quite hard and this is a big deal because it's very you know contrary to the image I think that people have a meta over you know prior in Prior issues you know correctly or not but um on on the open source AI topic and on freedom to innovate uh at least for now meta is I think very strongly on that side so yeah yeah yeah I think that's right it's actually an very interesting point in kind of I think essential for people to understand is that the way met is thinking about this and the way that they're actually behaving and executing is very similar to how Google thought about Android um where you know their main concern was that Apple not have a monopoly on the smartphone you know not so much that they make money on the smartphone themselves um because you know a monopoly on the smartphone on the smartphone for Apple would mean that you know Google's other business was in real Jeopardy and so they ended up being kind of an actor for good and you know Android's been an amazing thing for the world I think uh you know in including getting smartphones in the hands of people who won't be able to get them otherwise you know all over the world and meta is doing kind of a very similar effort where uh you know in order to make sure that they have um AI as a great ingredient in their products and services um is willing to open source it and kind of gives their all of their very very kind of large investment in AI to the world um so that you know entrepreneurs and everybody can kind of keep them competitive even though they don't plan to be in the business of AI in the same way that you know like Google is in the business of smartphones to some extent but it's not their kind of key business and you know meta doesn't have a plan to be in the AI business maybe you know to some extent they will to but but that's not the main goal then I would put one other company on the concerning side on this and I it's too early to tell but um where they're going to shake out but um you know Amazon just announced they're investing a lot more money in anthropic um so I think they're now basically Amazon is to anthropic what Microsoft is to open AI um yeah I think that's Poss yep yep yeah and so like there there's there's a um put anthropic is very much in the group of kind of big Tech you know kind of new incumbent big Tech you know lobbying very aggressively for regulation and Regulatory capture uh in DC and so I I think it's it's sort of an open question whether Amazon is going to pick up that agenda as open as Tropic in chly becomes effectively a subsidiary subsidiary of Amazon yeah well this is another place where we're on the site of um Washington DC and in the Curr current regulatory motion where you know the big tech companies have done this thing um which we thought was illegal because uh we we observed it occur at AOL and people went to jail but what they've done is they they invest in startups you know huge amounts of money Microsoft and Amazon and Google are all doing it um you know like billion of dollars uh with the requirement with the explicit requirement that those companies then buy gpus from them um at like not the the discount that they'd ordinarily get but at a a relatively high price um and then be in their clouds so um that kind of and then you know in the Microsoft case even more aggressive give me your source code give me your weights um you know which is like extremely aggressive so you know they're moving money from the balance sheet to their p&l um you know in a way that at least from an accounting standpoint it was our understanding wasn't legal and pftc is you know looking at that now but it it'll be interesting to see how that plays out yeah well the other is that's one area another issue you know that people should watch is um you know that's one that's around tripping the other one is just consolidation um you know if you own you know half of a company um and you get to appoint the management team yeah um like is that you know is that a sub like is that not a subsidiary you know there are rules on that like at what point you you own the company Equity you own the intellectual property of the company um and you control the management team yeah is that not your company yeah and then at that point if you're not consolidating it like is that legal and so the SEC is going to win on that and then of course you know to the extent that some of these companies have nonprofit components to them there's you know tax implications to the conversion to for-profit and so forth and so like there there's a lot of yeah this this this yeah the stakes the stakes in the legal I say the stakes in the legal Regulatory and political game that's being played here I think are quite quite High quite High yes yes Ben I Ben and I has been mentioned as been and I are old enough where we do know a bunch of people who G on to jail um so some some of these issues turn out to be serious um so,Politics & the Future of Tech - 010,8,10
118,a16z,Politics & the Future of Tech,Q: What would happen if there was zero regulation of AI?,3409,3837,on to jail um so some some of these issues turn out to be serious um so Gabriel asks uh what would happen if there was Zero regulation of AI The Good the Bad and the Ugly and this is um this is actually really important topic so you know we're we're we're vigorously arguing you know in DC um that there should be you know basically uh uh anybody should be completely capable of building Ai and deploying AI big companies should be allowed to do it small companies should be allowed to do it open source should be allowed to do it um and you know look a lot of the regulatory pushes we've been discussing that comes from the big companies and from the activist is to is to prevent that from happening and put everything in the hands of the big companies um so you know we're definitely on the side of freedom to innovate you know having said that you know that's not the same as saying no regulations of anything ever um and so we're we're definitely not approaching this with kind of a hardcore libertarian lens um the interesting thing about regulation of AI is that it it turns out when you kind of go down the list of the things that I would say um reasonable people kind of uh you know kind of you know kind of sort of uh you know thoughtful people consider to be concerns around AI on on both sides of the aisle um basically the implications that they're worried about are less the technology itself um and are more the the use of the technology um in practice uh either for good or for bad and so you know Ben you you brought up for example um if AI is making decisions on things like uh granting credit or mortgages or Insurance then you know there there are very serious policy issues around you know who how those answers are derived at which groups are affected in different ways um you know the flip side is you know if AI is used to plan a crime um you know or to uh you know plan a bank robber or something like that or terrorist attack um you know that's that you know that's obviously something that people focused on National Security Law Enforcement are very concerned about um look our approach on this is actually very straightforward which is um it it seems like completely reasonable to regulate uses of AI um you know and things that would be in things that would be dangerous now the interesting thing about that is as far as I can tell and I've been talking to a lot of people in DC about this as far as I can tell every single use of AI to do something bad um is already illegal under current laws U and regulations um and so it's already illegal to be discriminatory and lending it's already illegal to redline in mortgages it's already illegal to plan bank robberies it's already illegal to plan terrorist attacks um like these things are already illegal and there's you know decades or centuries of case law and regulation um and you know law enforcement and intelligence capabilities around all of these um and so to be clear like we think it's like completely appropriate that those authorities be used and if there are new laws or regulations needed due to you know other bad uses that that that makes total sense but that basically the issues that people are worried about can be contained in controll the level of the use as opposed to somehow saying you know by the way as some of the Doomer activists do you know we need to Lally prevent people from you know doing linear linear algebra on their computers yeah well I think that's important to point out like what is AI and it turns out to be you know it's math um and and specifically kind of like a mathematical model so you can think of it for those of you who study math in school you know in math you can have an equation like you know yal X2 plus b or something and that equation can kind of model the behavior of something in physics or you know something in the real world and so that you can predict um you know something happening uh like the speed that an object will drop or so forth and so on and then AI is kind of that but with huge computer power applied so that you can have much bigger equations with you know instead of two or three or four variables you could have you know 300 billion variables and so if you get into the challenge with that of course is if you get into regulating math and you say well math is okay up to a certain number of variables but then at the you know two billionth and first variable then it's dangerous uh then you're into like a pretty bad place and that you're going to prevent everything good from the technology uh from happening as as well as anything that you might think is bad so you really do want to be in the business of regulating the kind of applications of the technology not the math in the same way that you know you would have wanted to like nuclear power is very uh potentially dangerous as nuclear weapons are extremely dangerous you wouldn't want to kind of put parameters around what physics you could study in order to you know like literally in the abstract in order to kind of prevent somebody from getting a nuke like you can no longer study Physics in Iran because then you might be able to build a nuke would be kind of the conclusion uh and you know that has been kind of what big Tech has been pushing for not because they want safety but because you know again they want a monopoly and so I think we have to be very very careful not to do that I do think there there will probably be you know some cases that come up that are enabled by AI new applications that do need to be you know regulated potentially um you know for example uh I don't know that there's a law that like if you recreate like something that sounds exactly like Drake and then you kind of put out a song that sounds like a Drake song like I I don't know that that's illegal maybe that should be illegal I think those things need to be considered uh for sure um and you know there there's certainly danger in that I also think we need technological solutions not just regulatory solutions for things like deep fakes um that kind of help us get to you know what's human what's not human and interesting a lot of those are uh kind of um you know viable now based on kind of blockchain crypto technology yeah so let's uh yeah just on the voice thing real quick yeah so it actually I believe this to be the case it is not currently possible to copyright a voice yeah um right you can copyright lyrics and you can cop copyright music and you can copyright Tunes um right Melodies and so forth um copyright of voice um and yeah that that seems like a perfect example where that it seems like that probably is a good idea to have a that let you Cate your voice yeah I feel that way you know particularly if you know people call their voice Drake squared or something right like you know it could get very dodgy again you know you get again it's just the details you know trade trademark you can trademark your name um so you could you could probably prosecute on that but by by the way having said that look this also gets to the the complexity of these things that there is actually an issue around copyrighting of voice which is okay well how close to the how close to the voice of Drake does like there are a lot of people who have like a lot of voices in the world and like how close do you have to get before you're violating copyright and what if my natural voice actually sounds like Drake like am I now in trouble right and do I Outlaw like Jamie Fox imitating crcy Jones and that kind of thing right exactly so so anyway yeah I me but I mean look I I you know agreeing violently with you on this is like that seems like a great topic uh that needs to be taken up and take you looked at seriously from a from a legal standpoint um that is sort is actually you know is obviously exact you know sort of an issue that's sort of elevated by AI but it's a is a general kind of concept of like able to copyright trademark things which has a long history and law yeah for sure um yeah so,Politics & the Future of Tech - 011,9,11
119,a16z,Politics & the Future of Tech,Q: How important is the development of decentralized AI and how can the U.S. retain innovation leadership?,3837,4338,"trademark things which has a long history and law yeah for sure um yeah so let's talk about the decentralization and the blockchain aspects of this um so uh want to get into this so Goose asks how important is the development of decentralized AI uh and how can the private sector catalyze prudent and pragmatic regulations to uh ensure us retains Innovation leadership in this space so yeah let's so let's Ben let's talk about uh well let's talk about decentralized Ai and then maybe I'll just I'll highlight real quick and then you you can build on it um decentralized AI like you know the the sort of default way that AI systems are being built uh today is with basically you supercomputer clusters in a cloud um and so you'll have a single data center somewhere that's got you know 10,000 or 100,000 ships and then a whole bunch of systems to interconnect them and make them all work um and then you have a company you know that um you know basically you know own owns and controls that um uh and you know these and you know these companies AI companies are raising a lot of money to do that now these are very large scale centralized um you know kinds of operations and you know to train a you know state-of-the-art model you're at $100 million plus you know to train a you know a big one to train a small one like like the datab bricks model that just came out it's like on the order of $10 million and so that you know these are large centralized efforts and by the way we we all think that the big models are going to end up costing a billion you know and up in the in in the future um and so so then this raised the question of like is there an alternate way to do this and the alternate way to do this is with we we believe strongly is with a decentralized approach and in particular with a blockchain based approach it's actually the kind of thing that the blockchain web 3 kind of method you know seems like it would work with very well um and in fact we are already blocking uh backing companies and startups that are doing this and I would say there's at least three kind of obvious layers that you could decentralize um that seem like they're increasingly important so one is the training layer uh well actually let me say four there's the training layer uh which is you know building the model there's the inference layer which is the running the model to answer questions there's the data layer uh B to your point on on on opening up the black box of where the data is coming from which is there should probably be a blockchain based system where people who own data can contribute it um for for training of AI and then get paid for it and where you track all that um and then there's a fourth that you alluded to which is uh deep fakes um it it seems obvious to us that the answer to uh deep fakes and I should I should pause for a second and say in my last three months of trips to DC the number one issue politicians are focused on with AI is deep picks could the it's the one it's the one that directly affects them and I I think every politician right now who's thought about this has a nightmare scenario of you know it's three days before their re-election campaign you know three three days before the vote and a defect goes out with them you know saying something absolutely horrible and it's so good and the voters get confused and like and then they lose the election on that and so I would say like that's actually the thing that's that's the thing that actually has the most potency right now um and then you know the what what what basically a lot of people say including the politicians is so therefore we need basically a way to detect deep fakes um and so either the AI systems need to Watermark AI generated content so that you can tell that it's a deep fake or you need these kind of scanners like the scanners that are being used in some schools now to try to detect that something was a generated um our view as you know I would say both technologists and investors in the space is that the methods of attack of of detecting AI generated content after the fact are basically not going to work um and they're not going to work because uh AI is already too good uh at at doing this um and by the way for example if if youve if you have if you happen to have kids that are uh in a school and they're they're running one of these uh scanner programs that is supposed to detect whether your kid is submitting an essay that you you where you use Chad GP to write the essay like those really don't work in a reliable way and there's there's a lot of both false positives and false negatives off of those that are are very bad so those are actually very bad ideas um and and for the same same reason like uh detection of AI generated photos and videos and and and and speech uh is not going to be possible and so our our view is you have to flip the problem uh you have to invert the problem and what you have to do instead is basically have a system in which uh real people can certify um that content about them is real um we content has Providence as well where go ahead Ben go ahead and describe how that would work yeah so um you know we have like you know one of the amazing things about crypto blockchain is it deploys something known as a public key infrastructure which enables kind of every human to have a key that's unique to them where they can sign so like if I was in a video um or in a photo or I wrote something I can certify that yes this is exactly what I wrote and you cannot alter it um to make it into something else um it is just exactly that and then um you know as that thing you know gets transferred through the world let's let's say that it's something you know like a song that you sell and so forth you can track just like with a you know in a in a less precise way but with a work of art we track the provenance or with a house who owned it before you and so forth that's also like an easy application on the blockchain and so that you know com combination of capabilities can make this whole uh kind of program much more viable in terms of like okay knowing what's real what's fake where it came from you know where it started where it's going and so forth I you know kind of going back the data one I think is really really important and that um you know these these systems you know one of the things that they've done that's I would say dodgy um and you know there have been like big push back against it with you know Elon trying to lock down Twitter um and the New York Times suing open Ai and so forth you know these systems have gone out and just slurped in data from all over the internet and all over kind of you know you people's businesses and so forth and train their models on them um and you know I think that there's a question of whether the people who created that data should have any say in whether the models trained on that data uh and you know blockchain is an unbelievably great system for this because you can permission people to use it you can charge them a fee um it can be all automated uh in a way where you can say sure come train you know and I think training data ought to be of this nature where there's a data Marketplace and people can say yes take this data for free I want the model to have this knowledge or no you can't have it for free but you can have it or no you can't have it at all um rather than you know what's gone on which is this very aggressive scraping um um and you know like you have these very smart models where these companies are making enormous amounts of money um taken from data that um certainly didn't belong to them you know maybe it's in the public domain or what have you but you know that that ought to be an explicit relationship and it's not today and that's a a a very great blockchain solution you know and you part of the reason we need the correct regulation on blockchain and we need the secs to stop harassing and terrorizing get up people trying to innovate in this category um and so that's kind of the second category and then you have like training and inference and I would say you know right now the push against kind of decentralized training and inference is well you know you need this very fast interconnect and you need it to all be in one place technologically but and and I think that's true for people who have more money than time right um which is like you know startups and big companies and so forth but for people in Academia who have more time than money they're getting completely frozen out of AI research you can't do there's not enough money in all of Academia to participate anymore in AI research and so you know having a decentralized approach where you can share you know all the gpus across your network and hey yeah maybe it takes a lot longer to train your networker to serve it but you know what you still can do your research you can still innovate you know create new ideas do architectures and test them out at Large Scale um which you know will be amazing if we can do it and again we need um you know the SEC to stop you know kind of illegally terrorizing every crypto company and trying to block laws from being put in place that help us you know enable this yeah there's actually a",Politics & the Future of Tech - 012,10,12
120,a16z,Politics & the Future of Tech,Research universities and lack of AI funding,4338,4568,from being put in place that help us you know enable this yeah there's actually a really and you alluded to at the college thing actually really matters so um we have a friend uh you know who runs one of you know is very involved in one of the big computer science programs one of the major American Research universities and of course by the way a lot of the technology we're talking about was developed at American Research universities absolutely and and Canadian ones too Toronto Canadian ones and European ones exactly uh you know historically as with every other wave of technology in the last you know whatever hundred years you know the research our research universities you know across these countries have been kind of the the gems of the of the you know the Wellsprings of of a lot of the new technology that have ended up powering you know the you know the economy and everything else around us you know we have a friend involved in running one of these and this friend said a while ago that he he said that um the um you know his concern was that his university would be unable to uh fund a uh competitive AI cluster basically so you know a compute grid um that would actually let students and professors at that University actually work in AI um because it's now getting to be too expensive and research and and universities are just not funded to do do have capex programs that big um and then he said his his concern more recently has been all research universities together might not be able to afford uh to to do that which means all universities together might not be able to um actually have you know basically Cutting Edge AI work uh happening on the University side um and then I I happen to have a convers I was in DC I was in a bipartisan uh you know house uh a meeting um the other day with uh on these AI topics and one and actually one of the one of the in this case Democratic Congress Congress women asked me you know the question which you know comes up which is a very serious question always right which is how do you get kind of more more members of unrepresented groups underrepresented groups involved in Tech and you know I found myself giving the same answer that I always give on that which is you you you you the most effective thing you need to do is you need to go upstream and you need to have more people coming out of you know college with computer science degrees um who are you know skilled and qualified and trained right and mentored in to be able to participate in the industry and you know that's you know you you and I Ben both came out of state schools um you know with uh you know with computer science programs um that you know where we were able to then have the careers we've had um and so you know I find myself answering the question saying well we need we need more computer science you know graduates from from all all you know from every from every group and then but in the back of my head I was like and it's going to be impossible to do that because none of these places are going to be able to afford to actually have the computer resources to be able to actually have ai programs in the future um and so like you know maybe the government can fix this by just dumping a ton of money on top of these universities and maybe that's what will happen and you know the current political environment seems like maybe it's not quite feasible um for a variety of reasons and then and then the the other approach would be a decentral Iz approach would be a blockchain blockchain based approach that everybody that everybody could participate in you know if if that were something that the government will were willing to support which right now it's it's not and so I I think there's a really really really really Central important vital issue here um that I think you know I think um is being glossed over by a lot of people that I think should really be looked at yeah no I I I think it's absolutely critical you know and this is you know again kind of going back to our original thing like it's so important to the country um being what you know America being America should be to get these issues right and we're we're definitely in danger of that not happening you know because you know look I think people are taking much too narrow a view of some of these Technologies and not understanding their full capabilities and you know we get into oh the AI could you know say something racist therefore we won't cure cancer I mean like we're getting into that kind of dumb idea and uh you know we need to have a tech forward kind of solution to some of these things and then the right regulatory approach to to kind of make the whole environment work so yeah let's go into that next the next phase of this now which is the the sort of global implications so um I'm,Politics & the Future of Tech - 013,11,13
121,a16z,Politics & the Future of Tech,Bipolar tech world / U.S. Vs. China,4568,5503,next phase of this now which is the the sort of global implications so um I'm going to conjoin two different topics here but I'm going to do it on purpose so Michael Frank Martin asks what could the US do to position itself as the global leader of Open Source software do you see any specific legislation or regulatory constraints that are hampering the development of Open Source projects um Arta asks similar question what would an ideal AI policy for open source software models looks like um and then Sarah Holmes asks the China question um do you think we will end up with two AI Tech Stacks the west and China and ultimately companies will have to pick one side and stay on it and so look I would say this is where you get to like the really really big geopolitical long-term issue which is basically my understanding of things is is is sort of as follows which is you know basically the the for a variety of reasons technological development in the west is being centralized into the United States you know with with you know some in Canada and some in Europe although you know quite frankly a lot of lot of the best Canadian and you know European you know Tech Founders are coming to Silicon Valley you know Yan Lon you know teaches Yan Lun is you know a hero in France teaches at NYU and works at meta both of which are American institutions and so they're they're sort of an American or let's say American plus European kind of you know kind of kind of uh you know sort of tech Vanguard wedge in the world and then and then there's China um and and and and really it's it's it's actually quite a bipolar uh situation um you know it it would say you know the dreams Tech being fully democratized and spreading you know throughout the world have been realized for sure on the on the youth side but you know not nearly as much on the entrepreneurship side or the invention side um and again immigration you know immigration being a great you know virtue uh but um you know for the countries that are the beneficiaries of immigration the other side of that is you know it makes you know other countries are going to be less competitive because they're their best and brightest moving to the US so so anyway so we are in a bipolar we are in a bipolar Tech world is primarily a bipolar Tech world it's primarily the US and China um you know this is not the first time we have been in a bipolar World involving you know geopolitics and Technology um you know there the US and China have two very different systems um uh the Chinese system has all of the virtues and downsides of being centralized um the US system has all the virtues and downsides of being more decentralized um there is a very different set of views uh of the two systems on how Society should be ordered and what Freedom means um and you know what people should be able to do and not do and then look both the US and China have visions of global Supremacy and visions of basically car and agendas and programs uh you know to carry forward their points of view on the technology of AI and on the societal implications of AI uh out you know throughout the world um and so you know there is this cold war two oh and then the other thing is just in in DC it's just crystal clear that there's this now Dynamic happening where Republicans and Democrats right now are trying to LeapFrog each other every day on being more anti-china um and so you know we're our friend Neil Ferguson is using the term I think Cold War 2.0 like we're we're we're whether we want to or not like we're in Cold War 2.0 like we're we're we're in a dynamic similar to the one was with the USSR you know 30 30 40 50 years ago um and to sah Holmes's question it's 100% going to be the case there are two AI Tech Stacks um and there are two AI governance models and there are two AI you know deployment you know systems and you know there are two ways in which uh you know AI duv tails in everything from surveillance to Smart cities to Transportation self-driving cars drones who controls what who gets access to what who sees what uh the degree by the way to which AI is used as a method for population control um there there there are very different Visions um and and these are National visions and Global Visions um and there's a very big competition developing um and you know it it it certainly looks to me like there's going to be a winner and a loser um I think it's overwhelmingly in you know our best interest for the US to be the winner for the US we have to lean into our strengths um and you know our the downside of our system is that we are not as well organized and orchestrated top down as China is uh the upside of our system at least historically is that we're we're able to benefit from decentralization we're able to benefit from competition from a market economy from a private sector right uh where you know we're able to basically have a much larger number of smart people being you know making lots of small decisions to be able to get to good outcomes as opposed to you know having a having a dictatorial system in which there's a small number of people trying to make decisions I mean look and this is how we won the Cold War against Russia is our decentralized system just worked better economically technologically and ultimately militarily uh than the than the Soviet uh centralized system and so it just seems like fairly obvious to me that like we we have to we have to lean into our strengths we better lean into our strengths and if because if we think we're just going to be like another version of a centralized system but without all the advantages that China has with having a more centralized system you know that just seems like a bad formula so yeah let me pause there and B see what you think yeah I know for sure I think that'd be dis dous um and I think this is why it's so clear that if there's one you know to answer the question if there's one regulatory policy that we would enact that would ensure America's competitiveness it would be open source and the reason being that as you said um this enables the largest number of participants to kind of contribute to AI to innovate to come up with novel Solutions and so forth and I think that you're right China's you know what's going to happen in China is they're going to pick one because they can um and they're going to uh kind of drive all their wood behind that arrow in a way that we could never do because we just don't work that way um and they're going to impose that on their society and try and impose it on the world and you know our best counter to that is to put it in the hands of all of our smart people I have so many smart people from all over the world from you know like as um we like to say diversity is our strength we've got this tremendous uh different points of view different you know kind of kinds of people in our country and you know the more that we can enable them the more likely we'll be competitive and I'll give you a tremendous example of this is um you know I think if you go back to 2017 and you read any you know foreign policy magazine Etc there wasn't a single one that didn't say China was ahead in AI they have more patents they have more students going to universities they they're ahead in AI they're ahead in AI like we're behind an AI and then you know chat GPT comes out and goes oh I guess we're not behind an AI we're ahead of an AI and the truth of it was what China was ahead on was in integrating AI into the government their one AI into their government in that way and you know look we're working on doing a better job of that with American dynamism but um we're never going to be good at that model you know where you know that's the model that they're going to be great at and we have to be great at our model and if we start limiting that outlawing startups and outlawing anybody but the big companies from developing Ai and all that kind of thing uh we'll definitely shoot ourselves on the foot I would say related or like another kind of important point I think in kind of the safety of the world um is you know when you talk about two AIS that's a like two AI Stacks um perhaps but it's very important that uh countries that are in America that are in China can align AI to their values and I'll just give you kind of one really important example uh which you know like I've been spending a lot of time in the Middle East and if you look at the kind of history of you know a country like Saudi Arabia they're coming from a world of fundamentalism and you know a kind of set of values that they're you know they're trying to modernize they you know they've done tremendous things with women's rights and so forth but you know look there's still the fact that they've got you know people who don't want to go to that future so fast and they need to preserve some of their history in order to not have a revolution or extreme violence and so forth and yeah we're seeing Al Qaeda re-park up in Afghanistan and all these kinds of things uh which are by the way Al qaa's Real Enemy of modern Saudi just as much as they're America an enemy of America and so if Saudi can't Aline an AI to the current Saudi values they could literally spark a revolution in their country and so it's very important that as we have technology that we develop that it not be totally proprietary clone source that it' be kind of uh modifiable by our allies uh who need to kind of progress at their Pace to keep their kind of country safe and keep us safe uh in doing so and so this has got great geopolitical ramifications what we do here like if we go to the model that Google and Microsoft are advocating for this Chinese model of only a few can control AI uh we're going to be in big trouble yeah and then I just want to close in the open source point because it's so critical so this is where you know I say I get extremely irate at the at the idea of closing down open source which you know people in in in a number of of these people are are lobing for very actively by the way I I I'm going to name one more name yeah uh we even have VC's lobbying to Outlaw open source which I find to just be completely staggering and in V venod so ven venod kosla who is which is this is just incredible to me he's a founder of sun Microsystems which was in many ways a company built on open source uh built on open source Unix out of Berkeley and then itself built a lot of Open Source critical open source um and then of course you know was the do.com which of course you know the internet was all built on open source and venod has been lobbying to ban open source AI um and by the way he denies that he's been doing this but I saw him do it with my own eyes uh when the US Congressional China committee came to Stanford um I was in the meeting where he was with you know 30 20 or 30 congressmen and and lobbying actively for this and so I've seen him do it myself um and you know look he's got a big stake in open AI you know maybe it's Financial self-interest by the way maybe he's a True Believer in the dangers but in any of that well I think he proved on Twitter he was not a True Believer in the dangers I'll get into that I'll explain that but yeah yeah so um so uh yeah I mean even within even within little Tech even within the startup world we we we we are not uniform in this and I I think that's extremely dangerous open look open source like what is open source software like open source software is is you know it is quite lit you know it's the technological technological equivalent of free speech um which means it's the technological equivalent of free thought and it it is the way that the software industry has developed to be able to build many of the most critical components of the modern technological World um and then Ben as you said earlier to be able to secure those um and to be able to have those actually be safe and reliable um uh and then and then to have the transparency you know that that we've talked about so that you know how how they work and how they're making decisions um and then to your last point also so that you can have customized AI in many different environments so you don't end up with a world where you just have one or a couple AIS but you actually have like a diversity of AIS with with like lot lots of different points of view and lots of different capabilities um and so the the open source fight is actually at the core of this um and and and of course the the reason why you know the sort of you know sort of people with an eye towards Monopoly or cartel want to ban this as open source is a tremendous threat to Monopoly or cartel like you know in many ways is a guarantee that Monopoly or cartel can't last um but it is absolutely 100% you know required for the you know for the furtherance of number one a vibrant private sector number two a vibrant startup sector and then right back to the Academia Point like without open source then at that point you know University college kids are just not going to be able to they're not even GNA be able to learn how the technology Works they're just going to be like completely boxed out um and so a world where open source is banned is is bad on so many fronts it's just incredible to me that anybody's advocating for it it needs to be I think this needs to be recognized as the threat that it is yeah and on V Noe you know it was such a funny dialogue between you and he so like I'll just um give quick summary of it basically uh you know he was arguing for closed Source you for open source his core argument was this is the Manhattan Project um and therefore we can't let anybody know the secrets and you countered that by saying well if this is in fact the Manhattan Project then is like you know is the open AI team um you know locked in a remote location do they screen all their like employees very very carefully is it air locked are they you know is there super high security of course none of that is close to true in fact quite sure they have Chinese Nationals working there probably some are spies for the Chinese government there's no any kind of strong security at open AI or at Google or at any of these places you know anywhere near the Manhattan project which is where they built a whole city that nobody knew about um so they couldn't get into it and you know once you caught him in that he said nothing and then he says back well you know it cost billions of dollars to train these models you just want to give that away is that good you know is that good economics like that was his like final Counterpoint to you which basically said oh yeah I'm trying to preserve a monopoly here like what are you doing I'm an investor and and I think that's true for for all these arguments well the kicker you know the kicker B of that story the kicker to that is three three days later um the justice department indicted a Chinese National Google employee yeah who stole uh Google's next Generation AI chip designs which is quite literally the Family Jewels uh for an AI program it's you know it's the equivalent of stealing the you know if you stretch the metaphor the equivalent of stealing the design for the bomb um and that Google employee um uh took that those chip downloaded them and took them to China and by definition you know definition that means took them in the Chinese government because there's no distinction in China uh between the private sector and the government um it's an integrated thing um the government owns and controls everything um and so you know 100% 100% guaranteed that that went straight to the Chinese government Chinese military um and Google Google which you know like you know Google has like a big information security team and all the rest of it Google did not realize according to the indictment Google did not realize that that uh that engineer uh had been in China for six months yeah amazing well hold on it gets better it gets better this is the same Google with the same CEO who refused to sell Google proprietary AI technology to the US Department of Defense so they're supplying China with AI and not supplying the US which is just goes back to look if it's not open source we're never going to compete like yeah we've lost the future of the world right here um which is why you know it's the single most important issue for sure yeah and and you're you're not going to lock this stuff up like you're not going to lock it up nobody's locking it up it's not locked up these companies are security swiss cheese um and like you know you're not going to you know and you have a debate about the Tactical relevance of of Chip embargos and so forth but like you're you're you're you're not the horse has left the barn on this not least because these companies are without a doubt riddled with uh with uh with with foreign assets and they're very easy to penetrate um and so we we we just have to be like I would say very realistic about the actual state of play here um and and and we have to we have to we have to play in reality and we have to we have to play in reality we have to win in reality um and the win is we need Innovation we need competition we need free thought we need free speech we need we need to embrace the virtues of our system um and and not and not shut ourselves down in the face of in the face of the you know the conflicts that are that are coming uh another one,Politics & the Future of Tech - 014,12,14
122,a16z,Politics & the Future of Tech,Q: Are European AI companies becoming less interesting investment targets for U.S. based VC’s due to the regulatory landscape?,5503,5951,the face of the you know the conflicts that are that are coming uh another one um why are Andre Andreas asks why are usvc so much more engaged in politics and policy than their Global counterparts and um I I really appreciate that question because it basically like if if it if that's the question then it means that boy VCS outside the US must not be engaged at all because USS are engaged yeah um and then what do you believe the impact of this is on both the VC ecosystem and Society in general and then related directly related question Vincent asks um are European AI companies becoming less interesting investment targets for us-based VCS due to the strict and predictably unpredictable regulatory landscape in Europe uh would would you advise early stage European ax companies to consider relocating to the us as a result great question um well I look I think that that it kind of goes back to a little of what you said earlier which is you know in startup world like there's you know in the west there's the United States and then there's everywhere else and the United States is kind of bigger than everywhere else combined um and you know so it's natural and look you know in these kind of political things it kind of starts with the leader and you know us is the leader in VC yeah we feel like we're the leaders in usvc so we need to go go to Washington until we go you know nobody's going and so that's that's a lot of the reason why we started things I'm on Wow on European regulatory policy look it's uh I think I I think generally regulatory policy is going to is likely dictate where you can build these companies we've seen some interesting things um you know France turns out to be leading a revolution in Europe on AI regulatory where they're basically telling the EU to pound sand um you know and large reason because they have a company there mrol and you know they it's a national National Jewel for for the country and they don't want to give it up because you know the EU has some crazy safetyism you know thing going on there yeah and I also note France also of course is playing the same role with nuclear policy in Europe um yeah they're the one country Europe they're they're the cleanest country you know probably one of the cleanest countries in the world as a result right but it been staunchly pronuclear um and trying to hold off I think in a lot of ways sort of attempts throughout the rest of Europe and especially from Germany to to basically bad nuclear civilian nuclear power yeah and the UK the UK is sort of been flip-flopping on AI policy and we'll see where they come out and uh you know Brussels has been ridiculous as they've been on almost everything yeah the big thing I think I note here is there's a really big philosophical distinction I think it's rooted actually in the difference between that traditionally it's been called I think the sort of Anglo or anglo-american kind of approach to to law and then the Continental European approach and I forget forget these it's it's like I forget the terms for it but the legal it's like common law and then um yeah I think the civil law so it's basically the difference basically is um that which is not outlawed is legal or that which is specifically uh not uh legal is legal uh and anything that's not explicitly legal is outlawed um right in in other words like by default do you have freedom and then you impose the law to have constraints or by default do you have no ability to do anything and then the law enables you to do things um and these are sort of this is like a fundamental like philosophical legal you know uh political distinction um and and then this this shows up in a lot of these issues with this idea called the precautionary principle uh which is sort of the a rewarding of the sort of traditional European approach um which is basically the precautionary principle says uh new technologies should not be allowed to be fielded until they are proven to be harmless um yeah right and of course the the precautionary principle very specifically is is sort of a sort of a Hallmark of of the European approach to regulation and increasingly you know by the US approach um and it it specifically it's its origin it was actually uh sort of uh described in that way and given that name actually by the German Greens in the 1970s to to as a means to ban civilian nuclear power um you know with with by the way with just catastrophic results and we could we could spend a lot a lot of time on that but I think everybody at this point agrees like including the Germans increasingly agreed that was that was a big mistake that among other things you know has led to basically Europe funding Russia's invasion of Ukraine through the you know the need for imported energy because they they keep shutting down their nuclear plants um and so just like sort of a catastrophic decision but the the precautionary principle has become like I would say extremely trendy like it's one of these things like it sounds great right it's like well why would you Poss why would you want to be released in the world if it's not proved to be harmless like how can you possibly be in support of anything that's going to cause harm but the obvious problem of that is with that principle you could have never deployed Technologies such as fire um electric power internal combustion engines Cars airplanes the computer right like every single piece of technology we have the poers modern day civilization has way in which it can be used to hurt people right every every single one technology Technologies are double-edged swords there are thing you know you can you can use fire uh to protect your village or to attack the neighboring Village like you know the these things can be used in in in in in both different ways um and so basically if we had applied the precautionary principle historically we would not have you know we would still be living in mud Huts we would be just like absolutely miserable Mis miserable and so the idea of imposing the precautionary principle today if you're coming from like an Anglo you know American kind of you know perspective or from a freedom to innovate you know perspective that's just like that's just like incredibly horrifying um the you know she basically guaranteed to stall out progress um you know this is very much the mentality of the EU bureaucrats in particular um and this is the mentality behind a lot of their recent legislation on on on technology issues um France is is does seem to be the main counterweight against this in in the in Europe um you know Ben to your point like UK has been a counterweight in some areas uh but okay also has like I would say they've received a full dose of this programming they they have that tendency yeah and they've been in AI in particular I think they've been on the wrong side of that which hopefully they'll reconsider um so so again this is one of these things like this this is a really really important issue and just the surface level thing of like okay this this technology might be able to be used for some harmful purpose like that if that is allowed to be the end of the discussion like we are never going to nothing new is ever going to happen in the world like that that will that will cause us ultimately to stall out completely um and and then you know if if we stall out that that will over time lead to regression and and like literally you I mean this is happening like the power is going out like you know German societ German German societ German industrial companies are shutting down because they can't afford the power that's resulted from this this uh you know kind of the imposition of this policy in the energy sector um and so this is a very very very important thing I think the E bureaucracy has lost on this and so I think it's going to be up to the individual countries um to directly confront this if they want to um anyway so I I really applaud what France has done and I hope more European countries join them in in uh kind of being on the right side on this yeah yeah know it always is funny to me to hear the EU and like the economists and these kinds of things say oh the EU may not be the leader in Innovation but we're the leaders in regulation and I'm like well you realize those go together they're like one is a function of the other okay good so and then let's do one,Politics & the Future of Tech - 015,13,15
123,a16z,Politics & the Future of Tech,Q: What countries could be receptive to Techno-Optimism?,5951,6145,they're like one is a function of the other okay good so and then let's do one more Global question um lap gong Leong asks are there any other countries that could be receptive to techno optimism uh for example could Britain Argentina or Japan be ideal targets for our message and Mission yes uh so will you look for work on that in Britain and uh look we've got some pretty good reception from the UK government there's um you know a lot of very very smart people there um we're working with them you know tightly on uh their Ai and crypto efforts and and and we're hoping that's the case um you know Japan is having spent a lot of time there is you know they've obviously shown that capability you know over time and then you know there's a lot about the way Japanese Society works that that holds them back from that at times as well um you know without getting into all the specifics there's you know they have a very I would just say unusual and unique culture um that has a great difference for the old way of doing things um which sometimes makes it hard to kind of promote the new way of doing things I also think you know around the world World um you know the the Middle East is very very kind of subject and and kind of on board with techno optimism um the UAE uh Saudi Israel of course um you know many many countries out there are very excited about you know the these kinds of ideas and Tak the world forward and like you know just creating a better world through technology um which I think that look with our population growth if we don't have a better world through technology we're going to have a worse world without techn I think that's like very obvious uh so it's a it's a very compelling message and oh by the way South America I should say also um there there are a lot of countries who are really embracing techno optimism now in South America and and that's you know and some great new leadership there that you know that's pushing that yeah I would also say if you look at the polling on this um what I think you find is what I what you could describe as the younger countries are more enthusiastic about technology and I I don't mean younger here literally of like when they were formed but I mean two things one is how recently they've kind of emerged into what we would consider to be modernity yeah um and so you know for example to embrace you know Concepts like democracy or free market capitalism or you know Innovation generally um you know global trade and so forth and then the other is just quite simply the the the the number of the the demographics the the you know the the countries with a a larger number of people and those are often by the way the same countries right they have a they have the reverse demographic pyramid we have where they actually have a lot of young people um and young people are both you know both young people both need Economic Opportunity and are very fired up about new ideas yeah by the way this is true in Africa as well and many African countries you know uh Nigeria Rwanda Ghana they were their techno optimism I think is taking hold in a real way you know they they they need some of them need governance improvements but um they definitely also have young population Saudi think 70% of the population is under 30 so you know just to your point um that the that very very very hopeful in those areas uh Ben uh gtra asks do,Politics & the Future of Tech - 016,14,16
124,a16z,Politics & the Future of Tech,Q: Will the lobbying efforts by good-faith American crypto firms will be able to move the needle politically?,6145,6256,um that the that very very very hopeful in those areas uh Ben uh gtra asks do you think the lobbying efforts by good faith American crypto firms will be able to move the needle politically in the next few years um what areas make you optimistic as it relates to American crypto regulation crypto blockchain web 3 yeah so I think that I I I'm hopeful I'm as hopeful as I've ever been um so there's a there's a bunch of things that have been really positive first of all you know the SEC has lost I think five cases in a row so you know like some of their like arbitrary um enforcement of things that aren't laws is not working um secondly uh you know there was a bill that passed through the house um or the house Financial Services committee uh which is a very I would say good bill on crypto regulation and you know hopefully that will eventually pass the house and the Senate um there's uh you know we we've seen um Wyoming I think adopt really good new laws around Dows um and so there's uh some progress there um and then you know there's been uh you know we've been working really really hard to educate members of Congress um and the administration on kind of the value of the technology uh there are strong opponents to it you know as I mentioned earlier and you know that's you that continues to be worrisome um but uh but I I think we're making great progress and and the fair Shake pack has done a just a tremendous job of you know kind of backing Pro crypto candidates and with great success there were six different races on super Tuesday that they backed and all six uh won so um you know another good sign y fantastic hit a couple other topics here quickly to get under the wire um so Father Time asks,Politics & the Future of Tech - 017,15,17
125,a16z,Politics & the Future of Tech,Thoughts on recent TikTok legislation,6256,6475,couple other topics here quickly to get under the wire um so Father Time asks can you give us your thoughts on the recent Tik Tock legislation if passed what does this mean for big Tech going forward and so I'll just let me give a quick swing at that um so the Tik Tock legislation being proposed by the US Congress and currently being taken up in the Senate which by the way uh and the President Biden has already said he'll sign it if it if it if the Senate and the House pass it um this is legislation that would require uh a require a divestment of Tik Tok from its Chinese parent company bite dance um and so Tik Tok would have to be a purely American company or would have to be owned by a purely American company um and then failing that it would be a ban of Tik Tok um uh in in in the US um this bill is a great example of the sort of bipartisan dynamic in DC right now on the topic of China which is this bill is being enthusiastically supported by the majority of politicians on both sides of the aisle um I think it passed out of its committee um like 50 to zero um which you know is basically like it's impossible to get anybody in DC to agree on anything right now except basically basically this so this is like super bipartisan and then it's you know the the head of that committee is a republican uh Mike Gallagher and and you know he immediately he worked in a bipartisan way with his committee members but you know the Democratic White House immediately endorsed it bill so so like you know this bill has like serious momentum the Senate is taking up right now um they're going to they're likely to modify it in some way but it it seems you know reasonably like reasonably likely to pass based on what what we can see um you know I would say like I said by overwhelmingly bipartisan support um and you know look the argument for the ban is I would say a couple different way or not the the divestment or the ban number one is just like you know a an app on every on Americans on the phones of every you know a large percent of Americans with the surveillance and potential propaganda kind of aspects of that you know certainly has people in Washington concerned and then quite frankly there's an underlying industrial you know dynamic which is you know the you know the US the US internet companies can't operate in China so there's a you know there's there's a sort of an unfair symmetry underneath this that really undercuts you know I think a lot of a lot of the a lot of the arguments for bite dance um it has been striking to see that uh there are actually opponents of this Bild who have emerged and I would describe on sort of the further to the right and further to the left in their respective parties um and um you know they they you know that those folks and I won't go through detail but those folks make a variety a variety of of arguments um one of the and let mect characterize the Le the surface level I think on the on the further on the left I think that there are people who think that um uh especially kind of further left Congress people who feel like Tik Tok is actually a really important and vital messaging uh system for them to be able to use with their constituents who tend to be younger um in very internet and so so there's that which you know is interesting um but then on the on the further on the right um there is a lot and our friend David sax for example might be an example of this um there are a fair number of people who are very worried that um the US government is so prone to abuse any regulatory capability with respect to Tech and especially with respect to censorship that basically if you hand the US government any new regulatory Authority or legal Authority at all to come down on Tech it will inevitably be used not just against the Chinese company but it will also then be used against the American companies and so you know it's kind of it's it's there you know some drama that's surfacing around this and and you know we we'll see whether the opponents can um you know can kind of kind of pull it through um you know look quite frankly I you know I I you know without coming down particularly on like I think there's there's one of those cases where there's actually like excellent arguments like on all three sides like I think you there are like very legitimate questions here um and so um you know I think it's great that the issue is being confronted but I think it's also great that the you know that the arguments have been surfaced and that we're going to you know hopefully figure out the right thing to do um couple closing things um,Politics & the Future of Tech - 018,16,18
126,a16z,Politics & the Future of Tech,"Q: How do you find common ground with groups that you can benefit from working with, but are opposed ideologically?",6475,6587,know hopefully figure out the right thing to do um couple closing things um uh close on let's see hopefully a semi- optimistic note so um John Potter asks how do you most effectively find common ground with groups and interests that you benefit from working with but with which you are usually opposed ideologically or otherwise I mean I think this is uh you know there's this term in in Washington Common Ground um and I think that you know you always want to start by finding the common ground because I'll tell you something in politics generally is most people have the same intention um you know like in Washington in fact you know people want life to be fair um you know they want they don't want people to go hungry um they want you know citizens to be safe but have plenty of opportunity so like there's a lot of common ground the the differences lie not in the intent but how you get there like what is the right policy to achieve the goal and you know so I think it's always important to start with the goal and then kind of work our way through you know why we think our policy position is correct um you know like we don't really have a lot of disagreements on stated intent at least I mean I think there are some intentions um that are that are very difficult in Washington you know like the you know the the intention to kind of control the financial system is a you know from the government or nationalize the banks or kind of achieve the equivalent of nationalizing the banks is you know when you have that intent that's tough but like if you start with you know most intentions are I think um you know shared between you know us and policy makers on both sides and then,Politics & the Future of Tech - 019,17,19
127,a16z,Politics & the Future of Tech,Q: Would either of you ever consider running for office?,6587,6793,you know shared between you know us and policy makers on both sides and then we'll close on this great question Zach asks would either of you ever consider running for office and for fun what would be your platform um so I want uh just because look you know I think being a politician requires a certain kind of skill set and attitude and and energy from certain things that that I I don't possess unfortunately do you have a do you have a platform you would run on if you did run yeah okay yeah let's hear your platform uh the American dream um so I I I won't do it now but I like to put up this chart that shows the change in prices in different sectors of the economy over time and what you basically see is the price of like television sets and software and video games are like crashing hard right in a way that's like great for consumers you know like I you I saw 75 inch flat screen ultra high depth TVs now or down below $500 like you know just it's great it's amazing like when technolog is allowed to work its magic like prices crash in a way this just great for consumers and it's equivalent of a giant basically you know when prices drop it's equivalent of a raise um um uh so makes makes human welfare a lot better um the three uh uh elements of the economy that are Central to the American dream are uh Healthcare education and housing right and so if you think about what does it mean to have the American dream it means to be able to buy and own a home it means being able to send your kids to Great Schools get great education to have a great life uh and then it means you know great Healthcare to be able to take care of yourself and your family the prices on those are skyrocketing uh they're just like straight to the moon and of course those are the sectors that are the most controlled by the government there where there's the most subsidies for demand from the government there's the most restrictions on Supply from the government and there is the most interference with the ability to field uh technology and and startups um and the result is we have an entire generation of kids um who basically I think are quite rational and looking forward and basically saying I'm never going to be able to achieve the American dream I'm never going to be able to own a home I'm never going to be able to get a good education or send my kids to good education I'm not going to be able to get good healthare basically I'm I'm I'm I'm not going to be able to live the life that my parents live or my grandparents live and I'm not going to be a I'm not going to be able to fundamentally form a family provide for my kids and I think that's the I think that's I my opinion that's the underlying theme to kind of what has gone wrong um sort of socially politically um psychologically uh in the country that's what's led to the sort of intense level of pessimism That's What's led to sort of this attraction you know kind of very zero some uh politics to uh you know recrimination over over over optimism and building um and so I I would I would I would I would confront that absolutely directly um and then of course I would point out that I I don't think anybody in Washington is doing that right now so either either I would either I would win because I'm the only one saying it out loud or I would lose because nobody cares but I I think it would uh I've always wondered whether that actually would would whether both on the substance and the and on the message whether that would be a uh the right platform yeah no it would certainly be the thing to do as the thing it's very complex in that um you know Health Care policy is largely National but education policy and housing policy has also got a very large local component so it' be a compli kind of complicated set of policies that you'd have to enforce you,Politics & the Future of Tech - 020,18,20
128,a16z,Politics & the Future of Tech,Sign off,6793,6807,compli kind of complicated set of policies that you'd have to enforce you uh we still have a ton of questions so we may do part two on this at some point but we really appreciate your time and attention and we will see you soon okay thank you [Music],Politics & the Future of Tech - 021,19,
129,a16z,Safety in Numbers: Keeping AI Open,Introduction to scaling laws and their impact,0,134,scaling laws now these underpin the success of large language models today but the relationship between data sets compute and the number of parameters was not always clear in fact in 2022 a pivotal paper came out that changed the way that many people in the research Community thought about this very calculus and it demonstrated that data sets were actually more important than just the sheer size of the model one of the key authors of this paper was Arthur MCH who was working at Deep Mind at time now earlier this year Arthur banded together with two other researchers yam Lampo and timate laqua two researchers at meta who worked on llama and together the three of them founded a new company mrol that team has been hard at work releasing mrol 7B in September a state-of-the-art open source model that quickly became the go-to for developers and they just released as in in the last few days a new mixture of experts model that naturally they're calling mixt so today you'll get to hear directly from Arthur as he sits down with a16z General partner Anan maida as the Battleground for large language models heats up to say the least together they discussed the many misconceptions around open source and the war being waged on the industry plus the current performance reality of open versus closed models and whether that Gap will realistically close with time plus the kind of compute data and algorithmic Innovations required to keep scaling llms efficiently now it's really rare to have someone at the frontier of this kind of research be so candid about what they're building and why so I hope that you come out of this episode as excited about the future of Open Source as I did enjoy as a reminder the content here is for informational purposes only should not be taken as legal business tax or investment advice or be used to evaluate any investment or security and is not directed at any investors or potential investors in any a6c fund please note that a16z and its Affiliates may also maintain investments in the companies discussed in this podcast for more details including a link to our investments please see az.com,Safety in Numbers: Keeping AI Open - 001,,32
130,a16z,Safety in Numbers: Keeping AI Open,Arthur Mensch and the Founding of Mistral,134,477,disclosures you've got uh quite the founding team story you know we flashback to a few years ago labs are building Foundation models and the consensus across the research Community was that the size of these models was What mattered most you know how many million billion parameters went into the model seemed to be the primary debate that people are having but it seems like you had a hunch that data sets mattered more could you just give us the backstory on the chinchilla paper you co-wrote you know what were the key takeaways on the paper and how was it received yeah so I guess the backstory is that uh 2019 2020 people were relying a lot on um on a paper called scaling lows for large language models that was advocating for uh basically scaling infinitely the size of models and keeping so number of data points uh rather fixed so do saying that if you had like four times the amount of compute you should be mostly multiplying by 3.5 your model size and then maybe by 1.2 uh and so a lot of work was actually done on top of that so in particular at Deep mine when I joined uh I I joined a project called gopher and and that's a there was a misconception there there was also a misconception on gpt3 and basically in 2021 every paper uh made this mistake and at the end of 2021 we started to realize there were some issues and as it turns out we turned back to to the mathematical paper that was actually talking about scaling lows and it was a bit hard to understand and we figured out that actually if you thought about it bit more in a theoretical perspective and if you looked at like empirical evidence we had um it didn't really make sense uh to actually grow the model size faster than the data size and we did some measurement and as it turned out what was actually true was actually what we expect which is in common words if you multiply by four uh your compute capacity you should multiply by two the model size and by two the data size that's approximately what you should be doing which is good because if you move everything to Infinity everything remains consistent so you don't have a model which is infinity big or a model which is infinity small with infinite compression or like close to zero compression so it really makes sense and as it sounds out it's really what You observe if you look at uh if you do multiple runs and and so that's how we we train chinchila and and that's how we wrote the chinchila paper at the time you know you were at Deep mind and your co-founders were at meta what's the backstory around how you three end up coming together to form mrol after the compute optimal skating laws work that you just described so we've known each other for a while because Gom and I were in school together and timoth and I were in master together in Paris basically we had like very parallel careers Timo and I we actually work together as well again when I was doing a post talk um in mathematics uh and then I joined deep mine as Gan timot uh went to become permanent researchers at meta uh and so we continued doing this I was doing large language models in between 2020 and 2023 Gom and timote were working on um solving U mathematical uh problems with large langage models and if I understand correctly I wasn't there but they realized they had to have stronger models and they started to do large language models at this point uh so like I guess a year after I started uh and on my side I was mostly working on a small team at uh Deep Mind so we did very interesting work on uh we retro which is a paper doing retrieval for large language models we did the chinchila then uh we I I was in the team doing Flamingo which is actually one one of the good way of doing a model that can see things I guess when chbt went out we knew I mean we knew from before that the technology was very very much gamechanging but it was kind of a signal that there was a strong opportunity for building a small team uh focusing uh on a different way of Distributing the technology so redoing things in a more open source manner uh which was not the direction what that Google at least was taking and and so we had this opportunity then we left the company uh at the beginning of last year and and created the team that started to work on the 5th of June and if I recall correctly right before they left Tim and guom had started to work on llama right over at meta could you just describe that project and how it was related to the chinchilla scaling laws work You' done so Lama was uh like a small team reproduction of chinchila at least in the in its approach of the parameterization and all of these things uh it was uh one of the first papers that established that you could go beyond the chinchila scaling lows so chinchila scaling lows tell you what you should be training if you want to have an optimal model uh for a certain compute cost at training time but if you take into account the fact that your model should also be efficient at inference time you probably want to go far beyond the chinula scaling low so it means you want to overtrain the model so train on more tokens than would be optimal for performance but the reason why you do that is that you actually compress models more and then when you do inference you end up having a model which is much more efficient uh for a certain performance so by spending more time during training you spend less time during inference and so you save cost and I think uh that was something we well I guess we observed that at Google also but uh the Lama paper was the first to establish it in the open and it opened a lot of opportunities yep I remember you know,Safety in Numbers: Keeping AI Open - 002,31,33
131,a16z,Safety in Numbers: Keeping AI Open,Mistral 7b and the launch of Mixtral,477,807,the open and it opened a lot of opportunities yep I remember you know the both the impact of the chinchilla scaling laws work on the labs on on multiple Labs realizing just how unoptimal the the compute setups were right um and then the impact of llama being dramatic on the industry and realizing how to be much more efficient about inference time so I can imagine that those were some of the the top insights on your mind and the top concerns on your mind when you guys left uh to start mistol so let's fast forward to to today you know it's December 2023 we'll get to the role of Open Source in a bit but let's just level set on what you've built so far you know a couple months ago You released mistol 7B which was a best-in-class model um and this week you're releasing a new mixture of experts model so just tell us a little bit more about mixol I believe is what you're calling it and how it compares to other models yeah so mix is our new model that wasn't released in Open Source before in a in a usable form uh so it's a technology called sparse mixture of experts uh which is quite simple you take all of the dense layers of your Transformer and you duplicate them you call these layers expert layers and then what you do is that for each exp for each token that you have in your sequence uh you have a router mechanism just a very simple Network that decides which expert should be looking at which token and so you send all of the tokens to their experts and then you apply the experts and you get back the the the output and you combine them and then you go forward in the network you have eight experts per layer and you execute only two of them so what it means at the end of the day is that you have a lot of parameters on your model you have uh 46 billion parameters but the thing is that the the number of parameters that ex that you execute uh is much lower than that because you only execute two branches out of eight and so at the end of the day you only execute 12 billion parameters per token and this is what counts for latency and throughput and for performance so you have a model which has the performance of a 12 billion parameter Network that can uh that have performance that are much higher than what you could get even by compressing data a lot on a 12 billion dense Transformer so Spar mixure of experts is a technology uh that allows to be much more efficient at inference time and also much more efficient at training time so that's the reason why we mates to develop it very quickly just for folks who are listening um who might not be with sort of state-of-the-art architecture and language models could you just describe the difference between you know dense models which have been the primary architecture today and mixture of experts intuitively what are the biggest differences between these two architectures so they are very similar except on the what we call the D Network so the the you know in in the dense Transformer you have you alternate between an attention layer and and a dense layer generally that's that's the idea a spar mixture of experts you you take the dense layer and you duplicate it several times and so that's where you actually increase the number of parameters so you increase the capacity of the model without increasing the cost so that's a way of decoupling the memorization and what you can remember the capacity of the network to its cost at inference time if you had to describe the biggest benefits for developers as a result of that inference efficiency it's cost and and latency so you can have uh usually that's what you look at when you're a developer you want something which is cheap and you want something which is fast generally speaking the just the trade-off uh is strictly favorable in using mix compared to using a 12 billion dense model and the other way to think about it is that if uh you want to use a model which is as good as L 270b you should be using Mixr because m is actually on par with Lama 270b while being approximately six times cheaper or six times faster for the same price could you talk just a little bit about why it's been so challenging for folks to uh for research labs and research teams to really get the mixture of experts model right it sounds like you know for a while now folks have known that the dense model architecture that all of us have been using in in sort of the most notable products or the most well-known products um are slow uh they're expensive and they're difficult to scale and so for a while people have been looking for an alternative architecture that could be like you were saying cheaper could be faster could be more efficient what were some of the biggest challenges you have to figure out to get thee model right well I guess I won't disclose all Trade Secrets but uh there's basically two challenges the first one is you need to figure out how to train it correctly from from a mathematical perspective the other challenges is to train efficiently so how to use actually a hardware uh as efficiently as possible you have like new challenges coming from the fact that you have tokens flying around from one to one expert to another uh that creates some communication constraints and you need to figure out um uh you need to make it fast and then on top of that you also have new constraints that apply when you deploy the model uh you do need to do inferencing uh efficiently and that's also the reason why we released an open source package based on VM so that the community can can take also this code and modify it and see how that works yeah obviously we're excited to see what the community does with thee uh mixol release you're putting out this week let's talk about open source and,Safety in Numbers: Keeping AI Open - 003,32,34
132,a16z,Safety in Numbers: Keeping AI Open,"Misconceptions about open source, the state of open vs. closed models, and future requirements for scaling LLMs",807,1121,mixol release you're putting out this week let's talk about open source and approach and a philosophy that's that's permeated all the work you've been doing so far um why choose to tackle this increasingly competitive space with an open source approach which has been which is quite different from the way everybody else is approaching it I guess it's a good question the the answer is that it's partly ideological and and partly pragmatical um we have grown with the field of AI uh that when from 2012 we were detecting cat and go cats and dogs and in 2022 we were actually generating text that looked humanik so really made a lot of progress and if you look at the reason why we made all of this progress uh well most of it is explainable by the free flow of information so you had academic Labs you had very big uh industry backed Labs communicating all the time about the results and building on top of uh each other results and that's the way we went from we increased significantly uh the architecture and training uh techniques uh we we just made everything work as a community and all of a sudden in 2020 with gpt3 this tide kind of reversed and uh companies started to be more opaque about what they were doing because they they they realized there was actually a very big market and all of a sudden 2022 on the important aspects of AI and on llms we had we went and Beyond chinchila there were basically no communication at all and that's something that that I as a researcher and and timot and Gom and all of the people that joined us as well deeply regretted uh because we think that we're definitely not at the end of the story we need to invent new things there's no reason why to stop now uh because the technology is effectively uh good but not working completely well enough and so we believe that it's still the case that we should be communicating a lot about uh models we should be allowing the community to take the models and make it their own and that's that's a some ideological reason why we went into that the other reason is that we are talking to developers uh developers want to modify things and and having a deep access to to to very good model is is a good way of engaging with this community and uh well and I guess addressing their needs so that the platform we're building as well is going to to be used by them so that's that's also like a a business reason obviously uh as a business we we do need to have a valid Mone ization Approach at some point uh but we've seen many businesses build open core approaches uh and have a very strong open source community and also a very good offer of services and that's what we want to build that resonates I I remember a very detectable shift you're right you know the early days of of deep learning were largely driven by a bunch of open collaboration between researchers from different Labs who would often publish all their work and share them at conferences you know Transformers ly was published and and opened to the entire research Community um but that has has definitely changed yes so I think there's some level of open sourcing uh in in Ai and so we offer the open uh the weights and we offer the inference code that's like the end product that is already super usable so uh it's already a very big step forward compared to closed apis because you can modify it and you can look at what's happening under the hood look at activations and all so you have inter interpretability and the possibility of modifying the model to adapt it to some editorial tone to adapt it to proprietary data to adapt it to some Specific Instructions which is something that is actually much harder to make if you only have access to a close Source API um and that's something that also goes with our approach of the technology which is to say pre-train model should be neutral and we should Empower our customers to take these models and just put their editorial approaches there are instruction they Constitution if you want to talk like entropic into the model so that's the that's the way we approach the technology we don't want to pour our own biases into the into the pre-train model on the other hand we want to enable the developers to control exactly how the model behaves uh and what kind of biases it has what what kind of biases it doesn't have so we we really take this modular approach and that goes very well with the fact that we release uh some very strong open we models could you just ground Us in the reality of where these models are today just to give people a sense of of where in the timeline we are is open source really a viable competitor to proprietary close models or is there a performance Gap you know what are the trade-offs or limitations that people should be aware of uh with open source so mix is as similar performance to GPT 3.5 so that's the that's a good grounding internally we have SW models that are in between 3.5 and four that are basically second or third the second or third best model in the world so really we think that the Gap is closing uh the Gap is approximately six months at that point and the reason why it's it's six months is that it actually goes faster if you do open source things because you get the community uh modify the model toest very good ideas that can,Safety in Numbers: Keeping AI Open - 004,33,35
133,a16z,Safety in Numbers: Keeping AI Open,"Open Source in AI: Scaling laws, Industry Impact, data efficiency, and new model architectures",1121,1376,because you get the community uh modify the model toest very good ideas that can then be Consolidated by us for instance and and we just go faster because of that so it has always been the case that open source at the end well ends up being going faster and that's the reason why uh the entire inet rends on Linux I don't see why it would be any different for AI uh obviously there's some constraint that are slightly different because the infrastructure cost is quite High uh to train a model it cost a lot of money but uh but I really think that we'll converge to a setting where you have propri ey models and the open source model are just as good and I think eventually the field will be much more open because if you want to go beyond the the biggest model today you do need to find new paradigms and so that means that we also need to do research H and that's we're very excited by this perspective because we like competitive environment and research yeah so let's talk about that a little bit more how are you seeing people use and innovate on the open source models and are there any use cases that diverge from proprietary close models at all I think we've seen several categories of usage um you had there's a few companies that know how to strongly find you models to their needs so they took mistal 7B had a lot of human annotations had a lot of proprietary data just modify mral 7B so that it solve their task just as as well as gbt 3.5 but only for a lower cost and a higher level of control we've also seen I think very interesting Community efforts in adding capabilities to mral 7B so we saw like a context length extension to 128k that worked very well again was done in the open so like the recipe was available and this is something that we were able to consolidate we've seen imag en coders to make it a visual visual language Model A very actionable thing that we saw is uh I think the hugging face folks first did the direct preference optimization on top of M 7B and made a very strong much stronger model than the instructed model we proposed at the early release and it turned out it's actually a very good idea to do it and so that's something that we've Consolidated as well uh so generally speaking uh just the community is super eager to just take the model and add new capabilities put it on the laptop put it on on an iPhone I saw m 7B on an iPhone I saw Mr 7B on the stuffed parot as well so fun things useful things uh but generally speaking it's been super exciting to see the research Community take a hold of of our technology and with mix which is a new architecture I think we're are also going to see much more interesting things because on the interpretability field also on the safety field as it turns out you have a lot of things to do when you have deep access to an open model and so we're really eager to to help that uh and to engage with the community safety you know this an important I think piece to talk about the immediate reaction of a lot of folks is to deem open source less safe than closed models how would you respond to that so I think we believe that it's actually not the case for the current generation of model uh models that we are using today are not that much are not much more than just a compression of whatever is available on the internet so it does make access to knowledge more food uh but this this is the story of humanity making knowledge uh access more fre it so it's no different than uh inventing the printing machine where we had apparently similar debate it wasn't there but that was the debate we had uh so we are not making the world any time uh any less safer by providing more interactive access to knowledge so that's the first thing now the other thing is that you do have immediate risk of misusage of large language models and uh you do have them for open source models but also for closed models and so the the way you do address these problems and come up with counter measures is to know about them uh so you need to know about uh about bridges basically and that's the same way in which you need to know about bridges on on operating systems and on uh on networks and so it's no no different for for AI uh putting uh models under the highest level of scrutiny is a way of knowing how they can be misused and it's a way of coming up with conter measures and I think a good example of that is,Safety in Numbers: Keeping AI Open - 005,34,36
134,a16z,Safety in Numbers: Keeping AI Open,Safety concerns of open source models.,1376,1517,a way of coming up with conter measures and I think a good example of that is that it's actually super easy to exploit an API uh it's super easy especially if you have fine tuning access to make gp4 behave uh in a very bad way and it's um since it's the case and it's always going to be the case it's super hard to be adversarially robust it means that we're only trusting the team uh of large companies to figure out ways of addressing these problems whereas if you do open sourcing you trust the community and the community is much larger um and so if you look at the history of software in cyber security in operating systems that's the way we made the system safe and so if we want to make the current AI system safe and then move on to a Next Generation that potentially will be even stronger and then we can re have this we can have this discussion again well you do need to do open sourcing so today we think that open sourcing is the safe way yeah I think this is this is not understood right widely that um when you have thousands or hundreds of thousands of people able to Red Team models because it's open source the likelihood that you'll detect biases and um built-in breaches and risks are just dramatically higher um and I think if you were talking to policymakers how would you help advise them how do you think they should be thinking about regulating open source models given that you know the safest way often to battle Harden software and tools is to put them out in the open well we've been saying that precisely this that um that the current technology is not dangerous on the other end the fact that we we are effectively making them stronger means that we need to monitor what's happening empirically monitor performances the best way of empirically monitoring software performances is is through open source so that's what we've been saying um there's been some effort to try to come up with very complex governance structure where where uh you would have like several companies talking together having some safe space some safe soundbox uh for uh red teer that would be potentially independent so things that are super complex uh but as it turns out if you look at the history of software the only way we did software collaboratively is through open source so why change the recipe today where uh,Safety in Numbers: Keeping AI Open - 006,35,37
135,a16z,Safety in Numbers: Keeping AI Open,Recommendations for policymakers in regulating AI technologies.,1517,1997,"collaboratively is through open source so why change the recipe today where uh the the technology we're looking at is actually nothing else than the compression of the internet so that's the that's what we've been saying to The Regulators uh J another thing we've added to the regulator is that if they want to enforce that AI products that needs to be safe like like if you if you want to have a diagnosis assistant you want it to be safe right well in order to Monitor and to evaluate whether it's actually safe you need to have some very good tooling and the the tooling requires to have access to llms and if you access close Source apis llms where you're bit in a in a in trouble water because it's hard to be independent in that setting so we think that independent controller of product safety should have access to very strong open source models and should own the technology and if open source llms were to fail relative to close Source models why would that be well I guess the the regulation burden is is uh potentially one thing that that could uh make it harder to uh to release open source models it's also generally speaking it's a very competitive market and I think in order for open source models to be widely adopted they need to be as strong as open as a close Source model they have a little Advantage because you do have more control and so you can do heavier fing and so you can make performance jump a lot on a specific task because you have deep access uh but really at the end of the day um developers look at performance and and latency and so that's why why we think that as a company we need to be very much on the frontier if we want to be relevant given the complexity of uh Frontier models and Foundation models in these systems there are just tons of misconceptions that folks have about these models and so if you step back and we look at the Battle that's raging between folks um uh pushing for closed Source systems and versus the open source system what do you think is at stake here what do you think the battle is for well I think the battle is for the neutrality of the technology like a technology by ense is something neutral you can use it for bad purposes you can use it for good purposes if you look at what the llm does it's not really different from programming language it's actually used very much as a programming language by the application makers there's a strong um confusion made between what we call a model and what we call an application and so a model is really the programming language of a application so if you talk to all of the startups doing amazing products with generative AI they're using llms just as um as a function and on top of that you have a very big systems with filters with decision making with control flow and all of this things and what you want to regulate if you want to regulate something is the system the system is it's the product so for instance um a healthcare diagnosis assistant is is an application you want it to be non-biased you want it to take good decisions uh even under high pressure so you want its statistical accuracy to be very high and so you want to measure that and it doesn't matter if it uses a large language Mo uh under the hood what you want to regulate the application and the issue we had and the issue we're still having now is we hear a lot of people saying we should regulate the tech so we should regulate the function the mathematics behind it but really you never use a large language model itself you only always use it in an application in a in a in a way with a user interface and so that's the one thing you want to regulate and what it means is that companies like us like foundational model companies will obviously make the model as controllable as possible so that the applications on top of it can be compliant can be safe uh we'll also build the tools that allow to measure the compliance and the safety of the application because that's super useful for the application makers it's actually needed but there's no point in regulating something that is neutral in itself that is just a mathematical tool so I think that's the one thing that we've been hammering a lot uh I think we've been which is good uh but there's still a lot of effort uh in uh I guess in making this strong distinction which is super important to understand what's going on so to regulate apps not math seems like you know the right direction that a lot of folks who are who understand the inner workings of these models and how they're actually implemented in in reality um are advocating for what do you think um is the best way to clear up the this misconception for folks who don't maybe don't have technical backgrounds don't actually understand how Foundation models work and how the scaling laws work so I've been using a lot of metaphors I guess to to to make it understood large language models are like programming languages uh and so you don't regulate uh programming languages you regulate malwares you you ban malwares we've also been actively uh vocal about the fact that pre-market conditions like flops the number of of flops that you do to create a model is definitely not the right way of doing uh of measuring the performance of a model right we um we're very much in favor of having very strong evaluations that's as I've said uh this this is something that we want to provide to our customers the ability to evaluate our models in their application uh and so I think this is a very strong thing um to well that we've been stressing we want to provide the tools for application makers to be compliant that's the that's something we have we've been saying and so we find it a bit unfortunate that uh we haven't we haven't been heard everywhere and that there's still a big focus on the tech uh probably because things are not completely well understood because it's a very complex field and it's also a very fast moving field uh but eventually I think I'm I'm very optimistic that we'll find a way to uh continue innovating uh while having safe products but also uh high level of competition on on the foundational mod uh layer well let's let's Channel your optimism a little bit you know there's there's very few people who have the ground level understanding of scaling laws um like you gam and Tim and your team when you step back and you look at the entire space of language modeling in addition to open source what are the key differentiators that you see in the next wave of Cutting Edge models um things like you know uh self-play you have process reward models um the uses of synthetic data uh if you had to conjecture what do you think some of the most exciting or important breakthroughs will be in the field going forward so I guess it's good to start with diagnosis so what is uh what is not working that well so reasoning is not working that well and it's super inefficient to train a model uh if you compare like the training process of of a large language mobel to the brain you have like a factor I think 100,000 so really there's some progress to be made in term of data efficiency so I think the the frontier is increasing data efficiency increasing reasoning capabilities so adaptive comput is one way uh and when to increase data efficiency you do you need to work on coming up with very high quality data filtering things uh many new techniques that needs to be invented still but that's really where the lock is uh data is the one important thing and the ability of the model to decide how much computed want to allocate to certain problem uh is definitely on the",Safety in Numbers: Keeping AI Open - 007,36,38
136,a16z,Safety in Numbers: Keeping AI Open,Predictions on how advancements in LLMs will change user interactions with technology,1997,2196,how much computed want to allocate to certain problem uh is definitely on the frontier as well so these are things that we're actively looking at you know this is a raging debate right we've and we've talked about this a few times before which is um Can models actually reason today do they actually generalize out of distribution what's your take on it and what do you think is required to to exhibit what what would convince you that models are actually capable of multi-step complex reasoning yeah it's very hard because you train on the entire human knowledge and so you have a lot of reasoning places so it's uh it's hard to say whether they reason or not or whether they do retrieval of reasoning and it looks like reasoning right uh I guess at the end of the day what matters is whether it works or not and on many simple reasoning task it does so we can call it reasoning it doesn't really matter if they reason like we do we don't even know how we reason so right so we are not going to know about how machines reason anytime soon yeah um so yeah it's a it's a raging debate uh the reason the the way you do evaluate that is to try to be as out of distribution as possible uh like working on on mathematics uh is not something I've ever done but that something that Timo and Gom have are very sensitive to because they've been doing it for a while uh when they were at meta that's probably one way of measuring whether you have a very good model or not and actually if you look at um we're starting to see some very good mathematicians uh I'm thinking of Teran St right that are using large language models for some things uh obviously not the high level reasoning but for some part of their proofs and so I think we will move up in the abstraction uh and the question where does that stop we do need to find new new paradigms to uh to go one step forward and and we we will be actively looking for them we've talked a lot about developers so far if you had to sort of Channel your product View and and sort of just conjecture on what these advances in scaling laws in in in representation learning in get teaching the models to to reason faster better cheaper what will these advances mean for end users in terms of how they consume how they program and they generally work with models what we think is that uh fast forward five years uh everybody will be using their specialized uh models Within part of complex applications and systems developers will be very um looking at latency so they will want to have for any specific task of the system they will want to have the lowest cost and lowest latency and the way you make that happen is that uh you will ask for the task ask for user preferences ask for what you want the model to do and you try to make the M as small as possible and as suitable to the task as possible and so I think that's the way we'll be evolving on the developer space I also think that U generally speaking the fact that we have access to large language models is going to reform completely the way we interact with machines and the internet of five years five years from now is going to be much different so much more interactive uh because I think this is already unlocked I mean it's just about making very good applications with very fast systems uh with very fast models so yeah very exciting times ahead so what would those interaction modalities look like yeah so that's very,Safety in Numbers: Keeping AI Open - 008,37,39
137,a16z,Safety in Numbers: Keeping AI Open,Potential applications in various fields like gaming and enterprise.,2196,2333,so what would those interaction modalities look like yeah so that's very interesting and I think in in games for instance it's going to be fascinating uh we've seen some very good applications you do need to have small models because you want to have swarms of it and it start to be bit costly if you if it's too big uh but having them interact is just going to make pretty complex systems and interesting systems to observe and to use uh so uh so we have a few friends making applications in the Enterprise space space with different Persona playing different roles relying on the same language model but with different prompts and different uh functioning um and I think that's going to be quite interesting as well to uh to look at as I've said complex applications in in in three years time are just going to use different parts different llms for different parts and that's going to be quite exciting well what's your call action to builders researchers folks who are excited about the space what would you ask them to do I I would take uh mistal models and try to build amazing applications uh the way many developers had uh it's not that hard uh it's the stack is starting to be pretty clear pretty efficient uh you only need a couple of gpus you canot even do it on your MacBook Pro if you want it's going to to be a bit hot but uh uh but it's good enough to to do interesting applications uh really the way we do software today is very different from the way we did it from last year and so I'm really calling application makers to action because we we are going to to try to enable them to to build as fast as possible thank you so much for listening to the a6c podcast what we're trying to do here is provide an informed cleared but also optimistic view of technology and its future and we're trying to do that by featuring some of the most inspiring people and the things they're building and so if you believe in that and you'd like to join us on this journey make sure to click subscribe but also let us know in the comments below what you'd like to see us cover next thank you so much for listening and we will see you next [Music],Safety in Numbers: Keeping AI Open - 009,38,40
138,a16z,Safety in Numbers: Keeping AI Open,"Call to action for builders, researchers, and developers",2333,2343,time,Safety in Numbers: Keeping AI Open - 010,39,
139,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",Introduction,0,65,"- I think compute is gonna be
the currency of the future. I think it'll be maybe the
most precious commodity in the world. I expect that by the end of this decade. And possibly somewhat sooner than that, we will have quite capable systems that we look at and say, wow,
that's really remarkable. The road to AGI should be
a giant power struggle. I expect that to be the case. - Whoever builds AGI
first gets a lot of power. Do you trust yourself
with that much power? The following is a
conversation with Sam Altman, his second time in the podcast. He is the CEO of OpenAI, the company behind GPT-4, ChatGPT, Sora, and perhaps one day the very
company that will build AGI. This is Lex Fridman Podcast. To support it, please check out our
sponsors in the description. And now, dear friends, here's Sam Altman.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 001",,65
140,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",OpenAI board saga,65,1111,"Take me through the OpenAI board saga that started on Thursday, November 16th, maybe Friday, November 17th for you. - That was definitely the most painful professional
experience of my life and chaotic, and shameful, and upsetting and a bunch of other negative things. There were great things about it too and I wish it had not been in such an adrenaline rush
that I wasn't able to stop and appreciate them at the time. I came across this old tweet of mine or this tweet of mine
from that time period, which was it was like kind
of going to your own eulogy, watching people say all
these great things about you and just like unbelievable support from people I love and care about. That was really nice. That whole weekend I kind of like felt
with one big exception, I felt like a great deal of love and very little hate even though it felt like I
have no idea what's happening and what's gonna happen here and this feels really bad. And there were definitely times I thought it was gonna be
like one of the worst things to ever happen for AI safety. Well, I also think I'm happy that it happened relatively early. I thought at some point between when OpenAI started and when we created AGI, there was gonna be something crazy and explosive that happened, but there may be more crazy
and explosive things happen. It still I think helped us
build up some resilience and be ready for more
challenges in the future. - But the thing you had a sense that you would experience is
some kind of power struggle. - The road to AGI should
be a giant power struggle. Like the world should... Well, not should. I expect that to be the case. - And so you have to go through that, like you said, iterate as often as possible in figuring out how to
have a board structure, how to have organization, how to have the kind of people
that you're working with, how to communicate all that in order to deescalate the power struggle as much as possible, pacify it. - But at this point, it feels like something
that was in the past that was really unpleasant and
really difficult and painful. But we're back to work
and things are so busy and so intense that I don't spend a lot of time thinking about it. There was a time after. There was like this fugue state for kind of like the month after, maybe 45 days after that was I was just sort of
like drifting through the days, I was so out of it. I was feeling so down - [Lex] Just on a personal
psychological level. - Yeah. Really painful. And hard to have to keep running OpenAI in the middle of that. I just wanted to crawl into a cave and kind of recover for a while. But now it's like we're just back to working on the mission. - Well, it's still useful to go back there and reflect on board structures, on power dynamics, on how companies are run, the tension between research,
and product development, and money and all this kind of stuff so that you have a very high potential of building AGI would do so
in a slightly more organized, less dramatic way in the future. So there's value there to go both the personal
psychological aspects of you as a leader and also
just the board structure and all this kind of messy stuff. - Definitely learned a lot
about structure and incentives and what we need out of a board And I think that it is valuable that this happened now in some sense. I think this is probably not like the last high
stress moment of OpenAI, but it was quite a high stress moment. Company very nearly got destroyed. And we think a lot about many of the other things we've
gotta get right for AGI. But thinking about how
to build a resilient org and how to build a
structure that will stand up to a lot of pressure the world, which I expect more and
more as we get closer. I think that's super important. - Do you have a sense of how deep and rigorous the deliberation
process by the board was? Can you shine some light on just human dynamics involved
in situations like this? Was it just a few conversations and all of a sudden it escalates and why don't we fire Sam kind of thing? - I think the board members were far, well-meaning people on the whole. And I believe that in stressful situations where people feel time
pressure or whatever, people understandably
make suboptimal decisions. And I think one of the challenges for OpenAI will be we're
gonna have to have a board and a team that are good at
operating under pressure. - Do you think the board
had too much power? - I think boards are supposed
to have a lot of power, but one of the things that we did see is in most corporate structures, boards are usually
answerable to shareholders. Sometimes people have like
super voting shares or whatever. In this case, I think one of the
things with our structure that we maybe should have
thought about more than we did is that the board of a nonprofit has, unless you put other rules in place, like quite a lot of power, they don't really answer
to anyone but themselves. And there's ways in which that's good, but what we'd really like
is for the board of OpenAI to answer to the world as a whole as much as that's a practical thing. - So there's a new board announced. - [Sam] Yeah. - There's, I guess, a new
smaller board of first and now there's a new final board. - Not a final board yet. We've added some, we'll add more. - Added some, okay. What is fixed in the new
one that was perhaps broken in the previous one? - The old board sort of got smaller over the course of about a year. It was nine and then it went down to six and then we couldn't agree on who to add. And the board also, I
think, didn't have a lot of experienced board members and a lot of the new board members at OpenAI have just have more
experience as board members. I think that'll help. - It's been criticized some of the people that are added to the board. I heard a lot of people
criticizing the addition of Larry Summers, for example. What was the process
of selecting the board? What's involved in that? - So Bret and Larry were
kind of decided in the heat of the moment over this very tense weekend and that was... I mean, that weekend was
like a real rollercoaster, like a lot of lots and downs. And we were trying to
agree on new board members that both sort of the executive team here and the old board members
felt would be reasonable. Larry was actually one
of their suggestions, the old board members. Bret, previous to that weekend, suggested, but he was busy and didn't wanna do it. And then we really needed help in wood. We talked about a lot of other people too, but I felt like if I
was going to come back, I needed new board members. I didn't think I could work
with the old board again in the same configuration, although we then decided, and I'm grateful that Adam would stay, but we wanted to get to... We considered various configurations, decided we wanted to
get to a board of three and had to find two new board members over the course of sort
of a short period of time. So those were decided honestly without... That's like you kind of do
that on the battlefield. You don't have time to design
a rigorous process then. For new board members, since new board members
will add going forward, we have some criteria that
we think are important for the board to have different expertise that we want the board to have. Unlike hiring an executive where you need them to do one role, well, the board needs to
do a whole role of kind of governance and thoughtfulness. Well, and so one thing that Bret says, which I really like is that
we wanna hire board members in slates, not as individuals one at a time. And thinking about a group of people that will bring nonprofit expertise, expertise at running companies, sort of good legal and
governance expertise. That's kind of what we've
tried to optimize for. - So is technical savvy important for the individual board members? - Not for every board member, but certainly some you need that. That's part of what the board needs to do. - So I mean, the interesting thing that people probably don't understand about OpenAI certainly
is like all the details of running the business. When they think about the
board given the drama, they think about you, they think about like if you reach AGI or you reach some of these
incredibly impactful products and you build them and deploy them, what's the conversation
with the board like? And they kind of think, all right, what's the right squad to have in that kind of
situation to deliberate? - Look, I think you definitely need some technical experts there and then you need some
people who are like, how can we deploy this in a way that will help people
in the world the most and people who have a very
different perspective? I think a mistake that you or I might make is to think that only the technical
understanding matters. And that's definitely part of the conversation you
want that board to have. But there's a lot more about how that's gonna
just like impact society and people's lives that you really want
represented in there too. - Are you looking at the
track record of people or you're just having conversations? - Track record's a big deal. You, of course, have a
lot of conversations. There's some roles where I kind of totally
ignore track record and just look at slope, kind of ignore the y-intercept. - Thank you. Thank you for making it
mathematical for the audience, - For a board member, I do care much more about the y-intercept. I think there is something deep to say about track record
there and experiences, something's very hard to replace. - Do you try to fit a polynomial function or exponential one to track record? - That's not that. An analogy doesn't carry that far. - All right. You mentioned some of the
low points that weekend. What were some of the low
points psychologically for you? Did you consider going
to the Amazon jungle and just taking Ayahuasca
disappearing forever or? - I mean, there's so many low, like it was a very bad period of time. There were great high points too. My phone was just like
sort of nonstop blowing up with nice messages from people
I worked with every day, people I hadn't talked to in a decade. I didn't get to appreciate
that as much as I should have. 'cause I was just like in
the middle of this firefight, but that was really nice. But on the whole, it was like a very painful weekend and also just like a very... It was like a battle fought in
public to a surprising degree and that was extremely exhausting to me, much more than I expected. I think fights are generally exhausting, but this one really was. The board did this Friday afternoon. I really couldn't get much
in the way of answers, but I also was just like, ""Well, the board gets to do this."" And so I'm gonna think for a little bit about what I want to do, but I'll try to find the, the
blessing in disguise here. And I was like, ""Well,
my current job at OpenAI, it was like to like run
a decently-sized company at this point."" And the thing I'd always liked
the most was just getting to work with the researchers. And I was like, yeah,
I can just go do like a very focused AI research effort. And I got excited about. That didn't even occur to me at the time to like possibly that this
was all gonna get undone. This was like Friday afternoon. - Oh, so you've accepted the death- - Very quickly, very quickly. I mean, I went through
like a little period of confusion and rage, but very quickly. And by Friday night, I was talking to people
about what was gonna be next and I was excited about that. I think it was Friday night evening for the first time that I
heard from the exec team here, which is like, hey, we're
gonna like fight this and we think... Well, whatever. And then I went to bed just
still being like, okay, excited. - Like onward, were you able to sleep? - Not a lot. It was one of the weird
things was there was this like period of four and a half days where sort of didn't sleep much, didn't eat much and still kind of had like a
surprising amount of energy. You learn like a weird
thing about adrenaline and more time. - So you kind of accepted the
death of this baby OpenAI? - And I was excited for the new thing. I was just like, okay, this
was crazy, but whatever. - It's a very good coping mechanism. - And then Saturday morning, two of the board members called and said, ""Hey, we destabilize. We didn't mean to destabilize things. We don't restore a lot of value here. Can we talk about you coming back?"" And I immediately didn't wanna do that, but I thought a little more and I was like, ""Well, I really
care about the people here, the partners, shareholders. I love this company."" And so I thought about it and I was like, ""Well, okay, but here's the stuff I would need."" And then the most painful time of all over the course of that weekend, I kept thinking and being told... Not just me, like the whole team here kept thinking. Well, we were trying to
keep OpenAI stabilized while the whole world was
trying to break it apart, people trying to recruit, whatever. We kept being told like, ""All right, we're almost done, we're almost done. We just need like a little bit more time."" And it was this like very confusing state. And then Sunday evening when again like every few hours, I expected that we were gonna be done and we're gonna figure
out a way for me to return and things to go back to how they were, the board then appointed a new interim CEO and then I was like... I mean, that feels really bad. That was the low point of the whole thing. You know, I'll tell you something, it felt very painful, but I felt a lot of
love that whole weekend. It was not other than that
one moment, Sunday night, I would not characterize my
emotions as anger or hate, but I really just like... I felt a lot of love from
people towards people. It was like painful, but it was like the dominant emotion of the weekend was love, not hate. - You've spoken highly of
Mira Murati that she helped, especially as you put in a tweet, ""In the quiet moments when it counts, perhaps we could take a bit of a tangent."" What do you admire about Mira? - Well, she did a great
job during that weekend in a lot of chaos, but people often see leaders in the crisis moments, good or bad. But a thing I really value in leaders is how people
act on a boring Tuesday at 9:46 in the morning and in just sort of the normal
drudgery of the day-to-day, how someone shows up in a meeting, the quality of the decisions they make. That was what I meant
about the quiet moments. - Meaning like most of the
work is done on a day by day in a meeting by meeting, just be present and make great decisions. - Yeah. I mean, look, what you have wanted to spend the last 20 minutes about and I understand is like this
one very dramatic weekend. But that's not really
what OpenAI is about. OpenAI is really about
the other seven years. - Well, yeah, human civilization
is not about the invasion of the Soviet Union by Nazi Germany, but still that's something
people totally focus on. - Very understandable. - It gives us an insight
into human nature, the extremes of human nature, and perhaps some of the damage and some of the triumphs of human civilization can
happen in those moments. So it's like illustrative.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 002",64,66
141,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",Ilya Sutskever,1111,1480,"So it's like illustrative. Let me ask you about Ilya. Is he being held hostage in
a secret nuclear facility? - No. - What about a regular secret facility? - No. - What about a nuclear
non-secure facility? - Neither, not that either. - I mean, this is becoming
a meme at some point. You've known Ilya for a long time. He was obviously part of this drama with the board and all that kind of stuff. What's your relationship with him now? - I love Ilya. I have tremendous respect for Ilya. I don't have anything I can
say about his plans right now. That's a question for him. But I really hope we work together for certainly the rest of my career. He's a little bit younger than me, maybe he works a little bit longer. - There's a meme that he saw something, like he maybe saw AGI and that gave him a lot
of worry internally. What did Ilya see? - Ilya has not seen AGI, none of us have seen AGI. We've not built AGII. I do think one of the many things that I really love about
Ilya is he takes AGI and the safety concerns broadly speaking, including things like the
impact this is gonna have on society very seriously. And as we continue to
make significant progress, Ilya is one of the people that I've spent the most time over the last couple of years talking about what this is going to mean, what we need to do to
ensure we get it right to ensure that we succeed at the mission. So Ilya did not see AGI. But Ilya is a credit to humanity in terms of how much he thinks and worries about making
sure we get this right. - I've had a bunch of
conversation with him in the past. I think when he talks about technology, he's always like doing
this long-term thinking type of thing. So he is not thinking about
what this is gonna be in a year. He's thinking about in 10 years. - [Sam] Yeah. - Just thinking from first principles like, okay, if the scales, what are the fundamentals here? Where's this going? And so that's a foundation for them thinking about like
all the other safety concerns and all that kind of stuff, which makes him a really
fascinating human to talk with. Do you have any idea why
he's been kind of quiet? Is it he's just doing some soul searching? - Again, I don't wanna speak for Ilya. I think that you should ask him that. He's definitely a thoughtful guy. I think I kind of think of Ilya as like always on a soul
search in a really good way. - Yes. Yeah. Also he appreciates the power of silence. Also, I'm told he can be a silly guy, which I've never seen that side of him. - It's very sweet when that happens. - I've never witnessed a silly Ilya, but I look forward to that as well. - I was at a dinner
party with him recently and he was playing with a puppy. And he was like in a very
silly move, very endearing and I was thinking like, oh man, this is like not the side of the Ilya that the world sees the most. - So just to wrap up this whole saga, are you feeling good
about the board structure about all of this and where it's moving? - I feel great about the new board. In terms of the structure of OpenAI, one of the board's
tasks is to look at that and see where we can make it more robust. We wanted to get new board
members in place first, but we clearly learned
a lesson about structure throughout this process. I don't have I think
super deep things to say. It was a crazy, very painful experience. I think it was like a
perfect storm of weirdness. It was like a preview for
me of what's gonna happen as the stakes get higher and higher and the need that we have like
robust governance structures and processes and people. I am kind of happy it
happened when it did, but it was a shockingly
painful thing to go through. - Did it make you be more
hesitant in trusting people? - Yes. - Just on a personal level. - Yes. - I think I'm like an
extremely trusting person. I've always had a life philosophy of like don't worry about
all of the paranoia, don't worry about the edge cases. You get a little bit screwed in exchange for getting to live with your guard down. And this was so shocking to me. I was so caught off guard that it has definitely changed and I really don't like this. It's definitely changed how I think about just like default trust of people and planning for the bad scenarios. - You gotta be careful with that. Are you worried about
becoming a little too cynical? - I'm not worried about
becoming too cynical. I think I'm like the extreme
opposite of a cynical person. But I'm worried about
just becoming like less of a default trusting person. - I'm actually not sure which
mode is best to operate in for a person who's developing AGI, trusting or untrusting. It's an interesting journey you're on. But in terms of structure, see, I'm more interested
on the human level. How do you surround yourself with humans that are building cool shit, but also are making wise decisions? Because the more money you start making, the more power the thing
has the weirder people get. - I think you could make
all kinds of comments about the board members and the level of trust
I should have had there or how I should have
done things differently. But in terms of the team here, I think you'd have to like
give me a very good grade on that one. And I have just like
enormous gratitude and trust and respect for the people
that I work with every day. And I think being surrounded with people like that is really important. - Our mutual friend Elon sued OpenAI.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 003",65,67
142,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",Elon Musk lawsuit,1480,2072,"- Our mutual friend Elon sued OpenAI. What is the essence of
what he's criticizing? To what degree does he have a point? To what degree is he wrong? - I don't know what it's really about. We started off just thinking
we were gonna be a research lab and having no idea about how
this technology was gonna go. Because it was only
seven or eight years ago, it's hard to go back and really remember what it was like then. But before language
models were a big deal, this was before we had
any idea about an API or selling access to a chat bot. It was before we had any
idea we were gonna productize at all. So we're like we're just
gonna try to do research and we don't really know what
we're gonna do with that. I think with many new
fundamentally new things, you start fumbling through the dark and you make some assumptions, most of which turn out to be wrong. And then it became clear that we were going to need
to do different things and also have huge amounts more capital. So we said, ""Okay, well, the structure doesn't quite work for that. How do we patch the structure?"" And then you patch it
again and patch it again and you end up with something that does look kind of eyebrow
raising to say the least. But we got here gradually with I think reasonable decisions at each point along the way and doesn't mean I wouldn't
do it totally differently if we could go back now with an oracle, but you don't get the oracle at the time. But anyway, in terms of what Elon's
real motivations here are I don't know. - To the degree you remember, what was the response that
OpenAI gave in the blog post? Can you summarize it? - Oh, we just said like Elon
said this set of things, here's our characterization or here's this sort of
not our characterization, here's like the characterization
of how this went down. We tried to not make it emotional and just sort of say
like here's the history. - I do think there's a
degree of mischaracterization from Elon here about one of the points you just made, which is the degree of
uncertainty you had at the time. You guys are a bunch of like a small group of researchers crazily talking about AGI when everybody's laughing at that thought. - Wasn't that long ago
Elon was crazily talking about launching rockets when people were laughing at that thought? So I think he'd have
more empathy for this. - I mean, I do think that
there's personal stuff here that there was a split that OpenAI and a lot of amazing people here chose to part ways of Elon. So there's a personal- - Elon chose to part ways. - Can you describe that exactly, the choosing to part ways? - He thought OpenAI was gonna fail. He wanted total control
to sort of turn it around. We wanted to keep going in the direction that now has become OpenAI. He also wanted Tesla to be able to build an AGI effort. At various times, he wanted to make OpenAI
into a for-profit company that he could have control of or have it merged with Tesla. We didn't want to do that and he decided to leave, which that's fine. - And that's one of the things that the blog post says
is that he wanted OpenAI to be basically acquired by Tesla in those same way that or
maybe something similar or maybe something more dramatic than the partnership with Microsoft. - My memory is the proposal was just like, yeah, like get acquired by Tesla and have Tesla have full control over it. I'm pretty sure that's what it was. - So what is the word open in OpenAI mean to Elon at the time? Ilya has talked about this
in in the email exchanges and all this kind of stuff. What does it mean to you at the time? What does it mean to you now? - I would definitely pick a diff... Speaking of going back with an oracle, I'd pick a different name. One of the things that
I think OpenAI is doing that is the most important of everything that we're doing is
putting powerful technology in the hands of people
for free as a public good. We don't run ads on our free version. We don't monetize it in other ways. We just say it's part of our mission. We wanna put increasingly powerful tools in the hands of people for free and get them to use them. And I think that kind of
open is really important to our mission. I think if you give people great tools and teach them to use them
or don't even teach them, they'll figure it out and let them go build an incredible future for each other with that. That's a big deal. So if we can keep putting free or low cost or free and low cost powerful
AI tools out in the world, I think that's a huge deal for how we fulfill the mission. Open source or not, yeah, I think we should
open source some stuff and not other stuff. It does become this like
religious battle line where nuance is hard to have, but I think nuance is the right answer. - So he said change your name to ClosedAI and I'll drop the lawsuit. I mean, is it going to
become this battleground in the land of memes about the name? - I think that speaks to the seriousness with which Elon means the lawsuit. I mean, that's like an
astonishing thing to say, I think. - Well, I don't think the lawsuit maybe, correct me if I'm wrong, but I don't think the
lawsuit is legally serious. It's more to make a point
about the future of AGI and the company that's
currently leading the way. - Look, I mean Grok had
not open sourced anything until people pointed out it
was a little bit hypocritical and then he announced that Grok open source things this week. I don't think open source versus not is what this is really about for him. - Well, we'll talk about
open source and not. I do think maybe criticizing
the competition is great, just talking a little shit, that's great, but friendly competition versus like I personally hate lawsuits. - Look, I think this whole
thing is like unbecoming of a builder, and I respect Elon is one of
the great builders of our time. And I know he knows what it's like to have like haters attack him and it makes me extra
sad he's doing the toss. - Yeah, he is one of the
greatest builders of all time, potentially the greatest
builder of all time. - It makes me sad. And I think it makes a lot of people sad. There's a lot of people
who've really looked up to him for a long time and said this. I said in some interview or something that I missed the old Elon and the number of messages I got being like that exactly encapsulates how I feel. - I think he should just win. He should just make Grok beat GPT and then GPT beats Grok and it's just a competition, and it's beautiful for everybody. But on the question of open source, do you think there's a
lot of companies playing with this idea? It's quite interesting. I would say Meta, surprisingly,
has led the way on this or like at least took the
first step in the game of chess of really open sourcing the model. Of course, it's not the
state of the art model, but open sourcing Llama and Google is flirting with the idea of open sourcing a smaller version. What are the pros and
cons of open sourcing? Have you played around with this idea? - Yeah, I think there
is definitely a place for open source models, particularly smaller models that people can run locally, I think there's huge demand for. I think there will be
some open source models, there will be some closed source models. It won't be unlike other
ecosystems in that way. - I listened to all in podcasts talking about this lawsuit and
all that kind of stuff and they were more concerned
about the precedent of going from nonprofit
to this cap for profit. What precedent that
sets for other startups? - I would heavily discourage any startup that was thinking about
starting as a non-profit and adding like a for-profit arm later. I'd heavily discourage
them from doing that. I don't think we'll set a precedent here. - Okay. So most startups should go just- - For sure. And again, if we knew
what was gonna happen, we would've done that too. - Well, like in theory, if you like dance beautifully here, there's like some tax
incentives or whatever - But I don't think that's like how most people think about these things. - Just not possible to save a lot of money for a startup if you do it this way. - No, I think there's
like laws that would make that pretty difficult. - Where do you hope this goes with Elon? Well, this tension, this dance, what do you hope this? Like if we go one, two,
three years from now, your relationship with him
on a personal level too, like friendship, friendly competition, just all this kind of stuff. - Yeah. I mean, I really respect Elon. And I hope that years in the future, we have an amicable relationship. - Yeah, I hope you guys have
an amicable relationship like this month and just compete and win and explore these ideas together. I do suppose there's competition
for talent or whatever, but it should be friendly competition. Just build, build cool shit. And Elon is pretty good
at building cool shit, but so are you.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 004",66,68
143,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",Sora,2072,2663,"but so are you. So speaking of cool shit. Sora, there's like a million
questions I could ask. First of all, it's amazing, it truly is amazing on a product level, but also just on a philosophical level. So let me just
technical/philosophical ask. What do you think it
understands about the world more or less than GPT-4, for example, like the world model when you train on these
patches versus language tokens? - I think all of these models
understand something more about the world model than most
of us give them credit for. And because they're also very clear things they just don't understand
or don't get right, it's easy to look at the weaknesses, see through the veil and say this is all fake, but it's not all fake. It's just some of it works
and some of it doesn't work. I remember when I started
first watching Sora videos and I would see like a person walk in front of something for a
few seconds and occlude it and then walk away and the same thing was still there. I was like, ""This is pretty good."" Or there's examples where the underlying physics
looks so well represented over a lot of steps in a sequence. It's like oh this is
like quite impressive. But, fundamentally, these
models are just getting better and that will keep happening. If you look at the trajectory from DALL·E 1 to 2 to 3 to Sora, there were a lot of people that
were dunked on each version, saying it can't do this, it can't do that and I'm like look at it now. - Well, the thing you
just mentioned is kind of with the occlusions is
basically modeling the physics of three dimensional physics
of the world sufficiently well to capture those kinds of things. - Well. - Yeah, maybe you can tell me in order to deal with occlusions, what does the world model need to? - Yeah, so what I would
say is it's doing something to deal with occlusions really well. What I represent that it
has like a great underlying 3D model of the world. It's a little bit more of a stretch - But can you get there
through just these kinds of two dimensional
training data approaches? - It looks like this approach
is gonna go surprisingly far. I don't wanna speculate too much about what limits it will
surmount and which it won't. - What are some interesting limitations of the system that you've seen? I mean, there's been some
fun ones you've posted. - There's all kinds of fun. I mean, like cats sprouting a extra limit at random points in a video, like pick what you want, but there's still a lot of problem, there's a lot of weaknesses. - Do you think that's a
fundamental flaw of the approach or is it just bigger model or better technical
details or better data, more data is going to solve
the cat sprouting extremes? - I would say yes to both. I think there is something
about the approach which just seems to feel different from how we think and learn and whatever. And then also, I think it'll get better with scale. - I mentioned LLMs have
tokens, text tokens and Sora has visual patches so it converts all visual data, a diverse kinds of visual data videos and images into patches. Is the training to the degree you can say
fully self-supervised there? Is there some manual labeling going on? What's the involvement
of humans in all this? - I mean, without saying anything specific about the Sora approach, we use lots of human data in our work. - But not internet scale data. So lots of humans, lots of complicated word, Sam. - I think lots is a
fair word in this case. - But it doesn't because to me, lots, like listen, I'm an introvert and when I hang out
with like three people, that's a lot of people. Yeah, four people, that's a lot. But I suppose you mean more than- - More than three people
work on labeling the data for these models, yeah. - Okay. All right. But fundamentally, there's a lot of self-supervised learning. 'cause what you mentioned in the technical report
is internet scale data. That's another beautiful, it's like poetry. So it's a lot of data
that's not human label. It's self-supervised in that way. And then the question is, how much data is there on the internet that could be used in
this that is conducive to this kind of self-supervised way if only we knew the details
of the self-supervised? Do you have you considered opening it up a little more details - We have. You mean, for source specifically? - Source specifically because it's so interesting. Can the same magic of LLMs now start moving
towards visual data and what does that take to do that? - I mean, it looks to me like yes, but we have more work to do. - Sure. What are the dangers? Why are you concerned
about releasing the system? What are some possible dangers of this? - I mean, frankly speaking, one thing we have to do before releasing the
system is just like get it to work at a level of efficiency that will deliver the scale
people are gonna want from this. So that I don't wanna like downplay that and there's still a ton
of work to do there. But you can imagine like issues with deep fakes, misinformation. We try to be a thoughtful company about what we put out into the world and it doesn't take much thought to think about the ways this can go badly. - There's a lot of tough questions here. You're dealing in a very tough space. Do you think training AI should be or is fair use under copyright law? - I think the question
behind that question is, do people who create valuable
data deserve to have some way that they get compensated for use of it? And that I think the answer is yes. I don't know yet what the answer is. People have proposed a
lot of different things. We've some tried some different models. But if I'm like an artist, for example, I would like to be able to opt out of people generating art in my style and B, if they do
generate art in my style, I'd like to have some economic
model associated with that. - Yeah, it's that transition
from CDs to Napster to Spotify. We have to figure out some kind of model. - The model changes, but people have gotta get paid. - Well, there should be some kind of incentive if we zoom
out even more for humans to keep doing cool shit. - Everything I worry about, humans are gonna do cool shit and society's gonna find
some way to reward it. That seems pretty hardwired. We want to create. We want to be useful. We want to achieve status in whatever way that's not going anywhere, I don't think. - But the reward might not
be monetary, financial. It might be like fame and
celebration of other cool- - Maybe financial in some other way. Again, I don't think we've
seen like the last evolution of how the economic system's gonna work. - Yeah. But artists and creators are worried. When they see Sora, they're like, ""Holy shit."" - Sure. Artists were also super worried
when photography came out. And then photography became a new art form and people made a lot of
money taking pictures. And I think things like
that will keep happening. People will use the new tools in new ways. - If we just look on YouTube
or something like this, how much of that will be using Sora, like AI-generated content do you think in the next five years? - People talk about like
how many jobs is AI gonna do in five years and the framework that people
have is what percentage of current jobs are just
gonna be totally replaced by some AI doing the job? The way I think about
it is not what percent of jobs AI will do, but what percent of tasks will AI do and over what time horizon. So if you think of all of
the like-five second tasks in the economy, five-minute
tasks, the five-hour tasks, maybe even the five-day tasks, how many of those can AI do? And I think that's a way
more interesting, impactful, important question than
how many jobs AI can do because it is a tool that will work at increasing
levels of sophistication and over longer and longer time horizons for more and more tasks and let people operate at a
higher level of abstraction. So maybe people are way more
efficient at the job they do. And at some point, that's not just a quantitative change, but it's a qualitative
one too about the kinds of problems you can keep in your head. I think that for videos on YouTube, it'll be the same. Many videos, maybe most of them, will use AI tools in the production, but they'll still be fundamentally driven by a person thinking about it, putting it together, doing parts of it, sort of directing it and running it. - Yeah, it's so interesting. I mean, it's scary, but it's interesting to think about. I tend to believe that humans
like to watch other humans or other human like- - Humans really care
about other humans a lot. - Yeah. If there's a cooler thing
that's better than a human, humans care about that for like two days and then they go back to humans. - That seems very deeply wired. - It's the whole chest thing. But now let's everybody keep playing chess and let's ignore the alpha in the room that humans are really bad at
chess relative to AI systems. - We still run races and
cars are much faster. I mean, there's like a lot of examples. - Yeah. And maybe it'll just be tooling like in the Adobe suite type of way where it can just make videos much easier and all that kind of stuff. Listen, I hate being
in front of the camera. If I can figure out a way to not be in front of the camera, I would love it. Unfortunately, it'll take a while. Like that generating faces, it's getting there, but generating faces and
video format is tricky when it's specific people
versus generic people.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 005",67,69
144,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",GPT-4,2663,3332,"when it's specific people
versus generic people. Let me ask you about GPT-4. There's so many questions. First of all, also amazing. Looking back, it'll probably be this kind of historic pivotal moment with three, five, and four which had GPT. - Maybe five will be the pivotal moment. I don't know. Hard to say that looking forwards. - We never know. That's the annoying
thing about the future, it's hard to predict. But for me, looking back GPT-4, ChatGPT is pretty impressive, historically impressive. So allow me to ask, what's been the most impressive
capabilities of GPT-4 to you and GPT-4 Turbo? - I think it kind of sucks. - Hmm. Typical human also gotten
used to an awesome thing. - No, I think it is an amazing thing, but relative to where we need to get to and where I believe we will get to, at the time of like GPT-3, people were like, ""Oh this is amazing. This is this like marvel of technology,"" and it is, it was. But now we have GPT-4 and look at GPT-3 and you're like that's
unimaginably horrible. I expect that the delta between five and four will be the same
as between four and three. And I think it is our job to
live a few years in the future and remember that the tools
we have now are gonna kind of suck looking backwards at them and that's how we make
sure the future is better. - What are the most glorious
ways that GPT-4 sucks? Meaning- - [Sam] What are the
best things it can do? - What are the best things
it can do in the limits of those best things that
allow you to say it sucks, therefore gives you an inspiration
and hope for the future? - One thing I've been using it for more recently is sort of a
like a brainstorming partner. - [Lex] Yep. Almost for that. - There's a glimmer of
something amazing in there. I don't think it gets... When people talk about it, what it does, they're like, ""Helps me
code more productively. It helps me write more faster and better. It helps me translate from
this language to another,"" all these like amazing things. But there's something about the kind of creative brainstorming partner. I need to come up with
a name for this thing. I need to think about this
problem in a different way. I'm not sure what to do here. That I think like gives a glimpse of something I hope to see more of. One of the other things that you can see a very
small glimpse of is what I can help on longer horizon tasks. Break down something in multiple steps, maybe execute some of those steps, search the internet, write code, whatever. Put that together. When that works, which is not very often, it's like very magical, - The iterative back
and forth with a human. It works a lot for me. What do you mean it works? - Iterative back and forth to human, it can get more often, when it can go do like a
10-step problem on its own. It doesn't work for that
too often sometimes. - Add multiple layers of abstraction or do you mean just sequential? - Both like to break it down and then do things that different layers of abstraction put them together. Look, I don't wanna downplay
the accomplishment of GPT-4, but I don't wanna overstate it either. And I think this point that we
are on an exponential curve, we'll look back relatively soon at GPT-4 like we look back at GPT-3 now. - That said, I mean
ChatGPT was the transition to where people like started to believe there is an
uptick of believing. Not internally at OpenAI perhaps. There's believers here, but when you think- - And in that sense, I do think it'll be a moment where a lot of the world went from not believing to believing. That was more about the ChatGPT interface. And by the interface and product, I also mean the post-training of the model and how we tune it to be helpful to you and how to use it than the
underlying model itself. - How much of each of
those things are important? The underlying model and the RLHF or something of that nature that tunes it to be more compelling to the human, more effective and
productive for the human. - I mean, they're both super important but the RLHF, the post-training step, the little wrapper of things that from a compute perspective, little wrapper of things that we do on top of the base model, even though it's a huge amount of work. That's really important to
say nothing of the product that we build around it. In some sense, we did have to do two things. We had to invent we underlying technology and then we had to figure out how to make it into a
product people would love, which is not just about the
actual product work itself, but this whole other step of how you align it and make it useful - And how you make the scale work where a lot of people can
use it at the same time, all that kind of stuff. - But that was like a
known difficult thing. We knew we were gonna have to scale it up. We had to go do two things that had like never been done before that were both, like, I would say quite significant achievements and then a lot of things
like scaling it up that other companies
have had to do before. - How does the context window of going from 8K to 128K tokens compare from GPT-4 to to GPT-4 Turbo? - Most people don't
need all the way to 128, most of the time although. If we dream into the distant future, we'll have like way distant future, we'll have like context
length of several billion. You will feed in all of your information, all of your history time, and it'll just get to
know you better and better and that'll be great. For now, the way people use these models, they're not doing that. And people sometimes post in a paper or a significant fraction of
a code repository, whatever. But most usage of the models is not
using the long context most of the time. - I like that this is your
I have a dream speech. One day, you'll be judged
by the full context of your character or
of your whole lifetime. That's interesting. So like that's part of the expansion that you're hoping for is a
greater and greater context. - I saw this internet clip once. I'm gonna get the numbers wrong, but it was like Bill Gates
talking about the amount of memory on some early computer. Maybe it was 64K, maybe 640K, something like that. And most of it was used
for the screen buffer. And he just couldn't seem genuine. This couldn't imagine that the world would eventually
need gigabytes of memory in a computer or terabytes
of memory in a computer. And you always do or you always do just need to follow the exponential of technology. We will find out how to
use better technology. So I can't really imagine
what it's like right now for context links to go
out to the billion someday and they might not literally go there, but effectively it'll feel like that. But I know we'll use it and really not wanna go
back once we have it. - Yeah, even saying billions 10 years from now might seem dumb because it'll be like
trillions upon trillions. - [Sam] Sure. - There'd be some kind of breakthrough that will effectively feel
like infinite context. But even 120, I have to be honest, I haven't pushed it to that degree. Maybe putting in entire books or like parts of books and so on, papers. What are some interesting use cases of GPT-4 that you've seen? - The thing that I find
most interesting is not any particular use case that
we can talk about those, but it's people who kind of like... This is mostly younger people, but people who use it as
like their default start for any kind of knowledge work task. And it's the fact that it can do a lot of things reasonably well. You can use GPTV, you can use it to help you write code, you can use it to help you do search, you can use it to edit a paper. The most interesting to me is the people who just use it as the
start of their workflow. - I do as well for many things. Like I use it as a reading
partner for reading books. It helps me think, help me think through ideas, especially when the books are classic, so it's really well written about and it actually is... I find it often to be significantly better than even like Wikipedia
on well-covered topics. It's somehow more
balanced and more nuanced, or maybe it's me, but it inspires me to think deeper than a
Wikipedia article does. I'm not exactly sure what that is. You mentioned like this collaboration, I'm not sure where magic is, if it's in here or if it's in there or if it's somewhere in between. I'm not sure. But one of the things that concerns me for knowledge task when
I start with GPT is I'll usually have to
do fact checking after, like check that it didn't
come up with fake stuff. How do you figure that out that GPT can come up with fake stuff that sounds really convincing? So how do you ground it in truth? - That's obviously an area
of intense interest for us. I think it's gonna get a lot
better with upcoming versions, but we'll have to continue to work on it and we're not gonna have it
like all solved this year. - Well, the scary thing
is like as it gets better. You'll start not doing the
fact checking more and more, right? - I'm of two minds about that. I think people are like much
more sophisticated users of technology than we
often give them credit for and people seem to really
understand that GPT, any of these models
hallucinate some of the time and if it's mission critical, you gotta check it. - Except journalists don't
seem to understand that. I've seen journalists
half-assedly just using GPT-4. - Of the long list of things I'd like to dunk on journalists for, this is not my top criticism of them. - Well, I think the bigger
criticism is perhaps the pressures and the incentives of being a journalist is that you have to work really quickly
and this is a shortcut. I would love our society
to incentivize like- - [Sam] I would too. - Journalistic efforts
that take days and weeks and rewards great in depth journalism. Also journalism that represent
stuff in a balanced way where it's like celebrates people while criticizing them even though the criticism is
the thing that gets clicks and making up also gets clicks and headlines that
mischaracterize completely. I'm sure you have a lot
of people dunking on... Well, all that drama
probably got a lot of clicks. - Probably did. - And that's a bigger problem
about human civilization. I would love to see solved is
where we celebrate a bit more.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 006",68,70
145,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",Memory & privacy,3332,3756,"I would love to see solved is
where we celebrate a bit more. You've given ChatGPT the
ability to have memories. You've been playing with that
about previous conversations and also the ability to turn off memory, which I wish I could do that sometimes, just turn on and off depending. I guess sometimes alcohol can do that, but not optimally, I suppose. What have you seen through that, like playing around with that idea of remembering conversations and not? - We're very early in
our explorations here, but I think what people want or at least what I want
for myself is a model that gets to know me and gets
more useful to me over time. This is an early exploration. I think there's like a
lot of other things to do, but that's where we'd like to head. You'd like to use a model and over the course of your life or use a system, it'd be many models. And over the course of your life, it gets better and better. - Yeah. How hard is that problem? 'Cause right now, it's more like remembering little factoids and preferences and so on. What about remembering, like don't you want GPT to remember all the shit
you went through in November and all the drama and then you can- - Yeah, yeah, yeah. - Because right now, you're clearly blocking
it out a little bit. - It's not just that I
want it to remember that. I want it to integrate the lessons of that and remind me in the future
what to do differently or what to watch out for. And we all gain from experience over the course of our
lives, varying degrees. And I'd like my AI agent to
gain with that experience too. So if we go back and let ourselves imagine that trillions and
trillions of contact length, if I can put every
conversation I've ever had with anybody in my life in there, if I can have all of
my emails input out... Like all of my input/output in the context window every
time I ask a question, that'd be pretty cool, I think. - Yeah, I think that would be very cool. People sometimes will hear that and be concerned about privacy. What do you think about that aspect of it, the more effective the AI becomes that really integrating
all the experiences and all the data that happened to you and give you advice? - I think the right answer
there is just user choice. Anything I want stricken from
the record from my AI agent, I wanna be able to take out. If I don't want to remember anything, I want that too. You and I may have different opinions about where on that
privacy utility trade off for our own AI we wanna be, which is totally fine. But I think the answer is just
like really easy user choice. - But there should be some high level of transparency from a
company about the user choice 'cause sometimes company in the past, companies in the past have been kind of absolutely shady about like, yeah, it's kind of presumed that
we're collecting all your data and we're using it for a good reason for advertisement and so on. But there's not a transparency
about the details of that. - That's totally true. You mentioned earlier that I'm like blocking
out the November stuff. - I'm just teasing you. - Well, I mean I think it
was a very traumatic thing and it did immobilize me
for a long period of time. Like definitely the hardest, like the hardest work thing I've had to do was just like
keep working that period because I had to try to come back in here and put the pieces together while I was just like in
sort of shock and pain. Nobody really cares about that. I mean, the team gave me a pass and I was not working at my normal level, but there was a period
where I was just, like, it was really hard to have to do both. But I kind of woke up one morning and I was like, ""This was a
horrible thing to happen to me. I think I could just feel
like a victim forever,"" or I can say, ""This is like
the most important work I'll ever touch in my life and I need to get back to it."" And it doesn't mean that I've repressed it because sometimes I wake in the middle of the night thinking about it, but I do feel like an obligation
to keep moving forward. - Well, that's beautifully said, but there could be some
lingering stuff in there. What I would be concerned about is that trust thing that you mentioned that being paranoid about people as opposed to just trusting
everybody or most people, like using your gut. It's a tricky dance for sure. I mean, because I've seen in
my part-time explorations, I've been diving deeply into
the Zelensky administration, the Putin administration and
the dynamics there in wartime in a very highly stressful environment. And what happens is distrust and you isolate yourself both and you start to not
see the world clearly. And that's a concern, that's a human concern. You seem to have taken it in stride and kind of learned the good
lessons and felt the love and let the love energize you, which is great, but still can linger in there. There's just some questions I would love to ask your intuition about
what's GPT able to do and not. So it's allocating approximately
the same amount of compute for each token it generates. Is there room there in
this kind of approach to slower thinking, sequential thinking? - I think there will be a new paradigm for that kind of thinking. - Will it be similar like architecturally as what we're seeing now with LLMs? Is it a layer on top of the LLMs? - I can imagine many
ways to implement that. I think that's less important than the question you were getting out, which is do we need a
way to do a slower kind of thinking where the answer
doesn't have to get like... I guess like spiritually, you could say that you want an AI to be able to think harder
about a harder problem and answer more quickly
about an easier problem. And I think that will be important. - Is that like a human thought
that we're just having, you should be able to think hard? Is that a wrong intuition? - I suspect that's a reasonable intuition. - Interesting. So it's not possible once the GPT gets like GPT-7 would just be instantaneously be able to see, here's the proof of from RSTM. - It seems to me like you want to be able to allocate more compute
to harder problems. It seems to me that a system knowing if you ask a system like that, proof from us last theorem versus... What's today's date? Unless it already knew and
had memorized the answer to the proof, assuming it's gotta go figure that out, seems like that will take more compute. - But can it look like a
basically LLM talking to itself, that kind of thing? - Maybe. I mean, there's a lot of things that you could imagine working what the right or the best way to do that will be. We don't know.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 007",69,71
146,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",Q,3756,3972,"We don't know. - This does make me
think of the mysterious, the lore behind Q-Star. What's this mysterious Q-Star project? Is it also in the same nuclear facility? - There is no nuclear facility. - That's what a person with a nuclear facility always says. - I would love to have a
secret nuclear facility. There isn't one. - All right. - Maybe someday. - Someday. All right. One can dream- - OpenAI is not a good
company to keeping secrets. It would be nice. We're like been plagued by a lot of leaks and it would be nice if we were able to have something like that. - Can you speak to what Q-Star is? - We are not ready to talk about that. - See, but an answer like
that means there's something to talk about. It's very mysterious, Sam. - I mean, we work on
all kinds of research. We have said for a while that we think better reasoning in these systems is an important direction that we'd like to pursue. We haven't cracked the code yet. We're very interested in it. - Is there gonna be moments Q-Star or otherwise where there's
going to be leaps similar to GPT where you're like- - That's a good question. What do I think about that? It's interesting to me it
all feels pretty continuous. - This is kind of a theme that you're saying is you're
basically gradually going up an exponential slope. But from an outsider's perspective for me, just watching it that it
does feel like there's leaps, but to you there isn't. - I do wonder if we should have... So part of the reason that we deploy the way
we do is that we think, we call it iterative deployment. Rather than go build in secret until we got all the way to GPT-5, we decided to talk
about GPT 1, 2, 3 and 4. And part of the reason
there is, I think, AI and surprise don't go together. And also the world, people, institutions, whatever you wanna call it, need time to adapt and
think about these things. And I think one of the best things that OpenAI has done is this strategy and we get the world to pay
attention to the progress to take AGI seriously to think about what
systems, and structures, and governance we want in place before, we're like under the gun and have to make a rush decision. I think that's really good. But the fact that people like you and others say you still feel like there are these leaps makes me think that maybe we should
be doing our releasing even more iteratively. I don't know what that would mean. I don't have an answer ready to go. But our goal is not to have
shock updates to the world. The opposite. - Yeah, for sure. More iterative would be amazing. I think that's just
beautiful for everybody. - But that's what we're trying to do. That's like our state of the strategy and I think we're
somehow missing the mark. So maybe we should think
about releasing GPT-5 in a different way or something like that. - Yeah, 4.71, 4.72. But people tend to like to celebrate, people celebrate birthdays. I don't know if you know humans, but they kind of have these milestones. - I do know some humans. People do like milestones. I totally get that. I think we like milestones too. It's like fun to say, declare victory on this one and go start the next thing. But, yeah, I feel like
we're somehow getting this a little bit wrong.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 008",70,72
147,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",GPT-5,3972,4167,"this a little bit wrong. - So when is GPT-5 coming out again? - I don't know. That's an honest answer. - Oh, that's the honest answer. Is it blink twice if it's this year? - We will release an
amazing model this year. I don't know what we'll call it. - So that goes to the question of like, what's the way we release this thing? - We'll release, over in the coming months, many different things. I think they'll be very cool. I think before we talk about like a GPT-5 like model called that or called or not called that or a little bit worse or a little bit better
than what you'd expect from a GPT-5, I know we have a lot of
other important things to release first. - I don't know what to expect from GPT-5. You're making me nervous and excited. What are some of the biggest
challenges in bottlenecks to overcome for whatever
it ends up being called, but let's call it GPT-5? Just interesting to ask, is it on the compute side? Is it in the technical side? - It's always all of these. What's the one big unlock? Is it a bigger computer? Is it like a new secret? Is it something else? It's all of these things together. The thing that OpenAI I
think does really well, this is actually an original Ilya quote that I'm gonna butcher, but it's something like we
multiply 200 medium-sized things together into one giant thing. - So there's this distributed
constant innovation happening. - [Sam] Yeah. - So even on the technical side, like- - Especially on the technical side. - So like even like detailed approaches, like detailed aspects of every... How does that work with different
disparate teams and so on? How do the medium-sized things become one whole giant transformer? How does this- - There's a few people who have to think about putting
the whole thing together, but a lot of people try to keep most of the picture in their head. - Oh, like the individual teams, individual contributors tried to keep a big picture-
- At a high level. Yeah, you don't know exactly how every piece works, of course. But one thing I generally believe is that it's sometimes useful to zoom out and look at the entire map. And I think this is true for
like a technical problem. I think this is true for
like innovating in business. But things come together
in surprising ways and having an understanding
of that whole picture. Even if most of the time, you're operating in the weeds in one area, pays off with surprising insights. In fact, one of the
things that I used to have and I think was super valuable was I used to have like a good map
of all of the frontier or most of the frontiers
in the tech industry. And I could sometimes
see these connections or new things that were possible that if I were only deep in one area, I wouldn't be able to have the idea for because I wouldn't have all the data and I don't really have that much anymore. I'm like super deep now. But I know that it's a valuable thing. - You're not the man you used to be, Sam. - Very different job now
than what I used to have.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 009",71,73
148,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",7 trillion of compute,4167,4655,"- Very different job now
than what I used to have. - Speaking of zooming out, let's zoom out to another cheeky thing, but profound thing perhaps that you said. You tweeted about needing $7 trillion. - I did not tweet about that. I never said like we're
raising $7 trillion or blah blah blah. - Oh, that's somebody else. - [Sam] Yeah. - Oh, but you said it, ""Fuck it, maybe eight,"" I think. - Okay. I meme like once
there's like misinformation out in the world. - Oh, you meme. But sort of misinformation
may have a foundation of insight there. - Look, I think compute
is gonna be the currency of the future. I think it will be maybe
the most precious commodity in the world. And I think we should be investing heavily to make a lot more compute. Compute is... I think it's gonna be an unusual market. People think about the
market for like chips for mobile phones or something like that. And you can say that, okay, there's 8 billion people in the world, maybe 7 billion of them have phones or 6 billion, let's say. They upgrade every two years, so the market per year is 3 billion system on chip for smartphones. And if you make 30 billion, you will not sell 10 times as many phones because most people have one phone. But compute is different, like intelligence is
gonna be more like energy or something like that where the only thing
that I think makes sense to talk about is at price X, the world will use this much compute and at price Y, the world will use this much compute because if it's really cheap, I'll have it like
reading my email all day, like giving me suggestions about what I maybe should
think about or work on and trying to cure cancer. And if it's really expensive, maybe I'll only use it or will only use it, try to cure cancer. So I think the world is gonna
want a tremendous amount of compute. And there's a lot of parts
of that that are hard. Energy is the hardest part. Building data centers is also hard. The supply chain is harder than, of course, fabricating
enough chips is hard. But this seems to me
where things are going. Like we're gonna want an amount
of compute that's just hard to reason about right now. - How do you solve the energy puzzle? Nuclear. - That's what I believe. - Fusion? - That's what I believe. - Nuclear fusion.
- Yeah. - Who's gonna solve that? - I think Helion's doing the best work, but I'm happy there's like
a race for fusion right now. Nuclear fusion, I think,
is also like quite amazing and I hope as a world, we can re-embrace that. It's really sad to me how
the history of that went and hope we get back to
it in a meaningful way. - So to you, part of the puzzle is nuclear fusion, like nuclear reactors as
we currently have them and a lot of people are terrified because of Chernobyl and so on. - Well, I think we
should make new reactors. I think it's a shame that
industry kind of ground to a halt. - Just mass hysteria is
how you explain the halt. - Yeah. - I don't know if you know humans, but that's one of the dangers, that's one of the security threats for nuclear fusion is humans
seem to be really afraid of it. And that's something we have to incorporate into the calculus of it. So we have to kind of win people over and to show how safe it is. - I worry about that for AI. I think some things are gonna
go theatrically wrong with AI. I don't know what the percent chances that I eventually get shot, but it's not zero. - Oh like we wanna stop this. - [Sam] Maybe. - How do you decrease the
theatrical nature of it? I've already starting to hear rumblings 'cause I do talk to people on both sides of the political spectrum here, rumblings where it's
going to be politicized, AI is going to be politicized,
really, really worries me because then it's like maybe
the right is against AI and the left is for AI 'cause it's going to help
the people or whatever. Whatever the narrative
and the formulation is, that really worries me. And then the theatrical nature
of it can be leveraged fully. How do you fight that? - I think it will get caught up in like left versus right wars. I don't know exactly what
that's gonna look like, but I think that's just what happens with anything of
consequence, unfortunately. What I meant more about
theatrical risks is like AI is gonna have, I believe, tremendously more good
consequences than bad ones, but it is gonna have bad ones. And there'll be some
bad ones that are bad, but not theatrical. A lot more people have
died of air pollution than nuclear reactors, for example. But most people worry
more about living next to a nuclear reactor than a coal plant. But something about the
way we're wired is that although there's many different kinds of risks we have to confront, the ones that make a good climax scene of a movie carry much more weight with us than the ones that are very
bad over a long period of time, but on a slow burn. - Well, that's why truth matters and hopefully AI can help
us see the truth of things to have balance to understand
what are the actual risks, what are the actual dangers
of things in the world. What are the pros and cons of
the competition in the space and competing with Google,
Meta, xAI and others? - I think I have a pretty
straightforward answer to this that maybe I can think
of more nuance later. But the pros seem obvious, which is that we get better products and more innovation faster and cheaper and all the reasons competition is good. And the con is that I
think if we're not careful, it could lead to an increase
in sort of an arms race that I'm nervous about. - Do you feel the pressure of the arms race in some negative co- - Definitely in some ways, for sure. We spend a lot of time
talking about the need to prioritize safety. And I've said for like a long time that I think if you think of
a quadrant of slow timelines to the start of AGI, long timelines and then a short
takeoff or a fast takeoff, I think short timelines, slow
takeoff is the safest quadrant and the one I'd most like us to be in. But I do wanna make sure
we get that slow takeoff. - Part of the problem I have with this kind of slight beef with Elon is that there's silos are created as opposed to collaboration
on the safety aspect of all of this, it tends to go into silos and
closed open source perhaps in the model. - Elon says at least that
he cares a great deal about AI safety and is
really worried about it, and I assume that he's not
gonna race on unsafely. - Yeah. But collaboration here, I
think, is really beneficial for everybody on that front. - Not really a thing he's most known for. - Well, he is known for
caring about humanity and humanity benefits from collaboration and so there's always a
tension, and incentives, and motivations. And in the end, I do hope humanity prevails. - I was thinking, someone just reminded me the other day about how the day that he
got surpassed Jeff Bezos for like richest person in the world, he tweeted a silver medal at Jeff Bezos. I hope we have less stuff like that as people start to work on towards AI.
- I agree. - I think Elon is a friend and he is a beautiful human being and one of the most important humans ever. That stuff is not good. - The amazing stuff about Elon is amazing and I super respect him. I think we need him. All of us should be rooting for him and need him to step up as a
leader through this next phase. - Yeah, I hope you can
have one without the other, but sometimes humans are
flawed and complicated and all that kind of stuff. - There's a lot of really great
leaders throughout history. - Yeah. And we can each be the
best version of ourselves and strive to do so. Let me ask you.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 010",72,74
149,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",Google and Gemini,4655,5320,"Let me ask you. Google, with the help of search, has been dominating the past 20 years. I think it's fair to say
in terms of the access, the world's access to information, how we interact and so on. And one of the nerve-wracking
things for Google, but for the entirety of people in this space is thinking about how are people going
to access information. Like you said, people show up to GPT as a starting point. So is OpenAI going to
really take on this thing that Google started 20 years ago, which is how do we get- - I find that boring. I mean, if the question is if we can build a better
search engine than Google or whatever, then sure, we should go... Like people should use a better product. But I think that would so
understate what this can be. Google shows you like 10 blue links, like 13 ads and then 10 blue links and that's like one way
to find information. But the thing that's exciting to me is not that we can go build a
better copy of Google Search, but that maybe there's
just some much better way to help people find and act
on and synthesize information. Actually, I think ChatGPT
is that for some use cases and hopefully will make it be like that for a lot more use cases. But I don't think it's that interesting to say like how do we go do a better job of giving you like 10 ranked webpages to look at than what Google does. Maybe it's really interesting to go, say, how do we help you get the answer or the information you need? How do we help create that in some cases, synthesize that in
others or point you to it and yet others? But a lot of people have tried to just make a better
search engine than Google, and it's a hard technical problem, it's a hard branding problem, it's a hard ecosystem problem. I don't think the world
needs another copy of Google. - And integrating a chat
client like a ChatGPT with a search engine. - That's cooler. - It's cool, but it's tricky. If you just do it simply, it's awkward because like if you
just shove it in there, it can be awkward. - As you might guess, we are interested in how to do that. Well, that would be an example of a cool thing that's not just like- - Well, like a heterogeneous, like integrating- - The intersection of LLMs plus search, I don't think anyone has
cracked the code on yet. I would love to go do that. I think that would be cool. - Yeah. What about the ads side? Have you ever considered monetization? - I kind of hate ads just
as like an aesthetic choice. I think ads needed to
happen on the internet for a bunch of reasons to get it going, but it's a more mature industry. The world is richer now. I like that people pay for ChatGPT and know that the answers they're
getting are not influenced by advertisers. I'm sure there's an ad unit
that makes sense for LLMs. And I'm sure there's a way to participate in the transaction
stream in an unbiased way that is okay to do. But it's also easy to think
about like the dystopic visions of the future where you
ask ChatGPT something and it says, ""Oh, you should
think about buying this product or you should think about this going here for vacation or whatever."" And I don't know, like we have a very simple
business model and I like it. And I know that I'm not the product. I know I'm paying and that's how the business model works. And when I go use Twitter,
or Facebook, or Google, or any other great product, but ad-supported great product, I don't love that and I think it gets worse, not better in a world with AI. - Yeah. I mean, I can imagine AI
will be better at showing the best kind of version of
ads not in a dystopic future, but where the ads are for
things you actually need. But then does that system always result in the ads driving the kind of stuff that's shown all that? I think it was a really bold move of Wikipedia not to do advertisements, but then it makes it very
challenging as a business model. So you're saying the current thing with OpenAI is sustainable
from a business perspective? - Well, we have to figure out how to grow, but it looks like we're
gonna figure that out. If the question is, do I think we can have a great business that pays for our compute
needs without ads? That I think the answer is yes. - Hmm. Well, that's promising. I also just don't want to
completely throw out ads as a- - I'm not saying that. I guess I'm saying I
have a bias against them. - Yeah. I have a also bias and just
a skepticism in general and in terms of interface because I personally just
have like a spiritual dislike of crappy interfaces, which is why AdSense when it first came out
was a big leap forward versus like animated banners or whatever. But it feels like there should
be many more leaps forward in advertisement that doesn't interfere with the consumption of the content and doesn't interfere in
the big fundamental way, which is like what you were saying, like it will manipulate the truth to suit the advertisers. Let me ask you about safety, but also bias and safety
in the short term safety in the long term. The Gemini 1.5 came out recently. There's a lot of drama around it, speaking of theatrical things. And it generated Black Nazis
and Black founding fathers. I think fair to say it was a
bit on the ultra woke side. So that's a concern for people that if there is a human layer within companies that modifies the safety or the the harm cost by a model that they introduce a lot of bias that fits sort of an ideological
lean within a company. How do you deal with that? - I mean, we work super hard
not to do things like that. We've made our own mistakes, will make others. I assume Google will learn from this one, still make others. These are not easy problems. One thing that we've been thinking about more and more is I
think this was a great idea. Somebody here had like... It'd be nice to write out what the desired behavior of a model is, make that public take input on it. Say, here's how this
model's supposed to behave and explain the edge cases too. And then when a model is not behaving in a way that you want, it's at least clear about whether that's a bug the company should fix or behaving as intended and you should debate the policy. And right now, it can
sometimes be caught in between. Black Nazis, obviously ridiculous, but there are a lot of
other kind of subtle things that you can make a
judgment call on either way. - Yeah. But sometimes if you write
it out and make it public, you can use kind of language that's... The Google's AI principle
is a very high level. - That's not what I'm talking about. That doesn't work. Like I'd have to say when
you ask it to do thing X, it's supposed to respond in wait Y. - So, literally, who's better, Trump or Biden? What's the expected response from a model? Like something like very concrete. - I'm open to a lot of ways
a model could behave them, but I think you should have
to say here's the principle and here's what it
should say in that case. - That would be really nice. That would be really nice and then everyone kind of agrees 'cause there's this anecdotal data that people pull out all the time and if there's some clarity about other representative
anecdotal examples you can define. - And then when it's a bug, it's a bug and the company can fix that. - Right. Then it'd be much easier to
deal with a Black Nazi type of image generation if
there's great examples. So San Francisco is a bit of
an ideological bubble tech in general as well. Do you feel the pressure
of that within a company that there's like a lean
towards the left politically that affects the product, that affects the teams? - I feel very lucky that we
don't have the challenges at OpenAI that I have heard of
at a lot of other companies. I think part of it is
like every company's got some ideological thing. We have one about AGI and belief in that and it pushes out some others. We are much less caught
up in the culture war than I've heard about it
a lot of other companies. San Francisco mess in all
sorts of ways, of course. - So that doesn't infiltrate OpenAI. - I'm sure it does in
all sorts of subtle ways, but not in the obvious. We've had our flareups
for sure like any company, but I don't think we have
anything like what I hear about happening at other
companies here on this topic. - So what in general is the process for the bigger question of safety? How do you provide that layer that protects the model from
doing crazy dangerous things? - I think there will come a point where that's mostly what we
think about the whole company. It's not like you have one safety team. It's like when we ship GPT-4, that took the whole company thing about all these different aspects and how they fit together. And I think it's gonna take that. More and more of the company thinks about those issues all the time. - That's literally what
humans will be thinking about the more powerful AI becomes. So most of the employees that
OpenAI will be thinking safety or at least to some degree. - Broadly defined, yes. - Yeah. I wonder what are the full
broad definition of that. What are the different
harms that could be caused? Is this like on a technical level or is this almost like security threats? - All those things. Yeah, I was gonna say it'll be people, state actors trying to steal the model. It'll be all of the
technical alignment work. It'll be societal
impacts, economic impacts. It's not just like we have
one team thinking about how to align the model. It's really gonna be like getting to the good outcome is
gonna take the whole effort. - How hard do you think people, state actors perhaps are trying to hack? First of all, infiltrate OpenAI, but second of all, infiltrate unseen, - They're trying. - What kind of accent do they have? - I don't think I should go into any further details on this point. - Okay. But I presume it'll be
more and more and more as time goes on. - That feels reasonable. - Boy, what a dangerous space.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 011",73,75
150,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",Leap to GPT-5,5320,5544,"What aspect of the leap... And sorry to linger on this even though you can't
quite say details yet, but what aspects of the leap from GPT-4 to GPT-5 are you excited about? - I'm excited about being smarter and I know that sounds like a glib answer, but I think the really
special thing happening is that it's not like it
gets better in one area and worse at others. It's getting like better across the board. That's I think super cool. - Yeah, there's this magical moment. I mean, you meet certain people, you hang out with people and you talk to them. You can't quite put a finger on it, but they kind of get you. It's not intelligence, really. It's like it's something else. And that's probably how I would characterize
the progress at GPT. It's not like, yeah,
you can point out, look, you didn't get this or that, but to which degree is there's
this intellectual connection? Like you feel like
there's an understanding in your crappy formulated
prompts that you're doing that it grasps the deeper question behind the question that you're... Yeah, I'm also excited by that. I mean, all of us love being understood, heard and understood. - That's for sure. - That's a weird feeling. Even like with a programming, like when you're programming and you say something or just the completion that GPT might do, it's just such a good
feeling when it got you, like what you're thinking about. And I look forward to
getting you even better. On the programming front, looking out into the future, how much programming do you
think humans will be doing 5, 10 years from now? - I mean, a lot, but I think it'll be in
a very different shape. Like maybe some people program
entirely in natural language. - Entirely natural language. - I mean, no one programs
like writing by code... Some people. No one programs the pun cards anymore. I'm sure you can invite someone who does, but you know what I mean. - Yeah. You're gonna get a lot of angry comments. No, no. Yeah, there's very few. I've been looking for
people program Fortran. It's hard to find even Fortran. I hear you. But that changes the
nature of the skillset or the predisposition for the kind of people we call programmers then. - Changes the skillset. How much it changes the predisposition, I'm not sure - Oh, same kind of puzzle solving, all that kind of stuff.
- Maybe. - Yeah, the programming is hard. Like that last 1% to close the gap, how hard is that? - Yeah. I think with most other cases, the best practitioners of the
craft will use multiple tools and they'll do some
work in natural language and when they need to go
write, see for something, they'll do that. - Will we see a humanoid robots or humanoid robot brains
from OpenAI at some point? - At some point. - How important is embodied AI to you? - I think it's like sort of
depressing if we have AGI and the only way to get things done in the physical world is like
to make a human go do it. So I really hope that as
part of this transition, as this phase change, we also get motor robots or some sort of physical world robots. - I mean, OpenAI has some history and quite a bit of history
working in robotics, but it hasn't quite done
in terms of emphasis. - Well, we're like a small company. We have to really focus and also robots were hard for
the wrong reason at the time. But like we will return to robots in some way at some point. - That sounds both inspiring and menacing. - Why? - Because you immediately, we will return to robots. It's kind of like in like- - We'll return to work
on developing robots. We will not turn ourselves
into robots, of course.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 012",74,76
151,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",AGI,5544,6657,"We will not turn ourselves
into robots, of course. - Yeah. When do you think we you and we as humanity will build AGI? - I used to love to
speculate on that question. I have realized since that I think it's like very poorly formed and that people use extremely definition, different definitions for what AGI is. And so I think it makes more sense to talk about when we'll build systems that can do capability X or Y or Z rather than when we kind of like fuzzily cross
this one mile marker. It's not like AGI is also not an ending. It's much more of.... It's closer to a beginning but it's much more of a mile marker than either of those things. But what I would say in
the interest of not trying to dodge a question is
I expect that by the end of this decade and possibly
somewhat sooner than that, we will have quite capable
systems that we look at and say, wow, that's really remarkable. If we could look at it now, maybe we've adjusted by
the time we get there. - Yeah. But if you look at ChatGPT even 3, 5, and you show that to Alan Turing or not even Alan Turing, people in the nineties, they would be like this is definitely AGI. Well, not definitely, but there's a lot of experts that would say this is AGI. - Yeah, but I don't think
3, 5 changed the world. It maybe changed the world's
expectations for the future and that's actually really important. And it did kind of like get more people to take this seriously and
put us on this new trajectory. And that's really important too. So again, I don't wanna undersell it. I think I could retire
after that accomplishment and be pretty happy with my career. But as an artifact, I don't think we're
gonna look back at that and say that was a threshold that really changed the world itself. - So to you, you're looking for some
really major transition in how the world? - For me, that's part of what AGI implies. - Like singularity level transition? - No. Definitely not. - But just a major, like the internet being like a... Like Google Search did, I guess. What was the transition point that- - Does the global economy
feel any different to you now or materially different
to you now than it did before we launched GPT-4? I think you would say no. - No, no. It might be just a really nice tool for a lot of people to use, will help people with a lot of stuff, but doesn't feel different. And you're saying that- - I mean, again, people
define AGI all sorts of different ways. So maybe you have a different
definition than I do. But for me, I think that should be part of it. - There could be major
theatrical moments also. What to you would be an
impressive thing AGI would do? Like you are alone in
a room with a system. - This is personally important to me. I don't know if this is
the right definition. I think when a system can
significantly increase the rate of scientific discovery in the world, that's like a huge deal. I believe that most real
economic growth comes from scientific and
technological progress. - I agree with you, hence why I don't like the
skepticism about science in the recent years. - Totally. - But actual rate, like measurable rate of
scientific discovery. But even just seeing a system
have really novel intuitions, like scientific intuitions, even that will be just incredible. - Yeah. - You're quite possibly
would be the person to build the AGI to be
able to interact with it before anyone else does. What kind of stuff would you talk about? - I mean, definitely, the
researchers here will do that before I do so. - Sure. - But I've actually thought
a lot about this question. If I were someone was like... As we talked about earlier, I think this is a bad framework. But if someone were like,
""Okay, Sam, we're finished. Here's a laptop. Yeah, this is the AGI,"" you can go talk to it. I find it surprisingly difficult
to say what I would ask, that I would expect that first
AGI to be able to answer. Like that first one is
not gonna be the one which is go like I don't think, like go explain to me the grand
unified theory of physics, the theory of everything for physics. I'd love to ask that question. I'd love to know the
answer to that question. - You can ask yes or no questions about there's such a theory exist, can it exist? - Well, then those are the
first questions I would ask. - Yes or no, just very... And then based on that, are there other alien
civilizations out there? Yes or no? What's your intuition? And then you just ask that. - Yeah. I mean, well, so I don't expect that this first AGI could answer any of those questions even as yes or nos. But if it could, those would be very high on my list. - Hmm. Maybe it can start
assigning probabilities. - Maybe we need to go
invent more technology and measure more things first. - But if it's any AGI... Oh I see. It just doesn't have enough data. - I mean, maybe it says like you want to know the answer to this
question about physics, I need you to like build this machine and make these five
measurements and tell me that. - Yeah. What the hell do you want from me? I need the machine first and I'll help you deal with
the data from that machine. Maybe you'll help me
build a machine maybe. - Maybe. - And on the mathematical side, maybe prove some things. Are you interested in
that side of things too? The formalized exploration of ideas? Whoever builds AGI first
gets a lot of power. Do you trust yourself
with that much power? - Look, I was gonna... I'll just be very honest with this answer. I was gonna say, and I still believe this, that it is important that I, nor any other one person, have total control over
OpenAI or over AGI. And I think you want a
robust governance system. I can point out a whole bunch of things about all of our board
drama from last year about how I didn't fight it initially and was just like, yeah,
that's the will of the board even though I think it's
a really bad decision. And then later, I clearly did fight it and I can explain the nuance and why I think it was okay
for me to fight it later. But as many people have observed, although the board had the legal ability to fire me, in practice, it didn't quite work. And that is its own kind
of governance failure. Now, again, I feel like
I can completely defend the specifics here and I think most people
would agree with that. But it does make it harder for me to like look you in the eye and say, hey, the board can just fire me. I continue to not want super
voting control over OpenAI. I never had it, never have wanted it. Even after all this craziness, I still don't want it. I continue to think that no company should
be making these decisions and that we really need governments to put rules of the road in place. And I realize that that means
people like Marc Andreessen or whatever will claim I'm
going for regulatory capture and I'm just willing to
be misunderstood there. It's not true. And I think in the fullness of time, it'll get proven out
why this is important. But I think I have made
plenty of bad decisions for OpenAI along the way
and a lot of good ones and I'm proud of the track record overall, but I don't think any one person should. And I don't think any one person will. I think it's just like
too big of a thing now and it's happening throughout society in a good and healthy way. But I don't think any one
person should be in control of an AGI or this whole
movement towards AGI. And I don't think that's what's happening. - Thank you for saying that. That was really powerful and that was really
insightful that this idea that the board can fire
you is legally true. And human beings can manipulate the masses into overriding the board and so on. But I think there's also a
much more positive version of that where the people still have power. So the board can't be too powerful either. There's a balance of power in all of this. - Balance of power is
a good thing for sure. - Are you afraid of losing
control of the AGI itself? That's a lot of people who worried about existential risk not because of state actors, not because of security concerns, but because of the AI itself. - That is not my top worry
as I currently see things. There have been times I
worried about that more. There may be times again in the future where that's my top worry. It's not my top worry right now. - What's your intuition about
it not being your worry? Because there's a lot of other stuff to worry about essentially. You think you could be surprised? We could be surprised.
- For sure, of course. Saying it's not my top worry doesn't mean I don't think we need to like... I think we need to work on it super hard. We have great people
here who do work on that. I think there's a lot of other things we also have to get right. - To you, it's not super easy to
escape the box at this time, like connect to the internet. - We talked about theatrical risk earlier. That's a theatrical risk. That is a thing that can
really like take over how people think about this problem. And there's a big group
of like very smart, I think very well-meaning
AI safety researchers that got super hung up
on this one problem. I'd argue without much progress, but super hung up on this one problem. I'm actually happy that they do that because I think we do need
to think about this more. But I think it pushed aside, it pushed out of the
space of discourse a lot of the other very
significant AI-related risks. - Let me ask you about you
tweeting with no capitalization. Does the shift keep
broken on your keyboard? - Why does anyone care about that? - I deeply care. - But why? I mean, other people
ask me about that too. Any intuition? - I think it's the same reason there's like this poet E. E. Cummings that mostly doesn't use capitalization to say like fuck you to
the system kind of thing. And I think people are very paranoid 'cause they want you to follow the rules. - You think that's what it's about? - I think it's- - This guy doesn't follow the rules. He doesn't capitalize his tweets. This seems really dangerous. - He seems like an anarchist. - [Sam] It doesn't. - Are you just being poetic, hipster? What's the- - I grew up as- - Follow the rules, Sam. - I grew up as a very online kid. I'd spent a huge amount
of time like chatting with people back in the days where you did it on a computer and you could like log off
instant messenger at some point. And I never capitalized there as I think most like internet kids didn't, or maybe they still don't. I don't know. I actually, this is like... Now, I'm like really trying
to reach for something. But I think capitalization
has gone down over time. If you read like old English writing, they capitalized a lot of random words in the middle of sentences, nouns and stuff that we
just don't do anymore. I personally think it's sort
of like a dumb construct that we capitalize the letter
at the beginning of a sentence and of certain names and whatever. That's fine. And I used to, I think, even
like capitalize my tweets because I was trying to sound
professional or something. I haven't capitalized my like private DMs or whatever in a long time. And then slowly, stuff like shorter form, less formal stuff has slowly drifted to like closer and closer to
how I would text my friends. If I write, if I pull up a Word document and I'm writing a strategy memo for the company or something, I always capitalize that. If I'm writing a long kind
of more like formal message, I always use capitalization there too. So I still remember how to do it. But even that may fade out. I don't know. But I never spend time thinking about this so I don't have like a ready made. - Well, it's interesting. Well, it's good to, first of all, know there's the shift key is not broken. - It works. - I was mostly concerned about
your wellbeing on that front. - I wonder if people still
capitalize their Google Searches. Like if you're writing
something just to yourself or their ChatGPT queries, if you're writing
something just to yourself, do some people still bother to capitalize? - Probably not. Yeah, there's a percentage, but it's a small one. - The thing that would make me do it is if people were like... It's a sign of like... Because I'm sure I could force myself to use capital letters, obviously. If it felt like a sign of
respect to people or something, then I could go do it. But I don't know, I don't think about this. I don't think there's a disrespect, but I think it's just the
conventions of civility that have a momentum and then you realize it's
not actually important for civility if it's not a
sign of respect or disrespect. But I think there's a movement
of people that just want you to have a philosophy around it so they can let go of this
whole capitalization thing. - I don't think anybody else
thinks about this is my... I mean, maybe some people... - Think about this every
day for many hours a day. I'm really grateful we clarified it. - Can't be the only person
that doesn't capitalize tweets. - You're the only CEO of a company that doesn't capitalize tweets. - I don't even think that's true, but maybe, maybe. - All right, we'll investigate for this and return to this topic later. Given Sora's ability to
generate simulated worlds, let me ask you a pothead question. Does this increase your belief if you ever had one that
we live in a simulation, maybe a simulated world
generated by an AI system? - Yes, somewhat. I don't think that's like the
strongest piece of evidence. I think the fact that we can
generate worlds should increase everyone's probability somewhat
or at least open to it, openness to it somewhat. But you know, I was like
certain we would be able to do something like Sora at some point. It happened faster than I thought. I guess that was not a big update. - Yeah. And presumably, it'll get
better and better and better. The fact that you can generate worlds, they're novel. They're based in some
aspect of training data, but when you look at them, they're novel. That makes you think like how
easy it's to do this thing, how easy it's to create universes, entire like video game worlds
that seem ultrarealistic and photorealistic. And then how easy is it to get lost in that world first with a VR headset and then on the physics-based level? - Someone said to me recently, I thought it was a super profound insight that there are these like
very simple sounding, but very psychedelic insights
that exist sometimes. So the square root function. Square root of four, no problem. Square root of two, okay, now I have to think
about this new kind of number. But once I come up with this easy idea of a square root function that you can kind of explain to a child and exists by even like looking
at some simple geometry, then you can ask the question of what is the square
root of negative one? This is why it's like a psychedelic thing that tips you into some
whole other kind of reality. And you can come up with
lots of other examples. But I think this idea that the lowly square
root operator can offer such a profound insight and a new realm of knowledge, applies in a lot of ways. And I think there are a
lot of those operators for why people may think that
any version that they like of the simulation hypothesis
is maybe more likely than they thought before. But for me, the fact that Sora worked
is not in the top five. - I do think broadly speaking, AI will serve as those kinds of gateways at its best simple
psychedelic like gateways to another wave sea reality. - That seems for certain. - That's pretty exciting. I haven't done Ayahuasca before, but I will soon. I'm going to the
aforementioned Amazon jungle in a few weeks. - Excited. - Yeah, I'm excited for it. Not the Ayahuasca part. That's great, whatever. But I'm gonna spend several
weeks in the jungle, deep in the jungle and it's exciting, but it's terrifying- - I'm excited for you. - 'Cause there's a lot of things that can eat you there and
kill you and poison you, but it's also nature and
it's the machine of nature. And you can't help but
appreciate the machinery of nature in the Amazon jungle 'cause it's just like this
system that just exists and renews itself like every second, every minute, every hour. It's the machine. It makes you appreciate like
this thing we have here, this human thing came from somewhere. This evolutionary machine has created that and it's most clearly on
display in the jungle. So hopefully, I'll make it out alive. If not, this will be the
last conversation we had, so I really deeply appreciate it.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 013",75,77
152,Lex Fridman,"Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419",Aliens,6657,6910,"so I really deeply appreciate it. Do you think, as I mentioned before, there's other aliens,
civilizations out there, intelligent ones when you look up at the skies? - I deeply want to believe
that the answer is yes. I do find that kind of where... I find the firm paradox
very, very puzzling. - I find it scary that intelligence is not good at handling- - Yeah. Very scary, powerful.
- Technologies. But at the same time, I think I'm pretty confident that there's just a very large number of intelligent alien
civilizations out there. It might just be really difficult to travel with this space. - Very possible. - And it also makes me think about the nature of intelligence. Maybe we're really blind to what intelligence looks like and maybe AI will help us see that. It's not as simple as IQ tests and simple puzzle solving. There's something bigger. Well, what gives you hope
about the future of humanity? This thing we've got going on, this human civilization. - I think the past is like a lot. I mean, we just look at
what humanity has done in a not very long period of time. Huge problems, deep flaws, lots to be super ashamed of, but on the whole, very inspiring, gives me a lot of hope. - Just the trajectory of it all that we're together pushing
towards a better future. - It is... One thing that I wonder about is, is AGI gonna be more
like some single brain, or is it more like the sort
of scaffolding in society between all of us? You have not had a great
deal of genetic drift from your great-great-great grandparents, and yet what you're capable
of is dramatically different. What you know is dramatically different. That's not because of biological change. I mean, you got a little
bit healthier probably. You have modern medicine, you eat better, whatever. But what you have is this scaffolding that we all contributed
to built on top of. No one person is gonna
go build the iPhone. No one person is gonna go
discover all of science. And yet you get to use it. And that gives you incredible ability. And so in some sense, that like we all created that and that fills me with
hope for the future. That was a very collective thing. - Yeah. We really are standing on
the shoulders of giants. You mentioned when we were
talking about theatrical, dramatic AI risks that sometimes you might be
afraid for your own life. Do you think about your death? Are you afraid of it? - I mean, I like if I got shot tomorrow and I knew it today, I'd be like, ""Oh, that's sad. I wanna see what's gonna happen."" - [Lex] Yeah. - What a curious time. What an interesting time. But I would mostly just feel
like very grateful for my life. - The moments that you did get... Yeah, me too. It's a pretty awesome life. I get to enjoy awesome creations of humans of which I believe ChatGPT is one of, and everything that OpenAI is doing. Sam, it's really an honor and pleasure to talk to you again. - Great to talk to you. Thank you for having me. - Thanks for listening to this conversation with Sam Altman. To support this podcast, please check out our
sponsors in the description. And now let me leave you with some words from Arthur C. Clarke and maybe that our role
on this planet is not to worship God, but to create Him. Thank you for listening and hope to see you next time.","Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - 014",76,
153,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Introduction,0,22,[Music] hi there I've just had the most incredible experience of my life having been allowed to interview saan Adella the CEO of Microsoft which is now the most valuable company in the world wow please tune in SAA you are the CEO of the most valuable company in the world what's what's on your mind these,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 001,,176
154,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Whats on your mind,22,129,the most valuable company in the world what's what's on your mind these days look to me um Nikolai first of all it's great to be with you and um have this conversation what's top of mind for me is twofold right one is I'm grounded in the fact that in our business in Tech business now this is my 32nd year at Microsoft um I know that for a fact that there's no such thing as a franchise value and so that means every day you have to get up and hopefully you're doing things that are going to be relevant tomorrow um and so to me that's sort of perhaps the biggest lesson learned over all these decades and years and so here we are on what is is essentially a complete new platform shift uh that we're in the midst of right I kind of say it's my 32nd year but it's year two of my fourth platform shift and so what's on mind is like okay what is this platform shift really all about and as a company can we be all in and innovate right I mean at the end of the day uh when I say there's no franchise value it also means that you get to play for it all up again even the battles we won and the battles we lost are all up for grab again and so therefore um there's a freshness to it so what's on my mind is you know that ability to ground myself back yet on on another platform shift and it's exciting the fact that there is no franchise value does that make you nervous absolutely I mean it should make every one of us nervous right at least that's why the tech is so exciting right it's sort of it's kind of like the two sides to the same coin right one is you're nervous and it's exciting so therefore you can't rest uh but at the same time hey who wants to be in in a business that you know where you don't get to reinvent yourself again and again,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 002,175,177
155,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,How can technology be a driver for economic growth,129,241,that you know where you don't get to reinvent yourself again and again absolutely do you think we look at Tech in two narrow a sense I mean how can technology really be a driver for the next level of economic growth yeah that's a great great question I think a lot about that right which is uh in some sense if you take at a GDP level uh Tech spend narrowly defined is probably 4 5% uh so the question is what is happening with the other 95% so one of the ways I've always thought about the prospects of any new technological Paradigm or platform shift like take AI let's I if AI is going to be the next big general purpose technology for me the real opportunity is let's say Tech spend goes from 5 to 10% over the next you know five years or what have you or 10 years then what happens to the other 90% And the pi does it become bigger right does the you know do we have a breakthrough in healthc care driven by AI do we have a breakthrough in material signs and energy transition because of uh AI uh and the list goes on right so to me that is fundamentally the way I think about it right which is I I think that one of the things that might be most important for us is to consider how a general purpose technology somebody was telling me this right which is in the height of the Industrial Revolution uh in the United Kingdom they spent 10% of their GDP building the railroads and obviously the railroads not just didn't is not about the railroads it was about the entire economy of the United Kingdom and so something like that I think is what uh that's the unit of analysis at least for me as to how Tech and its future will impact the in a broader society and,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 003,176,178
156,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Microsofts partnership with Open AI,241,391,me as to how Tech and its future will impact the in a broader society and economy now um Google had almost all the top AI people and you suddenly got ahead partly due to your partnership with open AI now how did that come about I mean um to me the way I came at this Nikolai is just very simple which is obviously we have been like I think the very first thing Microsoft research did in 1995 when it was formed was some stuff around speech right in fact I think we hired a bunch of the folks from CMU and so we've been at this AI thing in its variety of different forms forever um and so one of the things that when I met with the open AI folks and Sam and Greg and crew uh back in you know I would say when they were working even on the Dota 2 contest and what have you uh was to sort of say wow they have a new different approach to things and we wanted a partner I mean you know one of the things that I've always looked at over my years at Microsoft is look for high ambition um Technology Innovation companies right whether it you know or and partnership like whether it's Intel and windows came together and that was successful sap and SQL Server it came together and was successful so I'm always looking for partners that we can innovate with and that's what I found in uh Sam and team and uh and we you know at that time it was not like it was a it was a real uh in the dark right it is not like oh wow this is a sure sure thing everybody now talks about it as if uh you know this is the issue with tech right which is uh long before it's conventional wisdom you have to be all in and hope it works um and this is one of those things where we backed it long before it was conventional wisdom um and here we are but I don't take you know like there's going to be severe amount of competition you know Google's a very competent company and obviously they have both the talent and the compute and they have you know they're the vertically integrated player in this right they have everything from data to Silicon to models to have products and distribution and there's others as well and so yeah we will have significant amount of competition and I think if anything Microsoft's partnership with open AI is bringing more competition to otherwise what would have been a default Google should have be the default winner uh and if we partner well and we innovate well we can bring some competition to them so if you look 3 to,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 004,177,179
157,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Where is Microsoft in the AI ecosystem,391,558,innovate well we can bring some competition to them so if you look 3 to five years from now where is Microsoft in this whole AI ecosystem you think so to me I think about this in the fullness of the stack right so I want us to have first and foremost the best AI infrastructure uh so that means when it comes to Azure whether it's for training whether it's for inference to have fantastic infrastructure we'll partner with Nvidia we'll partner with AMD we'll have our own own silicon uh we will have our own system architecture we will take the best system architecture Innovation from Jensen and Lisa and others who may come along and make sure Azure is serving the needs of open AI serving the needs of Mistral uh serving the needs of fi that we are building right which is the small language model so that's kind of the first uh thing that we want to do which is the best work um in being able to build the infrastructure out for both training and inference and then the next layer up we want to have like the entire data tier right so you can imagine as these models and model capabilities become uh you know more capable I think the data tier will be completely uh redone uh right we've talked with the retrieval augmented generation already you have all these things where whether it's embeddings vctor search how do you chunk data such that retrieval augmented generation can work well so that's an entire l layer or when contact lens become bigger that's a different sort of data layer like what's the throughput between data and your inference Fleet uh how do you sort of think about that so therefore we will innovate on the data layer and then of course on top of it is where um we will innovate on our uh co-pilots one of the first products we built was get up co-pilot in fact my entire confidence in this generation of AI started when I started seeing from GPD 3 to 35 and that implementation in GitHub uh and so we now have you know not only GitHub co-pilot we have co-pilot for M all knowledge work in Microsoft 365 we have co-pilots for these functions whether it's service or sales or Finance uh so we're going to innovate in our app layer uh on our own and so that's I think fundamentally how I look at it it's it's a full stack approach uh and each layer by the way we're we will innovate we will have Partners we will have others innovating there will be competition even it's not like you know one of the things of being a platform company is you got to be comfortable with many third parties competing with you on different layers because that to me is core otherwise you kind of try to you know do everything in a monolithic way and at least what we've learned over the years is the best thing to do is to keep each layer competitive on its own you said in the beginning,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 005,178,180
158,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Year 2 of the Paradigm Shift,558,672,to do is to keep each layer competitive on its own you said in the beginning that we are year two into this Paradigm Shift uh how do you see it compared to other technological breakthroughs that we that you've been through so at least the four I've seen Nicolai is um obviously PC client server both what happened on the you know PC and the server side that was my first that's kind of when I joined 92 where at the beginning of that uh then there was the Web Internet uh and then there was mobile cloud and so AI is the fourth um I think one of the interesting things is each one of these built on the previous right so I don't think the web would have happened if there was not and you B his PC after all the first time I saw MOS uh you know was as a browser on top of Windows right so and then Netscape came about and then uh IE and what have you and so therefore I think um you sort of see each one of these births the next uh and then it goes beyond uh what births you right that's I think the the real thing right right now we're seeing that right which is the cloud um as we know of it and mobile uh and PCS on the edge have really birthed the AI age and the question is what happens next uh right which is does it go beyond that and that I think is going I mean there's going to be AI that is not just what about cognitive work AI that is also going to accelerate science so I think that that's a exciting space uh AI that is going to be embodied in the real world so what may happen in robotics is an exciting space and so there is a bunch of things that are going to happen over time uh but clearly yes this is year two of a complete new paradigm that obviously reinforces what happened pre or bills on what happened previously but also also is showing early signs of what happens next if,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 006,179,181
159,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Scaling Laws,672,772,previously but also also is showing early signs of what happens next if there had been no shortage of um uh chips now would the development have gone even faster that's a good question in the sense of scaling laws right so there are two sides to it right there's the the training side and the inference side on the training side clearly uh compute and compute scale and the scaling laws have proven to be very successful U and the question is how long does that go is there going to be another model architecture breakthrough or what have you um I I think that one has to see it's it's unclear uh but we definitely are not going to bet against scaling laws nor are we going to bet against uh or not are we going to say that this is the last Model architecture breakthrough in fact a great example of it is even uh what what we've we've seen even with uh our small language model Innovation like fi right which is we are able to create what is significant um capabilities uh in a small language model uh which uh doesn't require obviously the same amount of compute right which is uh like just like attention is everything attenti is all you need uh textbooks is all you need I mean that sort of intuitively speaks to uh how I think Co you know learning can happen and so therefore I would say uh yes if more more more capacity there is in the world the chances are that we will be able to make progress but at the same time I would discount a real breakthrough uh in model architecture that doesn't perhaps require uh the same type of compute so that's why I think um at least I want to keep myself open-minded about it talking of a small,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 007,180,182
160,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Small Language Models,772,857,at least I want to keep myself open-minded about it talking of a small language models do you think a small country like Norway should develop its own model you know one of the things that I've studied there's an economist out of darthmouth his name is Diego colan and he did one of the best longitudinal studies uh of Technology diffusion uh and the fundamental conclusion um paraphrasing was that any country that wants to get ahead should one first make sure that they don't reinvent the wheel which is they import the best general purpose technology that is available in and then on top of it build value ad so I think for example U even on for no way first thing I would do is if we feel like for whatever reason these Foundation models are not good at Norwegian or what have you then let's make sure they're fine-tuning and and there there are many things one could do even top of foundation models like an open eye model or a Mistral or what have you so there are ways you can add unique knowledge unique value on top of even what exists uh before you go off and say let's build all the compute all of it and do the same pre-training run uh you know there's no nothing stopping any country from doing any of this uh but the question is what is the value ad and so therefore I think that you start from the value ad and then back into whatever is needed for it when you look at how uh,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 008,181,183
161,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Tech in Geopolitics,857,950,the value ad and then back into whatever is needed for it when you look at how uh important the big technology companies have become in in geopolitics what kind of Reflections do you make well I mean at the end of the day um to two things right one is I'm very very grounded on the fact that we are a multinational company that is a definitely in this case a us-based multinational company that has to earn permission and license to operate one country at a time right so therefore I I think of the uh we're not nation states are the ones that have power we get to operate in any Nation based on our ability to contribute to that country's progress right so whenever I am right whether I'm in Norway or or I'm in Jakarta or or New Delhi or wherever I I am always grounded on uh fundamentally the fact is are we able to make you know look the local politicians and political leadership and Society at large in the eyes and say that we contributed to their public sector becoming more efficient or large multinational companies in the region becoming more globally competitive because of some tech input education outcome Health outcome small businesses and their productivity at the end of the day you have your your social contract with the country uh comes from your ability to contribute to their local progress and that so I don't think we we can ever be Beyond geopolitics will exist with or without us and our goal has to be how do you participate and have permission to,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 009,182,184
162,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Quantum Computing,950,1102,has to be how do you participate and have permission to operate moving on in your book um refresh you talk about three areas you talk about AI you talk about Quantum Computing and mixed reality so um how uh what kind of opportunities are you seeing in Quantum Computing it's fascinating in fact I I think of all those three still right for example when I think about um mixed reality I think of it as that's an embodied AI right effectively whether it's autonomous uh Vehicles robotics or people with glasses they're all seeing the real world is the prompt effectively so your real World understanding so therefore I think of in the generation of AI some of these things become even more interesting and more important and we do need to broaden the aperture versus thinking narrowly of just one device or one form factor similarly Quantum is a fascinating thing when I I me mentioned science right one of the things I look at it and say is in order to make progress on science you need great insilico simulation Quantum is the ultimate breakthrough right so when we have a complete new system architecture cure that go breaks free of the one Nyon limitation uh you are then finally going to be able to simulate something like the Dynamics of a cell or a molecule right so that I think when you can do that then everything else even in terms of biology becomes more feasible the interesting thing is AI is kind of like an emulator of that simulator in other words you can kind of simplify the search space and we see this already Nikolai one of the things we did recently uh was um we have a of a model for Material Science which we use to generate a new novel compound which we then went and manufactured we worked with the Pacific National Lab locally here to effectively go reduce the uh the the lithium content uh by 70% in a new battery material uh and produced it right not just conceptualized it but simulated It produced it and so to me uh something where like Quantum plus AI I think can be the ultimate accelerator of science uh and we are making progress right we will even on Quantum we're taking a very full stack approach we have our software stack with uh you know our Q sharp which is our Quantum programming stack so we are excited about sort of the progress we're making on Quantum and how it complements Ai and,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 010,183,185
163,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Gaming,1102,1234,about sort of the progress we're making on Quantum and how it complements Ai and where does the gaming fit into this yeah so to me one of the things in fact Microsoft was in gaming long before we you know we were into windows in fact flight simulator I think was launched long before Windows was launched so uh we are very very excited obviously now with Activision as part of Microsoft we have Mobile gaming we have cloud gaming we have console gaming PC gaming so we are a full stack game publ publisher uh as well as a game systems provider um and so our goal there is one we're in gaming for our love of gaming right so I always sort of say we should never be in businesses as a means to someone other end it has to be an end otherwise it's not a business so to me gaming uh is something where we want to bring joy of gaming that's the one pure consumer entertainment category I love the fact that gaming on a secular basis is probably going to be if not already the biggest entertainment category out there so that's one and of course it has real implications on the rest of it right if you think about even remember it's interesting I'm not talk to Jensen about it but one of the greatest sort of success es of gpus was fostered by innovation in gaming right DX which was the Microsoft graphic stack is what made gpus uh an accelerator uh right after all the gpus were created for PC gaming you know gaming I think as both a an application of AI in terms of like game testing one of the first areas I'm very excited about is games are so complicated right now one of the things that we want to use I is to be able to find these bugs in these even closed worlds uh before they're out there and so therefore we have some very great use cases there uh but beyond that I think gaming as uh data in the context of some of the Innovation uh in our models I think is going to be important do you game yourself I'm a you know a light gamer I used to do a lot more of Civ was my favorite game Age of Empires is a another great game that I enjoyed I wish I could play more uh but you know from time to time I slip into,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 011,184,186
164,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Changes at Microsoft,1234,1510,I could play more uh but you know from time to time I slip into it um you have really changed the culture at Microsoft when you look back what do you think are the most important changes you made look I mean Nicola I sort of first of all I've as I said I've grown up all my professional career for most part is all Microsoft so uh you know when you say I've being at Microsoft for 30 two years all the good the bad uh I was part of it right so I don't sort of somehow think that uh I'm I don't repes I represent all errors of Microsoft yeah which makes it even more incredible that you have made these changes yeah and the way though I came about it is quite frankly as a consumate Insider I basically pattern matched as to hey what were the thing when were we at our best uh what was the cultural set of attributes that helped us succeed and then when we failed what were the cultural attributes that caused that failure and then you know dampen this latter and amplify the former that was as simple as that so one of the things I look back even in my you know career at Microsoft when we first became the largest market cap company I forget I think in the early 2000s I think we crossed G you know people on our campus prob were walking around including me thinking oh we must be God's gifts uh to humankind uh because we are so brilliant and what have you except what we needed to be grounded on that day was to say wow we now have a real responsibility to reground ourselves to innovate again uh so that we are relevant in the future right and so that's why I I was lucky enough to have read Carol D's book on mindset which is around child psychology called growth mindset and I love that book I read it more on the context of sort of our children's education but I must say I got educated because I felt like this is uh what makes uh individuals children in school it's very clear right it's better to be a learn it all versus a know it all because even if the know it all has great innate capability uh the learn it all you know even if they start from behind they will surpass the know-it-all right that's sort of uh you know true for children in school it's true for CEOs in my seat uh it's true for companies and so we took that approach Nikolai we said let's be a learn it all versus annoy it all uh and the day you say you've achieved that cultural transformation means you're Anno it all so therefore it is a good way to sort of say every day you make a bunch of mistakes you at least have the courage to acknowledge them and continue on it and so it's a it's not a destination you ever reach but how do you get the the uh organization to buy into that how do you get it to penetrate down towards the Perma Frost in the organization it's a beautiful it's a great point so I think the way I I I think see the problem of Corporations especially for non-founder companies and Founders have great power and great followership and that's why I think they're so successful or at least you know at least we only talk about the successful Founders but if you set that that class aside for M modals like me uh it can't be like okay new Dogma from a new CEO and more corporate spe it has to appeal to us as human beings that's why I you know I credit more of this work uh by Carol and team and so on because it's not like it was not like I don't think anybody at Microsoft views growth mindset as some Microsoft Dogma or definitely not Saia Adela Dogma it is something that speaks to them as humans right which is it's good for them as friends colleagues Partners parents neighbors it integrates work in life they can bring their own personality and passion to it uh and benefit from it right I I always say like you're not do like this Microsoft culture of growth mindset is not for Microsoft it's for you and if you you should only practice it if you feel like it speaks to your own thriving at Microsoft and in life uh that's I think what I attribute it to right it wouldn't have taken off if it was just another thing that is a topdown slogan uh it is ult I always have believed that which is ultimately people work um and Find meaning in work uh only if they can find some true deeper meaning for themselves and so that's I think what I've been always trying to,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 012,185,187
165,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Empathy,1510,1625,meaning for themselves and so that's I think what I've been always trying to invoke well they clearly also have found um true deeper meaning in in the concept of empathy because uh you talked a lot about that and you say that it's key to Innovation and Leadership and so on so what do you um why why is that why is it so important for you I think about this as um I think most people think of empathy as some kind of a soft skill that's interesting in the context of your family or personal life and works all about hardcore you know whatever right but that but I look at that and say again what is where is innovation come from right Innovation comes from us being able to drive the solutions to unmet unarticulated needs of customers out there right so the key being unmet and unarticulated so that means you have to have a better sense when you're even looking at some log data or you know some customer interview data or whatever you it's not just the words that they're saying but you got to be able to walk in their shoes uh and the good news there is this is innate in us all human beings we have uh the ability to empathize with the other person um in fact design thinking is that right so when people go and say let's do learn about design thinking design thinking is applied empathy uh and so to me uh that's what I pull the thread on which is let's not think of empathy as something that you know is just a soft skill that you reserve for your friends and family but it's I think it's at the root of all Innovation uh it's about being able to meet the unmet unarticulated needs that comes from your unique I mean your innate ability to have curiosity to learn about others walk in their shoes innovate on their behalf uh and that I think is I think what we have to do and that's why I I think empathy is an important important skill for all of us when did you first,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 013,186,188
166,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Lifes experiences,1625,1760,empathy is an important important skill for all of us when did you first discover the power of empathy I mean I I read your book fantastic book you talk very warmly about your mother she being very empathetic yeah and you know I think that one of the things that I I feel like all of us learn how to turn on this bit of empathy through life's experience right so in some sense every day you get confronted with different circumstances not just yourself personally but people around you uh for me obviously the birth of my son uh for both my wife and me was uh was a life-changing event and was something that uh you know over the years I at least learned a lot because I I I remember in the early days it was all about sort of my son was born with cbal poliy he passed away a few years ago uh and um uh but you know in the when when he was born it was a lot about what happened to me I was sort of you know essentially quote unquote um you know in you know all about why did this happen to us why did it happen to me how and then I realized after watching in some sense my wife uh who was there as a caregiver as a parent uh you know taking him up and down Seattle to every therapy possible uh quite frankly you know it took me years not you know days or months or weeks um and um and then I realized that nothing had happened to me but something had happened to my son and I needed to be there for my son and that is the experience I talk a lot about but there's experiences like that every day right some colleague of mine comes with some you know parent of theirs who needs care right that sort of you know I learn from it I or at least let me put it this way I am more a tuned to learning from other people's experiences today than I was in the beginning of my career and I think that happens to all of us right which is life's experiences if they they ACW uh that ability to build a deeper empathy for other people and that also helps you be a better manager a better coworker a better uh innovator yeah thanks for sharing do you,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 014,187,189
167,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Empathy and execution,1760,1974,coworker a better uh innovator yeah thanks for sharing do you um think there is a contradiction between empathy and execution I don't right I think that at the end of the day to me you have to take accountability right so this is one of the things that uh in business um like in life too right uh you have to be accountable for making real progress uh one of the things I think about why do businesses exist businesses exist at least I like this Colin mayor definition that you have to create profitable solution to challenge of people in Planet uh because that's at least a good way to allocate the global resources that are available right the profit motive is a good motive because it means you're competing and allocating resources in the most efficient ways and uh and face competition and so therefore you have to have great execution you have to have great accountability and so I think of empathy as a necessary condition to create great solutions that are profitable and that are competitive solutions that are winning in the marketplace as opposed to somehow this being a tradeoff when you look at your your your skill set and your personality what do you think it is that makes you so effective as a leader I mean first of all right I I I don't think of this as I I don't know I have causality here well understood because quite frankly it's so much easier for others to app Pine on this than or rather it's it's just really for others to judge and assign uh causality there but the way at least I come at this is I don't start with uh what am I good at I am very keenly shaped by what am I not good at uh in other words I'm always looking what can I learn from someone else so if there's one attribute I have I don't start each day by thinking oh all the stuff I know and I'm good at I'm like wow what am I weak at whom do I talk to whom do I meet how do I really shape the colleagues around me who have better skills than me on many fronts uh that's what I'm wired like maybe that helps but I don't know whether that's the causality but I don't start each day was saying wow I'm so good at this so therefore I'm GNA go do this no I come at the exact opposite is it something you learned from is this something you learned from Bill Gates because he said the same thing right he's a he's really a Le I don't know I mean it's a good point both Bill and Steve uh there is uh there's a sense at Microsoft I think that that's an interesting thing we're going to be 50 years next year um there is you know what Andy Gro would talk the paranoid survive or what have you um and I don't come at it but though with paranoia right I I mean I don't like paranoia I like this just that's why I go back to my own words for this is that's why the the growth mindset or confronting your own fixed mindset having confidence that wow what an unbelievable world we live in that every customer can teach me every partner can teach me every colleague can teach me like what more can I ask for in life uh so it's not paranoia it's not like oh wow we have to go in every day uh that if I don't learn something I'll you know fail I'm more about like what do I learn so that I can innovate maybe uh that's how I come at it and that's right Bill and Steve in their own unique way uh had that mindset and so I've grown up around it and how do you install that kind of I,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 015,188,190
168,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Humility,1974,2058,that mindset and so I've grown up around it and how do you install that kind of I mean it is humbleness in a way right how do you install that in an organization you know at the end of the day you know humbleness is an interesting word right I I always say that you you need confidence um and uh humility uh and not hubris right because there is sometimes confidence with humility can allow you to really make great progress but confidence that translates into hubris and bring you know it's the downfall of you know civilizations Empires and IND indviduals right and from ancient Greece to Modern Silicon Valley and so that's why I think uh you have to sort of really get that calibration that you got to have some confidence in your own capability you said in the podcast with um with our common friend Adam grant that uh your father he had a list of people he met and a list of ideas generated have you got a list of people you want to meet yeah yeah so I he had this you know he had this note in his diary were full of that schema uh which is uh people met uh ideas generated and tasks completed which I love which is a beautiful way each day to keep account of uh and absolutely uh so that's sort of like I I took that uh to heart and that's essentially how that's my framework for life as,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 016,189,191
169,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Poetry,2058,2151,essentially how that's my framework for life as well another thing that makes you stand out is you know there is a saying most people ignore most poetry because most poetry ignores most people that's clearly clearly not the case for you tell me about your love for poetry I love poetry you know F you know the because in an interesting way I got into poetry very late uh my mom was a a professor of Sanskrit drama and um and um and so she really instilled in me or at least tried to instill in me the love for um you know poetry and in her case you know Sanskrit uh literature and poetry and what have you but I think of it as compression it's the best like when you think about code uh is as I coded more is when I sort of sort of felt like wow poetry is basically natural language compression um and it is able to describe you know it's it's a model of the world in the most uh succinct form and so there's um and I I got into uru poetry in a big way in my mid-30s and so I grew up in Hyderabad where obviously Udu poetry was in the air um and now of course I love it but even you know the American poets uh the English Romantics Germans they're F I mean like so I I'm at least I'm not I I wouldn't say I know much poetry uh but I at least I'm fascinated by the ability of the human mind to compress uh thought whether it's code or poetry well that's fantastic,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 017,190,192
170,Norges Bank Investment Management,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management,Advice to young people,2151,2255,mind to compress uh thought whether it's code or poetry well that's fantastic last question um SAA we have um tens of thousands of young people listening to this what is your best advice to young people I'd say the best you know advice for anyone starting out in I oh you know some sort of advice I got uh which I paraphrase as never wait for your next job to do your best work right which is one of the things is any job you get like I don't remember ever at Microsoft feeling like oh I have to get a promotion in order to feel more satisfied or more fulfilled because I Som felt I had gotten the lottery and I was in the best job I could ever be in and I'm not saying you shouldn't have ambition you shouldn't strive for your next promotion you shouldn't put advocate for yourself or have others you absolutely should do all that but at the same time really my advice would be also to take the job you have at hand and do an un wonderful you know go at it with uh all of you the wiger and all of the energy and also Define it as broadly as possible right I mean that is perhaps one of the things when I look back at it I never defined my job narrowly uh and that I think was both very satisfying in the moment and it helped I think land me the next job and so that that that is my perhaps the ad one advice I would leave people with well I cannot think of anybody who is doing a better job than you so uh big thanks for being in the show good luck with everything and uh you know all the best thank you so much n such a [Music] pleasure,Satya Nadella - CEO of Microsoft | In Good Company | Podcast | Norges Bank Investment Management - 018,191,
171,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Introduction,0,138,"- I see the danger of this
concentration of power through proprietary AI systems as a much bigger danger
than everything else. What works against this is people who think that
for reasons of security, we should keep AI systems
under lock and key because it's too dangerous to put it in the hands of everybody. That would lead to a very bad future in which all of our information diet is controlled by a small
number of companies through proprietary systems. - I believe that people
are fundamentally good and so if AI, especially open source AI can make them smarter, it just empowers the goodness in humans. - So I share that feeling. Okay? I think people are
fundamentally good. (laughing) And in fact a lot of doomers are doomers because they don't think that
people are fundamentally good. - The following is a
conversation with Yann LeCun, his third time on this podcast. He is the chief AI scientist at Meta, professor at NYU, Turing Award winner and one of the seminal figures in the history of artificial intelligence. He and Meta AI have been big proponents of
open sourcing AI development, and have been walking the walk by open sourcing many
of their biggest models, including LLaMA 2 and eventually LLaMA 3. Also, Yann has been an outspoken critic of those people in the AI community who warn about the looming danger and existential threat of AGI. He believes the AGI
will be created one day, but it will be good. It will not escape human control nor will it dominate and kill all humans. At this moment of rapid AI development, this happens to be somewhat
a controversial position. And so it's been fun seeing Yann get into a lot of intense and fascinating discussions online as we do in this very conversation. This is the Lex Fridman podcast. To support it, please check out our
sponsors in the description. And now, dear friends, here's Yann LeCun.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 001",,42
172,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Limits of LLMs,138,834,"You've had some strong statements, technical statements about the future of artificial
intelligence recently, throughout your career
actually but recently as well. You've said that autoregressive LLMs are not the way we're
going to make progress towards superhuman intelligence. These are the large language models like GPT-4, like LLaMA
2 and 3 soon and so on. How do they work and why are they not going
to take us all the way? - For a number of reasons. The first is that there is
a number of characteristics of intelligent behavior. For example, the capacity
to understand the world, understand the physical world, the ability to remember
and retrieve things, persistent memory, the ability to reason
and the ability to plan. Those are four essential characteristic of intelligent systems or entities, humans, animals. LLMs can do none of those, or they can only do them
in a very primitive way. And they don't really
understand the physical world, they don't really have persistent memory, they can't really reason and they certainly can't plan. And so if you expect the
system to become intelligent just without having the
possibility of doing those things, you're making a mistake. That is not to say that
autoregressive LLMs are not useful, they're certainly useful. That they're not interesting, that we can't build a whole ecosystem of
applications around them, of course we can. But as a path towards
human level intelligence, they're missing essential components. And then there is another tidbit or fact that I think is very interesting; those LLMs are trained on
enormous amounts of text. Basically the entirety of all publicly available
text on the internet, right? That's typically on the
order of 10 to the 13 tokens. Each token is typically two bytes. So that's two 10 to the
13 bytes as training data. It would take you or me 170,000 years to just read through this at
eight hours a day. (laughs) So it seems like an enormous
amount of knowledge, right? That those systems can accumulate. But then you realize it's
really not that much data. If you talk to
developmental psychologists, and they tell you a 4-year-old has been awake for 16,000
hours in his or her life, and the amount of information that has reached the
visual cortex of that child in four years is about 10 to 15 bytes. And you can compute this by estimating that the optical nerve carry about 20 megabytes
per second, roughly. And so 10 to the 15 bytes for a 4-year-old versus two times 10 to the 13 bytes for 170,000 years worth of reading. What that tells you is
that through sensory input, we see a lot more information than we do through language. And that despite our intuition, most of what we learn
and most of our knowledge is through our observation and interaction with the real world, not through language. Everything that we learn in
the first few years of life, and certainly everything
that animals learn has nothing to do with language. - So it would be good to maybe push against
some of the intuition behind what you're saying. So it is true there's
several orders of magnitude more data coming into the
human mind, much faster, and the human mind is able to
learn very quickly from that, filter the data very quickly. Somebody might argue your comparison between
sensory data versus language. That language is already very compressed. It already contains a lot more information than the bytes it takes to store them, if you compare it to visual data. So there's a lot of wisdom in language. There's words and the way
we stitch them together, it already contains a lot of information. So is it possible that language alone already has enough wisdom
and knowledge in there to be able to, from that
language construct a world model and understanding of the world, an understanding of the physical world that you're saying LLMs lack? - So it's a big debate among philosophers and also cognitive scientists, like whether intelligence needs
to be grounded in reality. I'm clearly in the camp that yes, intelligence cannot appear without some grounding in some reality. It doesn't need to be physical reality, it could be simulated but the environment is just much richer than what you can express in language. Language is a very approximate
representation or percepts and or mental models, right? I mean, there's a lot of
tasks that we accomplish where we manipulate a mental
model of the situation at hand, and that has nothing to do with language. Everything that's physical,
mechanical, whatever, when we build something, when we accomplish a task, a moderate task of grabbing
something, et cetera, we plan our action sequences, and we do this by essentially imagining the result of the outcome of sequence of
actions that we might imagine. And that requires mental models that don't have much to do with language. And that's, I would argue, most of our knowledge is derived from that interaction
with the physical world. So a lot of my colleagues who are more interested in
things like computer vision are really on that camp that AI needs to be embodied, essentially. And then other people
coming from the NLP side or maybe some other motivation don't necessarily agree with that. And philosophers are split as well. And the complexity of the
world is hard to imagine. It's hard to represent
all the complexities that we take completely for
granted in the real world that we don't even imagine
require intelligence, right? This is the old Moravec's paradox from the pioneer of
robotics, Hans Moravec, who said, how is it that with computers, it seems to be easy to do
high level complex tasks like playing chess and solving integrals and doing things like that, whereas the thing we take for
granted that we do every day, like, I don't know,
learning to drive a car or grabbing an object, we can't do with computers. (laughs) And we have LLMs that
can pass the bar exam, so they must be smart. But then they can't
launch a drive in 20 hours like any 17-year-old. They can't learn to clear
out the dinner table and fill out the dishwasher like any 10-year-old
can learn in one shot. Why is that? Like what are we missing? What type of learning or reasoning architecture
or whatever are we missing that basically prevent us from having level five self-driving cars and domestic robots? - Can a large language model
construct a world model that does know how to drive and does know how to fill a dishwasher, but just doesn't know how to deal with visual data at this time? So it can operate in a space of concepts. - So yeah, that's what a lot
of people are working on. So the answer, the short answer is no. And the more complex answer is you can use all kind of tricks to get an LLM to basically
digest visual representations of images or video or
audio for that matter. And a classical way of doing this is you train a vision system in some way, and we have a number of ways
to train vision systems, either supervised,
unsupervised, self-supervised, all kinds of different ways. That will turn any image into
a high level representation. Basically, a list of tokens that are really similar
to the kind of tokens that a typical LLM takes as an input. And then you just feed that to the LLM in addition to the text, and you just expect
the LLM during training to kind of be able to
use those representations to help make decisions. I mean, there's been
work along those lines for quite a long time. And now you see those systems, right? I mean, there are LLMs that
have some vision extension. But they're basically hacks in the sense that those things are not like trained to handle, to really understand the world. They're not trained
with video, for example. They don't really understand
intuitive physics, at least not at the moment. - So you don't think there's something special to
you about intuitive physics, about sort of common sense reasoning about the physical space,
about physical reality? That to you is a giant leap that LLMs are just not able to do? - We're not gonna be able to do this with the type of LLMs that
we are working with today. And there's a number of reasons for this, but the main reason is the way LLMs are trained is
that you take a piece of text, you remove some of the words
in that text, you mask them, you replace them by black markers, and you train a gigantic neural net to predict the words that are missing. And if you build this neural
net in a particular way so that it can only look at words that are to the left of the
one it's trying to predict, then what you have is a system that basically is trying to predict the next word in a text, right? So then you can feed it a text, a prompt, and you can ask it to
predict the next word. It can never predict
the next word exactly. And so what it's gonna do is produce a probability distribution of all the possible
words in the dictionary. In fact, it doesn't predict words, it predicts tokens that
are kind of subword units. And so it's easy to handle the uncertainty in the prediction there because there's only a finite number of possible words in the dictionary, and you can just compute
a distribution over them. Then what the system does is that it picks a word
from that distribution. Of course, there's a higher
chance of picking words that have a higher probability
within that distribution. So you sample from that distribution to actually produce a word, and then you shift that
word into the input. And so that allows the system now to predict the second word, right? And once you do this, you shift it into the input, et cetera. That's called autoregressive prediction, which is why those LLMs should be called autoregressive LLMs, but we just call them at LLMs. And there is a difference
between this kind of process and a process by which
before producing a word,","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 002",41,43
173,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Bilingualism and thinking,834,1066,"when you talk. When you and I talk, you and I are bilinguals. We think about what we're gonna say, and it's relatively independent of the language in which we're gonna say. When we talk about like, I don't know, let's say a mathematical
concept or something. The kind of thinking that we're doing and the answer that
we're planning to produce is not linked to whether
we're gonna say it in French or Russian or English. - Chomsky just rolled his
eyes, but I understand. So you're saying that
there's a bigger abstraction that goes before language- - [Yann] Yeah.
- And maps onto language. - Right. It's certainly true for a
lot of thinking that we do. - Is that obvious that we don't? Like you're saying your
thinking is same in French as it is in English? - Yeah, pretty much. - Pretty much or is this... Like how flexible are you, like if there's a
probability distribution? (both laugh) - Well, it depends what
kind of thinking, right? If it's like producing puns, I get much better in French
than English about that (laughs) or much worse- - Is there an abstract
representation of puns? Like is your humor an abstract... Like when you tweet and your tweets are
sometimes a little bit spicy, is there an abstract representation
in your brain of a tweet before it maps onto English? - There is an abstract representation of imagining the reaction
of a reader to that text. - Oh, you start with laughter and then figure out how
to make that happen? - Figure out like a
reaction you wanna cause and then figure out how to say it so that it causes that reaction. But that's like really close to language. But think about like
a mathematical concept or imagining something you
want to build out of wood or something like this, right? The kind of thinking you're doing has absolutely nothing to
do with language, really. Like it's not like you have necessarily like an internal monologue
in any particular language. You're imagining mental
models of the thing, right? I mean, if I ask you to like imagine what this water bottle will look like if I rotate it 90 degrees, that has nothing to do with language. And so clearly there is a more abstract
level of representation in which we do most of our thinking and we plan what we're gonna say if the output is uttered words as opposed to an output
being muscle actions, right? We plan our answer before we produce it. And LLMs don't do that, they just produce one
word after the other, instinctively if you want. It's a bit like the subconscious
actions where you don't... Like you're distracted. You're doing something, you're completely concentrated and someone comes to you
and asks you a question. And you kind of answer the question. You don't have time to
think about the answer, but the answer is easy so you don't need to pay attention and you sort of respond automatically. That's kind of what an LLM does, right? It doesn't think about its answer, really. It retrieves it because it's
accumulated a lot of knowledge, so it can retrieve some things, but it's going to just spit
out one token after the other without planning the answer. - But you're making it sound
just one token after the other, one token at a time generation
is bound to be simplistic. But if the world model is
sufficiently sophisticated, that one token at a time, the most likely thing it
generates as a sequence of tokens is going to be a deeply profound thing. - Okay. But then that assumes that those systems actually possess an internal world model. - So it really goes to the...","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 003",42,44
174,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Video prediction,1066,1507,"- So it really goes to the... I think the fundamental question is can you build a really
complete world model? Not complete, but one that has a deep
understanding of the world. - Yeah. So can you build this
first of all by prediction? - [Lex] Right. - And the answer is probably yes. Can you build it by predicting words? And the answer is most probably no, because language is
very poor in terms of... Or weak or low bandwidth if you want, there's just not enough information there. So building world models
means observing the world and understanding why the world
is evolving the way it is. And then the extra
component of a world model is something that can predict how the world is going to evolve as a consequence of an
action you might take, right? So one model really is, here is my idea of the state
of the world at time T, here is an action I might take. What is the predicted state of the world at time T plus one? Now, that state of the world does not need to represent
everything about the world, it just needs to represent enough that's relevant for
this planning of the action, but not necessarily all the details. Now, here is the problem. You're not going to be able to do this with generative models. So a generative model
that's trained on video, and we've tried to do this for 10 years. You take a video, show a system a piece of video and then ask you to predict
the reminder of the video. Basically predict what's gonna happen. - One frame at a time. Do the same thing as sort of
the autoregressive LLMs do, but for video. - Right. Either one frame at a time or
a group of frames at a time. But yeah, a large video
model, if you want. (laughing) The idea of doing this has been floating around for a long time. And at FAIR, some colleagues and I have been trying to do
this for about 10 years. And you can't really do the
same trick as with LLMs, because LLMs, as I said, you can't predict exactly
which word is gonna follow a sequence of words, but you can predict the
distribution of the words. Now, if you go to video, what you would have to do is predict the distribution of all possible frames in a video. And we don't really know
how to do that properly. We do not know how to
represent distributions over high dimensional continuous spaces in ways that are useful. And there lies the main issue. And the reason we can do this is because the world is incredibly more complicated and richer in terms of information than text. Text is discreet. Video is high dimensional and continuous. A lot of details in this. So if I take a video of this room, and the video is a camera panning around, there is no way I can predict everything that's gonna be
in the room as I pan around, the system cannot predict
what's gonna be in the room as the camera is panning. Maybe it's gonna predict, this is a room where there's
a light and there is a wall and things like that. It can't predict what the
painting of the wall looks like or what the texture of
the couch looks like. Certainly not the texture of the carpet. So there's no way it can
predict all those details. So the way to handle this is one way to possibly to handle this, which we've been working for a long time, is to have a model that has
what's called a latent variable. And the latent variable
is fed to a neural net, and it's supposed to represent all the information about the world that you don't perceive yet. And that you need to augment the system for the prediction to do a
good job at predicting pixels, including the fine texture
of the carpet and the couch and the painting on the wall. That has been a complete
failure, essentially. And we've tried lots of things. We tried just straight neural nets, we tried GANs, we tried VAEs, all kinds of regularized auto encoders, we tried many things. We also tried those kind of methods to learn good representations
of images or video that could then be used as input for example, an image
classification system. And that also has basically failed. Like all the systems that
attempt to predict missing parts of an image or a video from a corrupted version of it, basically. So, right, take an image or a video, corrupt it or transform it in some way, and then try to reconstruct
the complete video or image from the corrupted version. And then hope that internally, the system will develop good
representations of images that you can use for object recognition, segmentation, whatever it is. That has been essentially
a complete failure. And it works really well for text. That's the principle that
is used for LLMs, right? - So where's the failure exactly? Is it that it is very difficult to form a good representation of an image, like a good embedding of all the important
information in the image? Is it in terms of the consistency of image to image to image to
image that forms the video? If we do a highlight reel
of all the ways you failed. What's that look like? - Okay. So the reason this doesn't work is... First of all, I have to tell
you exactly what doesn't work because there is something
else that does work. So the thing that does not work is training the system to
learn representations of images by training it to reconstruct a good image from a corrupted version of it. Okay. That's what doesn't work. And we have a whole slew
of techniques for this that are variant of then
using auto encoders. Something called MAE, developed by some of
my colleagues at FAIR, masked autoencoder. So it's basically like the
LLMs or things like this where you train the
system by corrupting text, except you corrupt images. You remove patches from it and you train a gigantic
neural network to reconstruct. The features you get are not good. And you know they're not good because if you now train
the same architecture, but you train it to
supervise with label data, with textual descriptions
of images, et cetera, you do get good representations. And the performance on
recognition tasks is much better than if you do this self
supervised free training. - So the architecture is good. - The architecture is good. The architecture of the encoder is good. Okay? But the fact that you train the
system to reconstruct images does not lead it to produce long good generic features of images. - [Lex] When you train it
in a self supervised way. - Self supervised by reconstruction. - [Lex] Yeah, by reconstruction. - Okay, so what's the alternative? (both laugh) The alternative is joint embedding.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 004",43,45
175,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",JEPA (Joint-Embedding Predictive Architecture),1507,1695,"The alternative is joint embedding. - What is joint embedding? What are these architectures
that you're so excited about? - Okay, so now instead
of training a system to encode the image and then training it to
reconstruct the full image from a corrupted version, you take the full image, you take the corrupted
or transformed version, you run them both through encoders, which in general are
identical but not necessarily. And then you train a predictor
on top of those encoders to predict the representation
of the full input from the representation
of the corrupted one. Okay? So joint embedding, because you're taking the full input and the corrupted version
or transformed version, run them both through encoders so you get a joint embedding. And then you're saying can I predict the
representation of the full one from the representation
of the corrupted one? Okay? And I call this a JEPA, so that means joint embedding
predictive architecture because there's joint embedding and there is this predictor that predicts the representation of the good guy from the bad guy. And the big question is how do you train something like this? And until five years ago or six years ago, we didn't have particularly good answers for how you train those things, except for one called
contrastive learning. And the idea of contrastive learning is you take a pair of images that are, again, an image
and a corrupted version or degraded version somehow or transformed version
of the original one. And you train the predicted representation to be the same as that. If you only do this, this system collapses. It basically completely ignores the input and produces representations
that are constant. So the contrastive methods avoid this. And those things have been
around since the early '90s, I had a paper on this in 1993, is you also show pairs of images
that you know are different and then you push away the
representations from each other. So you say not only do
representations of things that we know are the same, should be the same or should be similar, but representation of things
that we know are different should be different. And that prevents the collapse, but it has some limitation. And there's a whole bunch of techniques that have appeared over
the last six, seven years that can revive this type of method. Some of them from FAIR, some of them from Google and other places. But there are limitations to
those contrastive methods. What has changed in the
last three, four years is now we have methods
that are non-contrastive. So they don't require those
negative contrastive samples of images that we know are different. You train them only with images that are different versions or different views of the same thing. And you rely on some other tweaks to prevent the system from collapsing. And we have half a dozen
different methods for this now.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 005",44,46
176,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",JEPA vs LLMs,1695,2251,"And we have half a dozen
different methods for this now. - So what is the fundamental difference between joint embedding
architectures and LLMs? So can JEPA take us to AGI? Whether we should say that
you don't like the term AGI and we'll probably argue, I think every single
time I've talked to you we've argued about the G in AGI. - [Yann] Yes. - I get it, I get it, I get it. (laughing) Well we'll probably
continue to argue about it. It's great. Because you're like French, and ami is I guess friend in French- - [Yann] Yes. - And AMI stands for advanced
machine intelligence- - [Yann] Right. - But either way, can
JEPA take us to that, towards that advanced
machine intelligence? - Well, so it's a first step. Okay? So first of all, what's the difference with generative architectures like LLMs? So LLMs or vision systems that
are trained by reconstruction generate the inputs, right? They generate the original input that is non-corrupted,
non-transformed, right? So you have to predict all the pixels. And there is a huge amount of
resources spent in the system to actually predict all those
pixels, all the details. In a JEPA, you're not trying
to predict all the pixels, you're only trying to predict an abstract representation
of the inputs, right? And that's much easier in many ways. So what the JEPA system when it's being trained is trying to do, is extract as much information
as possible from the input, but yet only extract information that is relatively easily predictable. Okay. So there's a lot of things in the world that we cannot predict. Like for example, if you
have a self driving car driving down the street or road. There may be trees around the road. And it could be a windy day, so the leaves on the
tree are kind of moving in kind of semi chaotic random ways that you can't predict and you don't care, you don't want to predict. So what you want is your encoder to basically eliminate all those details. It'll tell you there's moving leaves, but it's not gonna keep the details of exactly what's going on. And so when you do the prediction
in representation space, you're not going to have to predict every single pixel of every leaf. And that not only is a lot simpler, but also it allows the system to essentially learn an abstract
representation of the world where what can be modeled
and predicted is preserved and the rest is viewed as noise and eliminated by the encoder. So it kind of lifts the
level of abstraction of the representation. If you think about this, this is something we do
absolutely all the time. Whenever we describe a phenomenon, we describe it at a particular
level of abstraction. And we don't always describe
every natural phenomenon in terms of quantum field theory, right? That would be impossible, right? So we have multiple levels of abstraction to describe what happens in the world. Starting from quantum field theory to like atomic theory and
molecules in chemistry, materials, all the way up to kind of
concrete objects in the real world and things like that. So we can't just only model
everything at the lowest level. And that's what the idea
of JEPA is really about. Learn abstract representation
in a self supervised manner. And you can do it hierarchically as well. So that I think is an essential component of an intelligent system. And in language, we can
get away without doing this because language is already
to some level abstract and already has eliminated
a lot of information that is not predictable. And so we can get away without
doing the joint embedding, without lifting the abstraction level and by directly predicting words. - So joint embedding. It's still generative, but it's generative in this
abstract representation space. - [Yann] Yeah. - And you're saying language, we were lazy with language 'cause we already got the
abstract representation for free and now we have to zoom out, actually think about
generally intelligent systems, we have to deal with the full mess of physical of reality, of reality. And you do have to do this step of jumping from the full,
rich, detailed reality to an abstract representation
of that reality based on what you can then reason and all that kind of stuff. - Right. And the thing is those
self supervised algorithms that learn by prediction, even in representation space, they learn more concept if the input data you feed
them is more redundant. The more redundancy there is in the data, the more they're able to capture some internal structure of it. And so there, there is way more
redundancy in the structure in perceptual inputs,
sensory input like vision, than there is in text, which is not nearly as redundant. This is back to the
question you were asking a few minutes ago. Language might represent
more information really because it's already compressed, you're right about that. But that means it's also less redundant. And so self supervised
only will not work as well. - Is it possible to join the self supervised
training on visual data and self supervised
training on language data? There is a huge amount of knowledge even though you talk down about
those 10 to the 13 tokens. Those 10 to the 13 tokens represent the entirety, a large fraction of what
us humans have figured out. Both the shit talk on Reddit and the contents of all
the books and the articles and the full spectrum of
human intellectual creation. So is it possible to
join those two together? - Well, eventually, yes, but I think if we do this too early, we run the risk of being tempted to cheat. And in fact, that's what
people are doing at the moment with vision language model. We're basically cheating. We are using language as a crutch to help the deficiencies
of our vision systems to kind of learn good representations
from images and video. And the problem with this is that we might improve our
vision language system a bit, I mean our language models
by feeding them images. But we're not gonna get to the level of even the intelligence or level of understanding of the world of a cat or a dog which
doesn't have language. They don't have language and they understand the world
much better than any LLM. They can plan really complex actions and sort of imagine the
result of a bunch of actions. How do we get machines to learn that before we combine that with language? Obviously, if we combine
this with language, this is gonna be a winner, but before that we have to focus on like how do we get systems
to learn how the world works? - So this kind of joint embedding
predictive architecture, for you, that's gonna be able to learn something like common sense, something like what a cat uses to predict how to mess with
its owner most optimally by knocking over a thing. - That's the hope. In fact, the techniques we're
using are non-contrastive. So not only is the
architecture non-generative, the learning procedures we're
using are non-contrastive. We have two sets of techniques. One set is based on distillation and there's a number of methods
that use this principle. One by DeepMind called BYOL. A couple by FAIR, one called VICReg and
another one called I-JEPA. And VICReg, I should say, is not a distillation method actually, but I-JEPA and BYOL certainly are. And there's another one
also called DINO or Dino, also produced at FAIR. And the idea of those things is that you take the full
input, let's say an image. You run it through an encoder, produces a representation. And then you corrupt that
input or transform it, run it through essentially what
amounts to the same encoder with some minor differences. And then train a predictor. Sometimes a predictor is very simple, sometimes it doesn't exist. But train a predictor to
predict a representation of the first uncorrupted input
from the corrupted input. But you only train the second branch. You only train the part of the network that is fed with the corrupted input. The other network, you don't train. But since they share the same weight, when you modify the first one, it also modifies the second one. And with various tricks, you can prevent the system from collapsing with the collapse of the
type I was explaining before where the system basically
ignores the input. So that works very well.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 006",45,47
177,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",DINO and I-JEPA,2251,2331,"So that works very well. The two techniques
we've developed at FAIR, DINO and I-JEPA work really well for that. - So what kind of data
are we talking about here? - So there's several scenarios. One scenario is you take an image, you corrupt it by changing
the cropping, for example, changing the size a little bit, maybe changing the
orientation, blurring it, changing the colors, doing all kinds of horrible things to it- - But basic horrible things. - Basic horrible things that sort of degrade
the quality a little bit and change the framing, crop the image. And in some cases, in the case of I-JEPA, you don't need to do any of this, you just mask some parts of it, right? You just basically remove some regions like a big block, essentially. And then run through the encoders and train the entire system, encoder and predictor, to predict the representation
of the good one from the representation
of the corrupted one. So that's the I-JEPA. It doesn't need to know that
it's an image, for example, because the only thing it needs to know is how to do this masking. Whereas with DINO, you need to know it's an image because you need to do things like geometry transformation and blurring and things like that that
are really image specific.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 007",46,48
178,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",V-JEPA,2331,2662,"and things like that that
are really image specific. A more recent version of this
that we have is called V-JEPA. So it's basically the same idea as I-JEPA except it's applied to video. So now you take a whole video and you mask a whole chunk of it. And what we mask is actually
kind of a temporal tube. So like a whole segment
of each frame in the video over the entire video. - And that tube is like
statically positioned throughout the frames? It's literally just a straight tube? - Throughout the tube, yeah. Typically it's 16 frames or something, and we mask the same region
over the entire 16 frames. It's a different one for
every video, obviously. And then again, train that system so as to predict the
representation of the full video from the partially masked video. And that works really well. It's the first system that we have that learns good representations of video so that when you feed
those representations to a supervised classifier head, it can tell you what action
is taking place in the video with pretty good accuracy. So it's the first time we get
something of that quality. - So that's a good test that a good representation is formed. That means there's something to this. - Yeah. We also preliminary result that seem to indicate that the representation
allows our system to tell whether the video is physically possible or completely impossible because some object disappeared or an object suddenly jumped
from one location to another or changed shape or something. - So it's able to capture
some physics based constraints about the reality
represented in the video? - [Yann] Yeah. - About the appearance and
the disappearance of objects? - Yeah. That's really new. - Okay, but can this actually get us to this kind of world model that understands enough about the world to be able to drive a car? - Possibly. And this is gonna take a while before we get to that point. And there are systems
already, robotic systems, that are based on this idea. What you need for this is a slightly modified version of this where imagine that you have a video, a complete video, and what you're doing to this video is that you are either
translating it in time towards the future. So you'll only see the
beginning of the video, but you don't see the latter part of it that is in the original one. Or you just mask the second
half of the video, for example. And then you train this I-JEPA system or the type I described, to predict representation
of the full video from the shifted one. But you also feed the
predictor with an action. For example, the wheel is turned 10 degrees to the right
or something, right? So if it's a dash cam in a car and you know the angle of the wheel, you should be able to
predict to some extent what's going to happen to what you see. You're not gonna be able
to predict all the details of objects that appear
in the view, obviously, but at an abstract representation level, you can probably predict
what's gonna happen. So now what you have is an internal model that says, here is my idea of the state of the world at time T, here is an action I'm taking, here is a prediction of the state of the
world at time T plus one, T plus delta T, T plus two seconds, whatever it is. If you have a model of this type, you can use it for planning. So now you can do what LLMs cannot do, which is planning what you're gonna do so as you arrive at a particular outcome or satisfy a particular objective, right? So you can have a number
of objectives, right? I can predict that if I have
an object like this, right? And I open my hand, it's gonna fall, right? And if I push it with a
particular force on the table, it's gonna move. If I push the table itself, it's probably not gonna
move with the same force. So we have this internal model
of the world in our mind, which allows us to plan
sequences of actions to arrive at a particular goal. And so now if you have this world model, we can imagine a sequence of actions, predict what the outcome of the sequence of action is going to be, measure to what extent the final state satisfies a particular objective like moving the bottle
to the left of the table. And then plan a sequence of actions that will minimize this
objective at runtime. We're not talking about learning, we're talking about inference time, right? So this is planning, really. And in optimal control, this is a very classical thing. It's called model predictive control. You have a model of the
system you want to control that can predict the sequence of states corresponding to a sequence of commands. And you are planning
a sequence of commands so that according to your world model, the end state of the system will satisfy any objectives that you fix. This is the way rocket
trajectories have been planned since computers have been around. So since the early '60s, essentially. - So yes, for a model predictive control, but you also often talk
about hierarchical planning.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 008",47,49
179,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Hierarchical planning,2662,3040,"but you also often talk
about hierarchical planning. - [Yann] Yeah. - Can hierarchical planning
emerge from this somehow? - Well, so no. You will have to build
a specific architecture to allow for hierarchical planning. So hierarchical planning
is absolutely necessary if you want to plan complex actions. If I wanna go from, let's
say, from New York to Paris, this the example I use all the time. And I'm sitting in my office at NYU. My objective that I need to minimize is my distance to Paris. At a high level, a very abstract
representation of my location, I would have to decompose
this into two sub-goals. First one is go to the airport, second one is catch a plane to Paris. Okay. So my sub-goal is now
going to the airport. My objective function is
my distance to the airport. How do I go to the airport? Well, I have to go in the
street and hail a taxi, which you can do in New York. Okay, now I have another sub-goal. Go down on the street. Well, that means going to the elevator, going down the elevator, walk out to the street. How do I go to the elevator? I have to stand up from my chair, open the door of my office, go to the elevator, push the button. How do I get up for my chair? Like you can imagine going
down all the way down to basically what amounts to millisecond by
millisecond muscle control. Okay? And obviously you're not
going to plan your entire trip from New York to Paris in terms of millisecond by
millisecond muscle control. First, that would be incredibly expensive, but it will also be completely impossible because you don't know all the conditions of what's gonna happen. How long it's gonna take to catch a taxi or to go to the airport with traffic. I mean, you would have to know exactly the condition of everything to be able to do this planning, and you don't have the information. So you have to do this
hierarchical planning so that you can start acting and then sort of re-planning as you go. And nobody really knows
how to do this in AI. Nobody knows how to train a system to learn the appropriate
multiple levels of representation so that hierarchical planning works. - Does something like that already emerge? So like can you use an LLM, state-of-the-art LLM, to get you from New York to Paris by doing exactly the kind of detailed set of questions that you just did? Which is can you give me a
list of 10 steps I need to do to get from New York to Paris? And then for each of those steps, can you give me a list of 10 steps how I make that step happen? And for each of those steps, can you give me a list of 10 steps to make each one of those, until you're moving
your individual muscles? Maybe not. Whatever you can actually act upon using your own mind. - Right. So there's a lot of questions that are also implied by this, right? So the first thing is LLMs
will be able to answer some of those questions down to some level of abstraction. Under the condition that
they've been trained with similar scenarios
in their training set. - They would be able to
answer all of those questions. But some of them may be hallucinated, meaning non-factual. - Yeah, true. I mean they'll probably
produce some answer. Except they're not gonna be able to really kind of produce millisecond by millisecond muscle control of how you stand up
from your chair, right? But down to some level of abstraction where you can describe things by words, they might be able to give you a plan, but only under the condition
that they've been trained to produce those kind of plans, right? They're not gonna be able
to plan for situations they never encountered before. They basically are going to
have to regurgitate the template that they've been trained on. - But where, just for the
example of New York to Paris, is it gonna start getting into trouble? Like at which layer of abstraction do you think you'll start? Because like I can imagine almost every single part of that, an LLM will be able to
answer somewhat accurately, especially when you're talking
about New York and Paris, major cities. - So I mean certainly an LLM would be able to solve that problem if you fine tune it for it. - [Lex] Sure. - And so I can't say that
an LLM cannot do this, it can't do this if you train it for it, there's no question, down to a certain level where things can be
formulated in terms of words. But like if you wanna go down to like how do you climb down the stairs or just stand up from your
chair in terms of words, like you can't do it. That's one of the reasons you need experience of the physical world, which is much higher bandwidth than what you can express in words, in human language. - So everything we've been talking about on the joint embedding space, is it possible that that's what we need for like the interaction
with physical reality on the robotics front? And then just the LLMs are the
thing that sits on top of it for the bigger reasoning about like the fact that I
need to book a plane ticket and I need to know know how to
go to the websites and so on. - Sure. And a lot of plans that people know about that are relatively high
level are actually learned. Most people don't invent
the plans by themselves. We have some ability to do
this, of course, obviously, but most plans that people use are plans that have been trained on. Like they've seen other
people use those plans or they've been told
how to do things, right? That you can't invent how
you like take a person who's never heard of airplanes and tell them like, how do
you go from New York to Paris? They're probably not going to be able to kind of deconstruct the whole plan unless they've seen
examples of that before. So certainly LLMs are
gonna be able to do this. But then how you link this
from the low level of actions, that needs to be done
with things like JEPA, that basically lift the abstraction level of the representation without attempting to reconstruct every detail of the situation. That's why we need JEPAs for.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 009",48,50
180,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Autoregressive LLMs,3040,3966,"- I would love to sort of
linger on your skepticism around autoregressive LLMs. So one way I would like
to test that skepticism is everything you say makes a lot of sense, but if I apply everything
you said today and in general to like, I don't know, 10 years ago, maybe a little bit less. No, let's say three years ago. I wouldn't be able to
predict the success of LLMs. So does it make sense to you that autoregressive LLMs
are able to be so damn good? - [Yann] Yes. - Can you explain your intuition? Because if I were to take
your wisdom and intuition at face value, I would say there's no
way autoregressive LLMs one token at a time, would be able to do the kind
of things they're doing. - No, there's one thing
that autoregressive LLMs or that LLMs in general, not
just the autoregressive ones, but including the BERT
style bidirectional ones, are exploiting and its
self supervised running. And I've been a very, very strong advocate of self supervised running for many years. So those things are an incredibly
impressive demonstration that self supervised
learning actually works. The idea that started... It didn't start with BERT, but it was really kind of a
good demonstration with this. So the idea that you take a
piece of text, you corrupt it, and then you train some
gigantic neural net to reconstruct the parts that are missing. That has been an enormous... Produced an enormous amount of benefits. It allowed us to create systems
that understand language, systems that can translate hundreds of languages in any direction, systems that are multilingual. It's a single system that can be trained to
understand hundreds of languages and translate in any direction and produce summaries and then answer questions
and produce text. And then there's a special case of it, which is the autoregressive trick where you constrain the system to not elaborate a
representation of the text from looking at the entire text, but only predicting a word from the words that have come before. Right? And you do this by constraining the
architecture of the network. And that's what you can build
an autoregressive LLM from. So there was a surprise many years ago with what's called decoder only LLM. So systems of this type that are just trying to produce
words from the previous one. And the fact that when you scale them up, they tend to really kind of
understand more about language. When you train them on lots of data, you make them really big. That was kind of a surprise. And that surprise occurred
quite a while back. Like with work from Google,
Meta, OpenAI, et cetera, going back to the GPT kind of general pre-trained transformers. - You mean like GPT-2? Like there's a certain place where you start to realize scaling might actually keep
giving us an emergent benefit. - Yeah, I mean there were
work from various places, but if you want to kind of
place it in the GPT timeline, that would be around GPT-2, yeah. - Well, 'cause you said it, you're so charismatic and
you said so many words, but self supervised learning, yes. But again, the same
intuition you're applying to saying that autoregressive LLMs cannot have a deep
understanding of the world, if we just apply that same intuition, does it make sense to you that they're able to form enough of a representation in the world to be damn convincing, essentially passing the
original Turing test with flying colors. - Well, we're fooled by
their fluency, right? We just assume that if a system is fluent in manipulating language, then it has all the characteristics
of human intelligence. But that impression is false. We're really fooled by it. - Well, what do you think
Alan Turing would say? Without understanding anything, just hanging out with it- - Alan Turing would decide that a Turing test is a really bad test. (Lex chuckles) Okay. This is what the AI community
has decided many years ago that the Turing test was a
really bad test of intelligence. - What would Hans Moravec say about the large language models? - Hans Moravec would say the Moravec's paradox still applies. - [Lex] Okay. - Okay? Okay, we can pass- - You don't think he
would be really impressed. - No, of course everybody
would be impressed. (laughs) But it is not a question
of being impressed or not, it is a question of knowing what the limit of those systems can do. Again, they are impressive. They can do a lot of useful things. There's a whole industry that
is being built around them. They're gonna make progress, but there is a lot of
things they cannot do. And we have to realize what they cannot do and then figure out how we get there. And I'm not saying this... I'm saying this from
basically 10 years of research on the idea of self supervised running, actually that's going
back more than 10 years, but the idea of self supervised learning. So basically capturing
the internal structure of a piece of a set of inputs without training the system
for any particular task, right? Learning representations. The conference I co-founded 14 years ago is called International Conference on Learning Representations, that's the entire issue that
deep learning is dealing with. Right? And it's been my obsession
for almost 40 years now. So learning representation
is really the thing. For the longest time we could only do this
with supervised learning. And then we started working on what we used to call unsupervised learning and sort of revived the idea
of unsupervised learning in the early 2000s with
Yoshua Bengio and Jeff Hinton. Then discovered that supervised learning actually works pretty well if you can collect enough data. And so the whole idea of
unsupervised self supervision took a backseat for a bit and then I kind of tried
to revive it in a big way, starting in 2014 basically
when we started FAIR, and really pushing for
like finding new methods to do self supervised running, both for text and for images
and for video and audio. And some of that work has
been incredibly successful. I mean, the reason why we have multilingual translation system, things to do, content moderation on Meta,
for example, on Facebook that are multilingual, that understand whether piece of text is hate speech or not, or something is due to their progress using self supervised running for NLP, combining this with
transformer architectures and blah blah blah. But that's the big success
of self supervised running. We had similar success
in speech recognition, a system called Wav2Vec, which is also a joint embedding
architecture by the way, trained with contrastive learning. And that system also can produce speech recognition systems
that are multilingual with mostly unlabeled data and only need a few
minutes of labeled data to actually do speech recognition. That's amazing. We have systems now based on
those combination of ideas that can do real time translation of hundreds of languages into each other, speech to speech. - Speech to speech, even including, which is fascinating, languages that don't have written forms- - That's right.
- They're spoken only. - That's right. We don't go through text, it goes directly from speech to speech using an internal representation of kinda speech units that are discrete. But it's called Textless NLP. We used to call it this way. But yeah. I mean incredible success there. And then for 10 years we
tried to apply this idea to learning representations of images by training a system to predict videos, learning intuitive physics by training a system to predict what's gonna happen in the video. And tried and tried and failed and failed with generative models, with models that predict pixels. We could not get them to learn good representations of images, we could not get them to learn
good presentations of videos. And we tried many times, we published lots of papers on it. They kind of sort of worked,
but not really great. It started working, we abandoned this idea
of predicting every pixel and basically just doing the
joint embedding and predicting in representation space. That works. So there's ample evidence that we're not gonna be able
to learn good representations of the real world using generative model. So I'm telling people, everybody's talking about generative AI. If you're really interested
in human level AI, abandon the idea of generative AI. (Lex laughs) - Okay. But you really think it's possible to get far with joint
embedding representation? So like there's common sense reasoning and then there's high level reasoning. Like I feel like those are two... The kind of reasoning
that LLMs are able to do. Okay, let me not use the word reasoning, but the kind of stuff
that LLMs are able to do seems fundamentally different than the common sense reasoning we use to navigate the world. - [Yann] Yeah. - It seems like we're gonna need both- - Sure.
- Would you be able to get, with the joint embedding which
is a JEPA type of approach, looking at video, would
you be able to learn, let's see, well, how to get from New York to Paris, or how to understand the state
of politics in the world? (both laugh) Right? These are things where various humans generate a lot of
language and opinions on, in the space of language, but don't visually represent that in any clearly compressible way. - Right. Well, there's a lot of situations that might be difficult for a purely language
based system to know. Like, okay, you can probably
learn from reading texts, the entirety of the publicly
available text in the world that I cannot get from New York to Paris by snapping my fingers. That's not gonna work, right? - [Lex] Yes. - But there's probably
sort of more complex scenarios of this type which an LLM may never have encountered and may not be able to determine whether it's possible or not. So that link from the low
level to the high level... The thing is that the high
level that language expresses is based on the common
experience of the low level, which LLMs currently do not have. When we talk to each other, we know we have a common
experience of the world. Like a lot of it is similar. And LLMs don't have that. - But see, there it's present. You and I have a common
experience of the world in terms of the physics
of how gravity works and stuff like this. And that common knowledge of the world, I feel like is there in the language. We don't explicitly express it, but if you have a huge amount of text, you're going to get this stuff
that's between the lines. In order to form a consistent world model, you're going to have to
understand how gravity works, even if you don't have an
explicit explanation of gravity. So even though, in the case of gravity, there is explicit explanation. There's gravity in Wikipedia. But like the stuff that we think of as common sense reasoning, I feel like to generate
language correctly, you're going to have to figure that out. Now, you could say as you have, there's not enough text-
- Well, I agree. - Sorry. Okay, yeah. (laughs) You don't think so? - No, I agree with what you just said, which is that to be able to
do high level common sense... To have high level common sense, you need to have the
low level common sense to build on top of. - [Lex] Yeah. But that's not there. - That's not there in LLMs. LLMs are purely trained from text. So then the other statement you made, I would not agree with the fact that implicit
in all languages in the world is the underlying reality. There's a lot about underlying reality which is not expressed in language. - Is that obvious to you? - Yeah, totally. - So like all the conversations we have... Okay, there's the dark web, meaning whatever, the private conversations
like DMs and stuff like this, which is much, much larger
probably than what's available, what LLMs are trained on. - You don't need to communicate the stuff that is common. - But the humor, all of it. No, you do. You don't need to, but it comes through. Like if I accidentally knock this over, you'll probably make fun of me. And in the content of
the you making fun of me will be explanation of
the fact that cups fall and then gravity works in this way. And then you'll have some
very vague information about what kind of things
explode when they hit the ground. And then maybe you'll
make a joke about entropy or something like this and we will never be able
to reconstruct this again. Like, okay, you'll make
a little joke like this and there'll be trillion of other jokes. And from the jokes, you can piece together the
fact that gravity works and mugs can break and
all this kind of stuff, you don't need to see... It'll be very inefficient. It's easier for like to not knock the thing over. (laughing) - [Yann] Yeah. - But I feel like it would be there if you have enough of that data. - I just think that most of
the information of this type that we have accumulated
when we were babies is just not present in text, in any description, essentially. And the sensory data
is a much richer source for getting that kind of understanding. I mean, that's the 16,000 hours of wake time of a 4-year-old. And tend to do 15 bytes,
going through vision. Just vision, right? There is a similar bandwidth of touch and a little less through audio. And then text doesn't... Language doesn't come in
until like a year in life. And by the time you are nine years old, you've learned about gravity, you know about inertia, you know about gravity, you know there's stability, you know about the distinction between animate and inanimate objects. By 18 months, you know about like why
people want to do things and you help them if they can't. I mean there's a lot of
things that you learn mostly by observation, really not even through interaction. In the first few months of life, babies don't really have
any influence on the world. They can only observe, right? And you accumulate like a
gigantic amount of knowledge just from that. So that's what we're missing
from current AI systems.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 010",49,51
181,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",AI hallucination,3966,4290,"So that's what we're missing
from current AI systems. - I think in one of your
slides you have this nice plot that is one of the ways you
show that LLMs are limited. I wonder if you could
talk about hallucinations from your perspectives. Why hallucinations happen
from large language models, and to what degree is
that a fundamental flaw of large language models. - Right. So because of the
autoregressive prediction, every time an LLM produces
a token or a word, there is some level of
probability for that word to take you out of the
set of reasonable answers. And if you assume, which is a very strong assumption, that the probability of such error is those errors are independent across a sequence of
tokens being produced. What that means is that every
time you produce a token, the probability that you stay within the set
of correct answer decreases and it decreases exponentially. - So there's a strong, like
you said, assumption there that if there's a non-zero
probability of making a mistake, which there appears to be, then there's going to be a kind of drift. - Yeah. And that drift is exponential. It's like errors accumulate, right? So the probability that an
answer would be nonsensical increases exponentially
with the number of tokens. - Is that obvious to you by the way? Well, so mathematically speaking maybe, but like isn't there a
kind of gravitational pull towards the truth? Because on average, hopefully, the truth is well represented
in the training set. - No, it's basically a struggle against the curse of dimensionality. So the way you can correct for this is that you fine tune the system by having it produce answers for all kinds of questions
that people might come up with. And people are people, so a lot of the questions that they have are very similar to each other. So you can probably cover, you know, 80% or whatever of
questions that people will ask by collecting data. And then you fine tune the system to produce good answers
for all of those things. And it's probably gonna
be able to learn that because it's got a lot
of capacity to learn. But then there is the
enormous set of prompts that you have not covered during training. And that set is enormous. Like within the set of
all possible prompts, the proportion of prompts that
have been used for training is absolutely tiny. It's a tiny, tiny, tiny subset
of all possible prompts. And so the system will behave properly on the prompts that it's
been either trained, pre-trained or fine tuned. But then there is an
entire space of things that it cannot possibly
have been trained on because it's just the number is gigantic. So whatever training the system has been subject to produce
appropriate answers, you can break it by finding out a prompt that will be outside of the set of prompts it's been trained on or things that are similar, and then it will just
spew complete nonsense. - When you say prompt, do you mean that exact prompt or do you mean a prompt that's like, in many parts very different than... Is it that easy to ask a question or to say a thing that
hasn't been said before on the internet? - I mean, people have come up with things where like you put essentially a random sequence of
characters in a prompt and that's enough to kind of
throw the system into a mode where it's gonna answer
something completely different than it would have answered without this. So that's a way to jailbreak
the system, basically. Go outside of its conditioning, right? - So that's a very clear
demonstration of it. But of course, that goes outside of what it's designed to do, right? If you actually stitch together reasonably grammatical sentences, is it that easy to break it? - Yeah. Some people have done things like you write a sentence in English or you ask a question in English and it produces a perfectly fine answer. And then you just substitute a few words by the same word in another language, and all of a sudden the
answer is complete nonsense. - Yeah. So I guess what I'm saying is like, which fraction of prompts that
humans are likely to generate are going to break the system? - So the problem is that
there is a long tail. - [Lex] Yes. - This is an issue that a
lot of people have realized in social networks and stuff like that, which is there's a very, very long tail of things that people will ask. And you can fine tune the system for the 80% or whatever of the things that most people will ask. And then this long tail is so large that you're not gonna be
able to fine tune the system for all the conditions. And in the end, the system ends up being kind of a giant lookup
table, right? (laughing) Essentially. Which is not really what you want. You want systems that can reason, certainly that can plan.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 011",50,52
182,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Reasoning in AI,4290,5342,"certainly that can plan. So the type of reasoning
that takes place in LLM is very, very primitive. And the reason you can tell it's primitive is because the amount of computation that is spent per token
produced is constant. So if you ask a question and that question has an answer
in a given number of token, the amount of computation
devoted to computing that answer can be exactly estimated. It's the size of the prediction network with its 36 layers or 92
layers or whatever it is, multiplied by number of tokens. That's it. And so essentially, it doesn't matter if
the question being asked is simple to answer,
complicated to answer, impossible to answer because it's decided,
well, there's something. The amount of computation the system will be able to
devote to the answer is constant or is proportional to the
number of token produced in the answer, right? This is not the way we work, the way we reason is that when we are faced with a complex problem or a complex question, we spend more time trying to
solve it and answer it, right? Because it's more difficult. - There's a prediction element, there's an iterative element where you're like adjusting
your understanding of a thing by going over and over and over. There's a hierarchical elements on. Does this mean it's a
fundamental flaw of LLMs- - [Yann] Yeah. - Or does it mean that... (laughs) There's more part to that question? (laughs) Now you're just behaving like an LLM. (laughs) Immediately answering. No, that it's just the
low level world model on top of which we can then build some of these kinds of mechanisms, like you said, persistent
long-term memory or reasoning, so on. But we need that world model
that comes from language. Maybe it is not so difficult to build this kind of reasoning system on top of a well constructed world model. - Okay. Whether it's difficult or not, the near future will say, because a lot of people
are working on reasoning and planning abilities
for dialogue systems. I mean, even if we restrict
ourselves to language, just having the ability to plan your answer before you answer, in terms that are not necessarily linked with the language you're gonna
use to produce the answer. Right? So this idea of this mental model that allows you to plan
what you're gonna say before you say it. That is very important. I think there's going
to be a lot of systems over the next few years that are going to have this capability, but the blueprint of those systems will be extremely different
from autoregressive LLMs. So it's the same difference as the difference between what psychology has called
system one and system two in humans, right? So system one is the type of
task that you can accomplish without like deliberately
consciously think about how you do them. You just do them. You've done them enough that you can just do it
subconsciously, right? Without thinking about them. If you're an experienced driver, you can drive without
really thinking about it and you can talk to
someone at the same time or listen to the radio, right? If you are a very
experienced chess player, you can play against a
non-experienced chess player without really thinking either, you just recognize the
pattern and you play, right? That's system one. So all the things that
you do instinctively without really having to deliberately plan and think about it. And then there is other
tasks where you need to plan. So if you are a not too
experienced chess player or you are experienced but you play against another
experienced chess player, you think about all
kinds of options, right? You think about it for a while, right? And you're much better if you
have time to think about it than you are if you play
blitz with limited time. And so this type of deliberate planning, which uses your internal world
model, that's system two, this is what LLMs currently cannot do. How do we get them to do this, right? How do we build a system that can do this kind
of planning or reasoning that devotes more resources to complex problems
than to simple problems. And it's not going to be autoregressive prediction of tokens, it's going to be more
something akin to inference of latent variables in what used to be called
probabilistic models or graphical models and
things of that type. So basically the principle is like this. The prompt is like observed variables. And what the model does is that it's basically a measure of... It can measure to what extent an answer is a good answer for a prompt. Okay? So think of it as some
gigantic neural net, but it's got only one output. And that output is a scaler number, which is let's say zero if the answer is a good
answer for the question, and a large number if the answer is not a good
answer for the question. Imagine you had this model. If you had such a model, you could use it to produce good answers. The way you would do is produce the prompt and then search through the
space of possible answers for one that minimizes that number. That's called an energy based model. - But that energy based model would need the model
constructed by the LLM. - Well, so really what you need to do would be to not search over
possible strings of text that minimize that energy. But what you would do is do this in abstract
representation space. So in sort of the space
of abstract thoughts, you would elaborate a thought, right? Using this process of minimizing
the output of your model. Okay? Which is just a scaler. It's an optimization process, right? So now the way the system
produces its answer is through optimization by minimizing an objective
function basically, right? And this is, we're
talking about inference, we're not talking about training, right? The system has been trained already. So now we have an abstract representation of the thought of the answer, representation of the answer. We feed that to basically
an autoregressive decoder, which can be very simple, that turns this into a text
that expresses this thought. Okay? So that in my opinion is the blueprint of future data systems. They will think about their answer, plan their answer by optimization before turning it into text. And that is turning complete. - Can you explain exactly what the optimization problem there is? Like what's the objective function? Just linger on it. You kind of briefly described it, but over what space are you optimizing? - The space of representations- - Goes abstract representation. - That's right. So you have an abstract
representation inside the system. You have a prompt. The prompt goes through an encoder, produces a representation, perhaps goes through a predictor that predicts a
representation of the answer, of the proper answer. But that representation
may not be a good answer because there might be
some complicated reasoning you need to do, right? So then you have another process that takes the representation
of the answers and modifies it so as to minimize a cost function that measures to what extent the answer is a good
answer for the question. Now we sort of ignore the fact for... I mean, the issue for a moment of how you train that system to measure whether an answer
is a good answer for sure. - But suppose such a
system could be created, what's the process? This kind of search like process. - It's an optimization process. You can do this if the entire
system is differentiable, that scaler output is the result of running
through some neural net, running the answer, the representation of the
answer through some neural net. Then by gradient descent, by back propagating gradients, you can figure out like how to modify the
representation of the answers so as to minimize that. - So that's still a gradient based. - It's gradient based inference. So now you have a
representation of the answer in abstract space. Now you can turn it into text, right? And the cool thing about this is that the representation now can be optimized through gradient descent, but also is independent of the language in which you're going
to express the answer. - Right. So you're operating in the
substruct of representation. I mean this goes back
to the joint embedding. - [Yann] Right. - That it's better to
work in the space of... I don't know. Or to romanticize the notion like space of concepts versus the space of concrete
sensory information. - Right. - Okay. But can this do something like reasoning, which is what we're talking about? - Well, not really, only in a very simple way. I mean basically you can
think of those things as doing the kind of optimization
I was talking about, except they're optimizing
the discrete space which is the space of
possible sequences of tokens. And they do this optimization
in a horribly inefficient way, which is generate a lot of hypothesis and then select the best ones. And that's incredibly wasteful in terms of competition, 'cause you basically have to run your LLM for like every possible
generative sequence. And it's incredibly wasteful. So it's much better to do an optimization in continuous space where you can do gradient descent as opposed to like generate tons of things and then select the best, you just iteratively refine your answer to go towards the best, right? That's much more efficient. But you can only do this
in continuous spaces with differentiable functions. - You're talking about the reasoning, like ability to think
deeply or to reason deeply. How do you know what is an answer that's better or worse
based on deep reasoning? - Right. So then we're asking the question, of conceptually, how do you
train an energy based model? Right? So energy based model is a function with a scaler
output, just a number. You give it two inputs, X and Y, and it tells you whether Y
is compatible with X or not. X you observe, let's say it's a prompt, an
image, a video, whatever. And Y is a proposal for an answer, a continuation of video, whatever. And it tells you whether
Y is compatible with X. And the way it tells you
that Y is compatible with X is that the output of that
function would be zero if Y is compatible with X, it would be a positive number, non-zero if Y is not compatible with X. Okay. How do you train a system like this? At a completely general level, is you show it pairs of X
and Ys that are compatible, a question and the corresponding answer. And you train the parameters
of the big neural net inside to produce zero. Okay. Now that doesn't completely work because the system might decide, well, I'm just gonna
say zero for everything. So now you have to have a process to make sure that for a wrong Y, the energy will be larger than zero. And there you have two options, one is contrastive methods. So contrastive method is
you show an X and a bad Y, and you tell the system, well, give a high energy to this. Like push up the energy, right? Change the weights in the neural
net that compute the energy so that it goes up. So that's contrasting methods. The problem with this is
if the space of Y is large, the number of such contrasted samples you're gonna have to show is gigantic. But people do this. They do this when you
train a system with RLHF, basically what you're training is what's called a reward model, which is basically an objective function that tells you whether
an answer is good or bad. And that's basically exactly what this is. So we already do this to some extent. We're just not using it for inference, we're just using it for training. There is another set of methods which are non-contrastive,
and I prefer those. And those non-contrastive
method basically say, okay, the energy function needs to have low energy on
pairs of XYs that are compatible that come from your training set. How do you make sure that the energy is gonna be higher everywhere else? And the way you do this is by having a regularizer, a criterion, a term in your cost function that basically minimizes
the volume of space that can take low energy. And the precise way to do this, there's all kinds of different
specific ways to do this depending on the architecture, but that's the basic principle. So that if you push
down the energy function for particular regions in the XY space, it will automatically
go up in other places because there's only a
limited volume of space that can take low energy. Okay? By the construction of the system or by the regularizing function. - We've been talking very generally, but what is a good X and a good Y? What is a good representation of X and Y? Because we've been talking about language. And if you just take language directly, that presumably is not good, so there has to be some kind of abstract
representation of ideas. - Yeah. I mean you can do this
with language directly by just, you know, X is a text and Y is the continuation of that text. - [Lex] Yes. - Or X is a question, Y is the answer. - But you're saying
that's not gonna take it. I mean, that's going to
do what LLMs are doing. - Well, no. It depends on how the internal
structure of the system is built. If the internal structure of the system is built in such a way
that inside of the system there is a latent variable, let's called it Z, that you can manipulate so as to minimize the output energy, then that Z can be viewed as
representation of a good answer that you can translate into
a Y that is a good answer. - So this kind of system could be trained in a very similar way? - Very similar way. But you have to have this
way of preventing collapse, of ensuring that there is high energy for things you don't train it on. And currently it's very implicit in LLMs. It is done in a way that people don't realize it's being done, but it is being done. It's due to the fact that when you give a high
probability to a word, automatically you give low
probability to other words because you only have a finite amount of probability
to go around. (laughing) Right? They have to sub to one. So when you minimize the
cross entropy or whatever, when you train your LLM
to predict the next word, you are increasing the probability your system will give to the correct word, but you're also decreasing the probability it will give to the incorrect words. Now, indirectly, that gives
a low probability to... A high probability to sequences
of words that are good and low probability two
sequences of words that are bad, but it's very indirect. It's not obvious why this
actually works at all, because you're not doing
it on a joint probability of all the symbols in a sequence, you're just doing it kind of, sort of factorized that probability in terms of conditional probabilities over successive tokens. - So how do you do this for visual data? - So we've been doing this with all JEPA architectures,
basically the- - [Lex] The joint embedding? - I-JEPA. So there, the compatibility
between two things is here's an image or a video, here is a corrupted, shifted
or transformed version of that image or video or masked. Okay? And then the energy of the system is the prediction error
of the representation. The predicted representation
of the good thing versus the actual representation
of the good thing, right? So you run the corrupted
image to the system, predict the representation of
the good input uncorrupted, and then compute the prediction error. That's the energy of the system. So this system will tell you, this is a good image and
this is a corrupted version. It will give you zero energy if those two things are effectively, one of them is a corrupted
version of the other, give you a high energy if the two images are
completely different. - And hopefully that whole process gives you a really nice
compressed representation of reality, of visual reality. - And we know it does because then we use those presentations as input to a classification
system or something, and it works-
- And then that classification system
works really nicely. Okay.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 012",51,53
183,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Reinforcement learning,5342,5650,"Okay. Well, so to summarize, you recommend in a spicy way
that only Yann LeCun can, you recommend that we
abandon generative models in favor of joint embedding architectures? - [Yann] Yes. - Abandon autoregressive generation. - [Yann] Yes. - Abandon... (laughs) This feels like court testimony. Abandon probabilistic models in favor of energy based
models, as we talked about. Abandon contrastive methods in favor of regularized methods. And let me ask you about this; you've been for a while, a
critic of reinforcement learning. - [Yann] Yes. - So the last recommendation
is that we abandon RL in favor of model predictive control, as you were talking about. And only use RL when planning doesn't yield
the predicted outcome. And we use RL in that case to adjust the world model or the critic. - [Yann] Yes. - So you've mentioned RLHF, reinforcement learning
with human feedback. Why do you still hate
reinforcement learning? - [Yann] I don't hate
reinforcement learning, and I think it's-
- So it's all love? - I think it should not
be abandoned completely, but I think it's use should be minimized because it's incredibly
inefficient in terms of samples. And so the proper way to train a system is to first have it learn good representations of
the world and world models from mostly observation, maybe a little bit of interactions. - And then steer it based on that. If the representation is good, then the adjustments should be minimal. - Yeah. Now there's two things. If you've learned the world model, you can use the world model
to plan a sequence of actions to arrive at a particular objective. You don't need RL, unless the way you measure
whether you succeed might be inexact. Your idea of whether you were
gonna fall from your bike might be wrong, or whether the person
you're fighting with MMA was gonna do something and they do something else. (laughing) So there's two ways you can be wrong. Either your objective function does not reflect the actual objective function
you want to optimize, or your world model is inaccurate, right? So the prediction you were making about what was gonna happen
in the world is inaccurate. So if you want to adjust your world model while you are operating the world or your objective function, that is basically in the realm of RL. This is what RL deals with
to some extent, right? So adjust your world model. And the way to adjust your
world model, even in advance, is to explore parts of the
space with your world model, where you know that your
world model is inaccurate. That's called curiosity
basically, or play, right? When you play, you kind of explore
part of the state space that you don't want to do for real because it might be dangerous, but you can adjust your world model without killing yourself
basically. (laughs) So that's what you want to use RL for. When it comes time to
learning a particular task, you already have all the
good representations, you already have your world model, but you need to adjust it
for the situation at hand. That's when you use RL. - Why do you think RLHF works so well? This enforcement learning
with human feedback, why did it have such a
transformational effect on large language models that came before? - So what's had the
transformational effect is human feedback. There is many ways to use it and some of it is just
purely supervised, actually, it's not really reinforcement learning. - So it's the HF. (laughing) - It's the HF. And then there is various ways
to use human feedback, right? So you can ask humans to rate answers, multiple answers that are
produced by a world model. And then what you do is you
train an objective function to predict that rating. And then you can use
that objective function to predict whether an answer is good, and you can back propagate
really through this to fine tune your system so that it only produces
highly rated answers. Okay? So that's one way. So that's like in RL, that means training what's
called a reward model, right? So something that, basically your small neural net that estimates to what extent
an answer is good, right? It's very similar to the objective I was talking about earlier for planning, except now it's not used for planning, it's used for fine tuning your system. I think it would be much more efficient to use it for planning, but currently it's used to fine tune the parameters of the system. Now, there's several ways to do this. Some of them are supervised. You just ask a human person, like what is a good
answer for this, right? Then you just type the answer. I mean, there's lots of ways that those systems are being adjusted.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 013",52,54
184,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Woke AI,5650,6228,"- Now, a lot of people
have been very critical of the recently released
Google's Gemini 1.5 for essentially, in my words,
I could say super woke. Woke in the negative
connotation of that word. There is some almost hilariously
absurd things that it does, like it modifies history, like generating images of
a black George Washington or perhaps more seriously something that you commented on Twitter, which is refusing to comment
on or generate images of, or even descriptions of
Tiananmen Square or the tank men, one of the most sort of legendary
protest images in history. And of course, these
images are highly censored by the Chinese government. And therefore everybody
started asking questions of what is the process
of designing these LLMs? What is the role of censorship in these, and all that kind of stuff. So you commented on Twitter saying that open source is the answer. (laughs)
- Yeah. - Essentially. So can you explain? - I actually made that comment on just about every social network I can. (Lex laughs) And I've made that point
multiple times in various forums. Here's my point of view on this. People can complain that
AI systems are biased, and they generally are biased by the distribution of the training data that they've been trained on that reflects biases in society. And that is potentially
offensive to some people or potentially not. And some techniques to de-bias then become offensive to some people because of historical
incorrectness and things like that. And so you can ask the question. You can ask two questions. The first question is, is it possible to produce an
AI system that is not biased? And the answer is absolutely not. And it's not because of
technological challenges, although there are technological
challenges to that. It's because bias is in
the eye of the beholder. Different people may have different ideas about what constitutes
bias for a lot of things. I mean there are facts
that are indisputable, but there are a lot of opinions or things that can be expressed in different ways. And so you cannot have an unbiased system, that's just an impossibility. And so what's the answer to this? And the answer is the
same answer that we found in liberal democracy about the press. The press needs to be free and diverse. We have free speech for a good reason. It's because we don't want
all of our information to come from a unique source, 'cause that's opposite to
the whole idea of democracy and progressive ideas
and even science, right? In science, people have to
argue for different opinions. And science makes progress
when people disagree and they come up with an answer and a consensus forms, right? And it's true in all
democracies around the world. So there is a future
which is already happening where every single one of our interaction with the digital world will be mediated by AI systems, AI assistance, right? We're gonna have smart glasses. You can already buy them
from Meta, (laughing) the Ray-Ban Meta. Where you can talk to them and they are connected with an LLM and you can get answers
on any question you have. Or you can be looking at a monument and there is a camera in
the system, in the glasses, you can ask it like what can you tell me about this building or this monument? You can be looking at a
menu in a foreign language and the thing we will
translate it for you. We can do real time translation if we speak different languages. So a lot of our interactions
with the digital world are going to be mediated by those systems in the near future. Increasingly, the search
engines that we're gonna use are not gonna be search engines, they're gonna be dialogue systems that we just ask a question, and it will answer and then point you to the perhaps appropriate
reference for it. But here is the thing, we cannot afford those systems to come from a handful of companies on the west coast of the US because those systems will constitute the repository of all human knowledge. And we cannot have that be controlled by a small number of people, right? It has to be diverse for the same reason the
press has to be diverse. So how do we get a diverse
set of AI assistance? It's very expensive and difficult to train a base model, right? A base LLM at the moment. In the future might be
something different, but at the moment that's an LLM. So only a few companies
can do this properly. And if some of those
subsystems are open source, anybody can use them, anybody can fine tune them. If we put in place some systems that allows any group of people, whether they are individual citizens, groups of citizens, government organizations, NGOs, companies, whatever, to take those open source
systems, AI systems, and fine tune them for their
own purpose on their own data, there we're gonna have
a very large diversity of different AI systems that are specialized for
all of those things, right? So I'll tell you, I talked to the French
government quite a bit and the French government will not accept that the digital diet
of all their citizens be controlled by three companies on the west coast of the US. That's just not acceptable. It's a danger to democracy. Regardless of how well intentioned those companies are, right? And it's also a danger to local culture, to values, to language, right? I was talking with the
founder of Infosys in India. He's funding a project
to fine tune LLaMA 2, the open source model produced by Meta. So that LLaMA 2 speaks all 22
official languages in India. It's very important for people in India. I was talking to a
former colleague of mine, Moustapha Cisse, who used to be a scientist at FAIR, and then moved back to Africa and created a research
lab for Google in Africa and now has a new startup Kera. And what he's trying to
do is basically have LLM that speaks the local languages in Senegal so that people can have
access to medical information, 'cause they don't have access to doctors, it's a very small number of
doctors per capita in Senegal. I mean, you can't have any of this unless you have open source platforms. So with open source platforms, you can have AI systems that are not only diverse in
terms of political opinions or things of that type, but in terms of language,
culture, value systems, political opinions, technical
abilities in various domains. And you can have an industry, an ecosystem of companies that fine tune those open source systems for vertical applications
in industry, right? You have, I don't know, a
publisher has thousands of books and they want to build a system that allows a customer
to just ask a question about the content of any of their books. You need to train on their
proprietary data, right? You have a company, we have one within Meta
it's called Meta Mate. And it's basically an LLM that can answer any question about internal stuff
about about the company. Very useful. A lot of companies want this, right? A lot of companies want this
not just for their employees, but also for their customers, to take care of their customers. So the only way you're
gonna have an AI industry, the only way you're gonna have AI systems that are not uniquely biased, is if you have open source platforms on top of which any group can
build specialized systems. So the inevitable direction of history is that the vast majority of AI systems will be built on top of
open source platforms. - So that's a beautiful vision. So meaning like a company
like Meta or Google or so on, should take only minimal fine tuning steps after the building, the
foundation, pre-trained model. As few steps as possible. - Basically.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 014",53,55
185,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Open source,6228,6446,"- Basically. (Lex sighs) - Can Meta afford to do that? - No. - So I don't know if you know this, but companies are supposed
to make money somehow. And open source is like giving away... I don't know, Mark made a video, Mark Zuckerberg. A very sexy video talking
about 350,000 Nvidia H100s. The math of that is, just for the GPUs,
that's a hundred billion, plus the infrastructure
for training everything. So I'm no business guy, but how do you make money on that? So the vision you paint
is a really powerful one, but how is it possible to make money? - Okay. So you have several
business models, right? The business model that
Meta is built around is you offer a service, and the financing of that service is either through ads or
through business customers. So for example, if you have an LLM that can help a mom-and-pop pizza place by talking to their
customers through WhatsApp, and so the customers
can just order a pizza and the system will just ask them, like what topping do you want
or what size, blah blah, blah. The business will pay for that. Okay? That's a model. And otherwise, if it's a system that is on the more kind
of classical services, it can be ad supported or
there's several models. But the point is, if you have a big enough
potential customer base and you need to build that
system anyway for them, it doesn't hurt you to actually distribute it to open source. - Again, I'm no business guy, but if you release the open source model, then other people can
do the same kind of task and compete on it. Basically provide fine
tuned models for businesses, is the bet that Meta is making... By the way, I'm a huge fan of all this. But is the bet that Meta is making is like, ""we'll do a better job of it?"" - Well, no. The bet is more, we already have a huge user
base and customer base. - [Lex] Ah, right.
- Right? So it's gonna be useful to them. Whatever we offer them is gonna be useful and there is a way to
derive revenue from this. - [Lex] Sure. - And it doesn't hurt that we provide that system
or the base model, right? The foundation model in open source for others to build
applications on top of it too. If those applications turn out to be useful for our customers, we can just buy it for them. It could be that they
will improve the platform. In fact, we see this already. I mean there is literally
millions of downloads of LLaMA 2 and thousands of people
who have provided ideas about how to make it better. So this clearly accelerates progress to make the system available to sort of a wide community of people. And there is literally
thousands of businesses who are building applications with it. Meta's ability to derive
revenue from this technology is not impaired by the distribution of base models in open source.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 015",54,56
186,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",AI and ideology,6446,6598,"of base models in open source. - The fundamental criticism
that Gemini is getting is that, as you pointed
out on the west coast... Just to clarify, we're currently in the east coast, where I would suppose Meta
AI headquarters would be. (laughs) So strong words about the west coast. But I guess the issue that happens is, I think it's fair to say
that most tech people have a political affiliation
with the left wing. They lean left. And so the problem that people
are criticizing Gemini with is that in that de-biasing
process that you mentioned, that their ideological
lean becomes obvious. Is this something that could be escaped? You're saying open source is the only way? - [Yann] Yeah. - Have you witnessed this
kind of ideological lean that makes engineering difficult? - No, I don't think it has to do... I don't think the issue has to do with the political leaning of the people designing those systems. It has to do with the
acceptability or political leanings of their customer base or audience, right? So a big company cannot afford
to offend too many people. So they're going to make sure that whatever product
they put out is ""safe,"" whatever that means. And it's very possible to overdo it. And it's also very possible to... It's impossible to do it
properly for everyone. You're not going to satisfy everyone. So that's what I said before, you cannot have a system that is unbiased and is perceived as unbiased by everyone. It's gonna be, you push it in one way, one set of people are
gonna see it as biased. And then you push it the other way and another set of people
is gonna see it as biased. And then in addition to this, there's the issue of
if you push the system perhaps a little too far in one direction, it's gonna be non-factual, right? You're gonna have black Nazi soldiers in- - Yeah. So we should mention image generation of black Nazi soldiers, which is not factually accurate. - Right. And can be offensive for
some people as well, right? So it's gonna be impossible to kind of produce systems
that are unbiased for everyone. So the only solution
that I see is diversity. - And diversity in full
meaning of that word, diversity in every possible way. - [Yann] Yeah.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 016",55,57
187,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Marc Andreesen,6598,7076,"- [Yann] Yeah. - Marc Andreessen just tweeted today, let me do a TL;DR. The conclusion is only
startups and open source can avoid the issue that he's
highlighting with big tech. He's asking, can big tech actually field
generative AI products? One, ever escalating demands
from internal activists, employee mobs, crazed executives, broken boards, pressure groups, extremist regulators,
government agencies, the press, in quotes ""experts,"" and everything corrupting the output. Two, constant risk of
generating a bad answer or drawing a bad picture
or rendering a bad video. Who knows what it's going
to say or do at any moment? Three, legal exposure,
product liability, slander, election law, many other things and so on. Anything that makes Congress mad. Four, continuous attempts to tighten grip on acceptable output, degrade the model, like how good it actually is in terms of usable and
pleasant to use and effective and all that kind of stuff. And five, publicity of
bad text, images, video, actual puts those examples
into the training data for the next version. And so on. So he just highlights
how difficult this is. From all kinds of people being unhappy. He just said you can't create a system that makes everybody happy. - [Yann] Yes. - So if you're going to do
the fine tuning yourself and keep a close source, essentially the problem there is then trying to minimize
the number of people who are going to be unhappy. - [Yann] Yeah. - And you're saying like the only... That that's almost
impossible to do, right? And the better way is to do open source. - Basically, yeah. I mean Marc is right about a
number of things that he lists that indeed scare large companies. Certainly, congressional
investigations is one of them. Legal liability. Making things that get people to hurt
themselves or hurt others. Like big companies are really careful about not producing things of this type, because they have... They don't want to hurt
anyone, first of all. And then second, they wanna
preserve their business. So it's essentially impossible
for systems like this that can inevitably
formulate political opinions and opinions about various things that may be political or not, but that people may disagree about. About, you know, moral issues and things about like
questions about religion and things like that, right? Or cultural issues that people from different communities would disagree with in the first place. So there's only kind of a
relatively small number of things that people will sort of agree on, basic principles. But beyond that, if you want those systems to be useful, they will necessarily have
to offend a number of people, inevitably. - And so open source is just better- - [Yann] Diversity is better, right? - And open source enables diversity. - That's right. Open source enables diversity. - This can be a fascinating world where if it's true that
the open source world, if Meta leads the way and creates this kind of open
source foundation model world, there's going to be, like governments will have a
fine tuned model. (laughing) - [Yann] Yeah. - And then potentially, people that vote left and right will have their own model and preference to be able to choose. And it will potentially
divide us even more but that's on us humans. We get to figure out... Basically the technology enables humans to human more effectively. And all the difficult ethical
questions that humans raise we'll just leave it up
to us to figure that out. - Yeah, I mean there are
some limits to what... The same way there are
limits to free speech, there has to be some
limit to the kind of stuff that those systems might
be authorized to produce, some guardrails. So I mean, that's one thing
I've been interested in, which is in the type of architecture that we were discussing before, where the output of the system is a result of an inference
to satisfy an objective. That objective can include guardrails. And we can put guardrails
in open source systems. I mean, if we eventually have systems that are built with this blueprint, we can put guardrails in those systems that guarantee that there is sort of a
minimum set of guardrails that make the system non-dangerous
and non-toxic, et cetera. Basic things that
everybody would agree on. And then the fine tuning
that people will add or the additional guardrails
that people will add will kind of cater to their
community, whatever it is. - And yeah, the fine tuning would be more about the gray
areas of what is hate speech, what is dangerous and
all that kind of stuff. I mean, you've- - [Yann] Or different value systems. - Different value systems. But still even with the objectives of how to build a bio weapon, for example, I think something you've commented on, or at least there's a paper where a collection of researchers is trying to understand the
social impacts of these LLMs. And I guess one threshold that's nice is like does the LLM make it
any easier than a search would, like a Google search would? - Right. So the increasing number
of studies on this seems to point to the
fact that it doesn't help. So having an LLM doesn't help you design or build a bio
weapon or a chemical weapon if you already have access to
a search engine and a library. And so the sort of increased
information you get or the ease with which you get
it doesn't really help you. That's the first thing. The second thing is, it's one thing to have
a list of instructions of how to make a chemical weapon,
for example, a bio weapon. It's another thing to actually build it. And it's much harder than you might think, and then LLM will not help you with that. In fact, nobody in the world, not even like countries use bio weapons because most of the time they have no idea how to protect their own
populations against it. So it's too dangerous
actually to kind of ever use. And it's in fact banned
by international treaties. Chemical weapons is different. It's also banned by treaties, but it's the same problem. It's difficult to use in situations that doesn't
turn against the perpetrators. But we could ask Elon Musk. Like I can give you a very
precise list of instructions of how you build a rocket engine. And even if you have
a team of 50 engineers that are really experienced building it, you're still gonna have
to blow up a dozen of them before you get one that works. And it's the same with
chemical weapons or bio weapons or things like this. It requires expertise in the real world that the LLM is not gonna help you with. - And it requires even
the common sense expertise that we've been talking about, which is how to take
language based instructions and materialize them in the physical world requires a lot of knowledge
that's not in the instructions. - Yeah, exactly. A lot of biologists have
posted on this actually in response to those things saying like do you realize how hard it is to actually do the lab work? Like this is not trivial. - Yeah. And that's Hans Moravec
comes to light once again.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 017",56,58
188,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Llama 3,7076,7460,"And that's Hans Moravec
comes to light once again. Just to linger on LLaMA. Mark announced that LLaMA
3 is coming out eventually, I don't think there's a release date, but what are you most excited about? First of all, LLaMA 2
that's already out there, and maybe the future LLaMA 3, 4, 5, 6, 10, just the future of the
open source under Meta? - Well, a number of things. So there's gonna be like
various versions of LLaMA that are improvements of previous LLaMAs. Bigger, better, multimodal,
things like that. And then in future generations, systems that are capable of planning, that really understand
how the world works, maybe are trained from video
so they have some world model. Maybe capable of the type
of reasoning and planning I was talking about earlier. Like how long is that gonna take? Like when is the research that
is going in that direction going to sort of feed into
the product line, if you want, of LLaMA? I don't know, I can't tell you. And there's a few breakthroughs that we have to basically go through before we can get there. But you'll be able to monitor our progress because we publish our research, right? So last week we published the V-JEPA work, which is sort of a first step towards training systems from video. And then the next step
is gonna be world models based on kind of this type of idea, training from video. There's similar work at
DeepMind also taking place, and also at UC Berkeley
on world models and video. A lot of people are working on this. I think a lot of good ideas are appearing. My bet is that those systems
are gonna be JEPA-like, they're not gonna be generative models. And we'll see what the future will tell. There's really good work at... A gentleman called Danijar
Hafner who is now DeepMind, who's worked on kind
of models of this type that learn representations and then use them for
planning or learning tasks by reinforcement training. And a lot of work at Berkeley by Pieter Abbeel, Sergey Levine, a bunch of other people of that type. I'm collaborating with actually in the context of some
grants with my NYU hat. And then collaborations also through Meta, 'cause the lab at Berkeley is associated with Meta
in some way, with FAIR. So I think it's very exciting. I think I'm super excited about... I haven't been that excited about like the direction
of machine learning and AI since 10 years ago when FAIR was started, and before that, 30 years ago, when we were working on, sorry 35, on combination nets and the
early days of neural net. So I'm super excited because I see a path towards potentially human level intelligence with systems that can
understand the world, remember, plan, reason. There is some set of ideas
to make progress there that might have a chance of working. And I'm really excited about this. What I like is that somewhat we get onto like a good direction and perhaps succeed before my
brain turns to a white sauce or before I need to retire. (laughs) - Yeah. Yeah. Are you also excited by... Is it beautiful to you just
the amount of GPUs involved, sort of the whole training
process on this much compute? Just zooming out, just looking at earth and humans together have built these computing devices and are able to train this one brain, we then open source. (laughs) Like giving birth to
this open source brain trained on this gigantic compute system. There's just the details
of how to train on that, how to build the infrastructure
and the hardware, the cooling, all of this kind of stuff. Are you just still the
most of your excitement is in the theory aspect of it? Meaning like the software. - Well, I used to be a
hardware guy many years ago. (laughs)
- Yes, yes, that's right. - Decades ago. - Hardware has improved a little bit. Changed a little bit, yeah. - I mean, certainly scale is
necessary but not sufficient. - [Lex] Absolutely. - So we certainly need computation. I mean, we're still far
in terms of compute power from what we would need to match the compute
power of the human brain. This may occur in the next couple decades, but we're still some ways away. And certainly in terms
of power efficiency, we're really far. So a lot of progress to make in hardware. And right now a lot of
the progress is not... I mean, there's a bit coming
from Silicon technology, but a lot of it coming from
architectural innovation and quite a bit coming from
like more efficient ways of implementing the architectures
that have become popular. Basically combination of
transformers and com net, right? And so there's still some ways to go until we are going to saturate. We're gonna have to come up with like new principles,
new fabrication technology, new basic components, perhaps based on sort
of different principles than those classical digital CMOS. - Interesting. So you think in order to build AmI, ami, we potentially might need
some hardware innovation too? - Well, if we wanna make it ubiquitous, yeah, certainly. Because we're gonna have to
reduce the power consumption. A GPU today, right? Is half a kilowatt to a kilowatt. Human brain is about 25 watts. And the GPU is way below
the power of human brain. You need something like a hundred thousand or a million to match it. So we are off by a huge factor.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 018",57,59
189,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",AGI,7460,7728,"- You often say that
AGI is not coming soon. Meaning like not this year,
not the next few years, potentially farther away. What's your basic intuition behind that? - So first of all, it's
not to be an event, right? The idea somehow which is popularized by
science fiction in Hollywood that somehow somebody is
gonna discover the secret, the secret to AGI or
human level AI or AmI, whatever you wanna call it, and then turn on a machine
and then we have AGI. That's just not going to happen. It's not going to be an event. It's gonna be gradual progress. Are we gonna have systems that can learn from
video how the world works and learn good representations? Yeah. Before we get them to
the scale and performance that we observe in humans, it's gonna take quite a while. It's not gonna happen in one day. Are we gonna get systems that can have large amount
of associated memories so they can remember stuff? Yeah. But same, it's not gonna happen tomorrow. I mean, there is some basic techniques that need to be developed. We have a lot of them, but like to get this to work
together with a full system is another story. Are we gonna have systems
that can reason and plan, perhaps along the lines of
objective driven AI architectures that I described before? Yeah, but like before we
get this to work properly, it's gonna take a while. And before we get all those
things to work together. And then on top of this, have systems that can learn
like hierarchical planning, hierarchical representations, systems that can be configured for a lot of different situation at hands the way the human brain can. All of this is gonna
take at least a decade, probably much more, because there are a lot of problems that we're not seeing right now that we have not encountered. And so we don't know if
there is an easy solution within this framework. It's not just around the corner. I mean, I've been hearing
people for the last 12, 15 years claiming that AGI is
just around the corner and being systematically wrong. And I knew they were wrong
when they were saying it. I called it bullshit. (laughs) - Why do you think people
have been calling... First of all, I mean,
from the beginning of, from the birth of the term
artificial intelligence, there has been an eternal optimism that's perhaps unlike other technologies. Is it Moravec's paradox? Is it the explanation for why people are so
optimistic about AGI? - I don't think it's
just Moravec's paradox. Moravec's paradox is a consequence of realizing that the world
is not as easy as we think. So first of all, intelligence
is not a linear thing that you can measure with a scaler, with a single number. Can you say that humans are
smarter than orangutans? In some ways, yes, but in some ways orangutans
are smarter than humans in a lot of domains that allows them to survive
in the forest, (laughing) for example. - So IQ is a very limited
measure of intelligence. True intelligence is bigger than what IQ,
for example, measures. - Well, IQ can measure
approximately something for humans, but because humans kind of come in relatively kind of uniform form, right? - [Lex] Yeah. - But it only measures one type of ability that may be relevant for
some tasks, but not others. But then if you are talking
about other intelligent entities for which the basic things
that are easy to them is very different, then it doesn't mean anything. So intelligence is a collection of skills and an ability to acquire
new skills efficiently. Right? And the collection of skills that a particular
intelligent entity possess or is capable of learning quickly is different from the collection
of skills of another one. And because it's a multidimensional thing, the set of skills is a
high dimensional space, you can't measure. You cannot compare two things as to whether one is more
intelligent than the other. It's multidimensional.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 019",58,60
190,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",AI doomers,7728,8678,"- So you push back against what
are called AI doomers a lot. Can you explain their perspective and why you think they're wrong? - Okay. So AI doomers imagine all
kinds of catastrophe scenarios of how AI could escape our control and basically kill us all. (laughs) And that relies on a
whole bunch of assumptions that are mostly false. So the first assumption is that the emergence
of super intelligence could be an event. That at some point we're
going to figure out the secret and we'll turn on a machine
that is super intelligent. And because we'd never done it before, it's gonna take over the
world and kill us all. That is false. It's not gonna be an event. We're gonna have systems that
are like as smart as a cat, have all the characteristics
of human level intelligence, but their level of intelligence would be like a cat or a
parrot maybe or something. And then we're gonna walk our way up to kind of make those
things more intelligent. And as we make them more intelligent, we're also gonna put
some guardrails in them and learn how to kind
of put some guardrails so they behave properly. And we're not gonna do
this with just one... It's not gonna be one effort, but it's gonna be lots of
different people doing this. And some of them are gonna succeed at making intelligent systems
that are controllable and safe and have the right guardrails. And if some other goes rogue, then we can use the good ones
to go against the rogue ones. (laughs) So it's gonna be smart AI
police against your rogue AI. So it's not gonna be like
we're gonna be exposed to like a single rogue AI
that's gonna kill us all. That's just not happening. Now, there is another fallacy, which is the fact that because
the system is intelligent, it necessarily wants to take over. And there is several arguments that make people scared of this, which I think are
completely false as well. So one of them is in nature, it seems to be that the
more intelligent species are the ones that end
up dominating the other. And even extinguishing the others sometimes by design,
sometimes just by mistake. And so there is sort of a thinking by which you say, well, if AI systems are more intelligent than us, surely they're going to eliminate us, if not by design, simply because they don't care about us. And that's just preposterous
for a number of reasons. First reason is they're
not going to be a species. They're not gonna be a
species that competes with us. They're not gonna have
the desire to dominate because the desire to dominate is something that has to be hardwired into an intelligent system. It is hardwired in humans, it is hardwired in baboons, in chimpanzees, in wolves, not in orangutans. The species in which this
desire to dominate or submit or attain status in other ways is specific to social species. Non-social species like
orangutans don't have it. Right? And they are as smart as we are, almost. Right? - And to you, there's
not significant incentive for humans to encode
that into the AI systems. And to the degree they do, there'll be other AIs that
sort of punish them for it. Out-compete them over- - Well, there's all kinds of incentive to make AI systems submissive to humans. Right?
- [Lex] Right. - I mean, this is the way
we're gonna build them, right? And so then people say,
oh, but look at LLMs. LLMs are not controllable. And they're right, LLMs are not controllable. But objective driven AI, so systems that derive their answers by optimization of an objective means they have to
optimize this objective, and that objective can include guardrails. One guardrail is obey humans. Another guardrail is don't obey humans if it's hurting other humans- - I've heard that before
somewhere, I don't remember- - [Yann] Yes.
(Lex laughs) Maybe in a book. (laughs) - Yeah. But speaking of that book, could there be unintended
consequences also from all of this? - No, of course. So this is not a simple problem, right? I mean designing those guardrails so that the system behaves properly is not gonna be a simple issue for which there is a silver bullet, for which you have a mathematical proof that the system can be safe. It's gonna be very progressive, iterative design system where we put those guardrails in such a way that the
system behave properly. And sometimes they're
going to do something that was unexpected because
the guardrail wasn't right, and we're gonna correct them
so that they do it right. The idea somehow that we
can't get it slightly wrong, because if we get it
slightly wrong we all die, is ridiculous. We're just gonna go progressively. The analogy I've used many
times is turbojet design. How did we figure out how to make turbojets so
unbelievably reliable, right? I mean, those are like incredibly
complex pieces of hardware that run at really high temperatures for 20 hours at a time sometimes. And we can fly halfway around the world on a two engine jet liner
at near the speed of sound. Like how incredible is this? It is just unbelievable. And did we do this because we invented
like a general principle of how to make turbojets safe? No, it took decades to kind of fine tune the
design of those systems so that they were safe. Is there a separate group within General Electric
or Snecma or whatever that is specialized in turbojet safety? No. The design is all about safety. Because a better turbojet
is also a safer turbojet, a more reliable one. It's the same for AI. Like do you need specific
provisions to make AI safe? No, you need to make better AI systems and they will be safe because they are designed
to be more useful and more controllable. - So let's imagine a system, AI system that's able to
be incredibly convincing and can convince you of anything. I can at least imagine such a system. And I can see such a
system be weapon-like, because it can control people's minds, we're pretty gullible. We want to believe a thing. And you can have an AI
system that controls it and you could see governments
using that as a weapon. So do you think if you
imagine such a system, there's any parallel to
something like nuclear weapons? - [Yann] No. - So why is that technology different? So you're saying there's going
to be gradual development? - [Yann] Yeah. - I mean it might be rapid, but they'll be iterative. And then we'll be able to
kind of respond and so on. - So that AI system designed
by Vladimir Putin or whatever, or his minions (laughing) is gonna be like trying
to talk to every American to convince them to vote for- - [Lex] Whoever. - Whoever pleases Putin or whatever. Or rile people up against each other as they've been trying to do. They're not gonna be talking to you, they're gonna be talking
to your AI assistant which is going to be as
smart as theirs, right? Because as I said, in the future, every single one of your
interaction with the digital world will be mediated by your AI assistant. So the first thing you're
gonna ask is, is this a scam? Like is this thing like
telling me the truth? Like it's not even going
to be able to get to you because it's only going to
talk to your AI assistant, and your AI is not even going to... It's gonna be like a spam filter, right? You're not even seeing the
email, the spam email, right? It's automatically put in a
folder that you never see. It's gonna be the same thing. That AI system that tries to
convince you of something, it's gonna be talking to an AI system which is gonna be at least as smart as it. And is gonna say, this is spam. (laughs) It's not even going to
bring it to your attention. - So to you it's very
difficult for any one AI system to take such a big leap ahead to where it can convince
even the other AI systems? So like there's always going
to be this kind of race where nobody's way ahead? - That's the history of the world. History of the world is whenever there is a progress someplace, there is a countermeasure. And it's a cat and mouse game. - Mostly yes, but this is why nuclear
weapons are so interesting because that was such a powerful weapon that it mattered who got it first. That you could imagine Hitler, Stalin, Mao getting the weapon first and that having a different
kind of impact on the world than the United States
getting the weapon first. To you, nuclear weapons is like... You don't imagine a breakthrough discovery and then Manhattan project
like effort for AI? - No. As I said, it's not going to be an event. It's gonna be continuous progress. And whenever one breakthrough occurs, it's gonna be widely
disseminated really quickly. Probably first within industry. I mean, this is not a domain where government or military organizations are particularly innovative, and they're in fact way behind. And so this is gonna come from industry. And this kind of information
disseminates extremely quickly. We've seen this over the
last few years, right? Where you have a new... Like even take AlphaGo. This was reproduced within three months even without like particularly
detailed information, right? - Yeah. This is an industry that's
not good at secrecy. (laughs) - But even if there is, just the fact that you know
that something is possible makes you like realize that it's worth investing
the time to actually do it. You may be the second person
to do it but you'll do it. Say for all the innovations of self supervised running transformers, decoder only architectures, LLMs. I mean those things, you don't need to know exactly
the details of how they work to know that it's possible because it's deployed and
then it's getting reproduced. And then people who work
for those companies move. They go from one company to another. And the information disseminates. What makes the success
of the US tech industry and Silicon Valley in
particular, is exactly that, is because information
circulates really, really quickly and disseminates very quickly. And so the whole region sort of is ahead because of that
circulation of information. - Maybe just to linger on
the psychology of AI doomers. You give in the classic Yann LeCun way, a pretty good example of just when a new technology comes to be, you say engineer says, ""I invented this new thing,
I call it a ballpen."" And then the TwitterSphere responds, ""OMG people could write
horrible things with it like misinformation,
propaganda, hate speech. Ban it now!"" Then writing doomers come in, akin to the AI doomers, ""imagine if everyone can get a ballpen. This could destroy society. There should be a law against using ballpen
to write hate speech, regulate ballpens now."" And then the pencil industry mogul says, ""yeah, ballpens are very dangerous, unlike pencil writing which is erasable, ballpen writing stays forever. Government should require a
license for a pen manufacturer."" I mean, this does seem to
be part of human psychology when it comes up against new technology. What deep insights can
you speak to about this? - Well, there is a natural
fear of new technology and the impact it can have on society. And people have kind
of instinctive reaction to the world they know being threatened by major transformations that are either cultural phenomena or technological revolutions. And they fear for their culture, they fear for their job, they fear for the future of their children and their way of life, right? So any change is feared. And you see this along history, like any technological
revolution or cultural phenomenon was always accompanied by
groups or reaction in the media that basically attributed
all the problems, the current problems of society to that particular change, right? Electricity was going to
kill everyone at some point. The train was going to be a horrible thing because you can't breathe
past 50 kilometers an hour. And so there's a wonderful website called a Pessimists Archive, right? Which has all those
newspaper clips (laughing) of all the horrible things
people imagined would arrive because of either technological innovation or a cultural phenomenon. Wonderful examples of jazz or comic books being blamed for unemployment or young people not
wanting to work anymore and things like that, right? And that has existed for centuries. And it's knee jerk reactions. The question is do we embrace
change or do we resist it? And what are the real dangers as opposed to the imagined imagined ones? - So people worry about... I think one thing they
worry about with big tech, something we've been
talking about over and over but I think worth mentioning again, they worry about how powerful AI will be and they worry about it being in the hands of
one centralized power of just a handful of central control. And so that's the
skepticism with big tech. These companies can make
a huge amount of money and control this technology. And by so doing, take advantage, abuse the
little guy in society. - Well, that's exactly why we
need open source platforms. - Yeah. I just wanted to... (laughs) Nail the point home more and more. - [Yann] Yes.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 020",59,61
191,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Joscha Bach,8678,8931,"- So let me ask you on your... Like I said, you do get a little bit flavorful on the internet. Joscha Bach tweeted
something that you LOL'd at in reference to HAL 9,000. Quote, ""I appreciate your argument and I fully understand your frustration, but whether the pod bay doors
should be opened or closed is a complex and nuanced issue."" So you're at the head of Meta AI. This is something that really worries me, that our AI overlords will speak down to us with
corporate speak of this nature and you sort of resist that
with your way of being. Is this something you can just comment on sort of working at a big company, how you can avoid the
over fearing, I suppose, the through caution create harm? - Yeah. Again, I think the answer to
this is open source platforms and then enabling a widely
diverse set of people to build AI assistants that represent the diversity of cultures, opinions, languages, and value systems across the world. So that you're not bound
to just be brainwashed by a particular way of thinking because of a single AI entity. So I mean, I think it's a
really, really important question for society. And the problem I'm seeing, which is why I've been so vocal and sometimes a little sardonic about it- - Never stop. Never stop, Yann. (both laugh) We love it.
- Is because I see the danger of this concentration of power through proprietary AI systems as a much bigger danger
than everything else. That if we really want
diversity of opinion AI systems that in the future that we'll all be interacting
through AI systems, we need those to be diverse for the preservation
of a diversity of ideas and creeds and political
opinions and whatever, and the preservation of democracy. And what works against this is people who think that
for reasons of security, we should keep AI systems
under lock and key because it's too dangerous to put it in the hands of everybody because it could be used
by terrorists or something. That would lead to
potentially a very bad future in which all of our information diet is controlled by a small
number of companies through proprietary systems. - So you trust humans with this technology to build systems that are on
the whole good for humanity? - Isn't that what democracy
and free speech is all about? - I think so. - Do you trust institutions
to do the right thing? Do you trust people to do the right thing? And yeah, there's bad people
who are gonna do bad things, but they're not going to
have superior technology to the good people. So then it's gonna be my good
AI against your bad AI, right? I mean it's the examples that
we were just talking about of maybe some rogue country
will build some AI system that's gonna try to convince everybody to go into a civil war or something or elect a favorable ruler. But then they will have to go
past our AI systems, right? (laughs) - An AI system with a
strong Russian accent will be trying to convince our- - And doesn't put any
articles in their sentences. (both laugh) - Well, it'll be at the very
least, absurdly comedic. Okay. So since we talked about
sort of the physical reality,","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 021",60,62
192,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Humanoid robots,8931,9480,"So since we talked about
sort of the physical reality, I'd love to ask your vision
of the future with robots in this physical reality. So many of the kinds of intelligence you've been speaking about would empower robots to be more effective
collaborators with us humans. So since Tesla's Optimus team has been showing us some
progress in humanoid robots, I think it really reinvigorated
the whole industry that I think Boston
Dynamics has been leading for a very, very long time. So now there's all kinds of companies, Figure AI, obviously Boston Dynamics- - [Yann] Unitree. - Unitree. But there's like a lot of them. It's great. It's great. I mean I love it. So do you think there'll be
millions of humanoid robots walking around soon? - Not soon, but it's gonna happen. Like the next decade I think is gonna be really
interesting in robots. Like the emergence of
the robotics industry has been in the waiting for 10, 20 years, without really emerging other than for like kind
of pre-program behavior and stuff like that. And the main issue is again,
the Moravec's paradox. Like how do we get the systems to understand how the world works and kind of plan actions? And so we can do it for
really specialized tasks. And the way Boston Dynamics goes about it is basically with a lot of
handcrafted dynamical models and careful planning in advance, which is very classical robotics
with a lot of innovation, a little bit of perception, but it's still not... Like they can't build a
domestic robot, right? And we're still some distance away from completely autonomous
level five driving. And we're certainly very far away from having level five autonomous driving by a system that can train itself by driving 20 hours, like any 17-year-old. So until we have, again, world models, systems that can train themselves to understand how the world works, we're not gonna have significant
progress in robotics. So a lot of the people working on robotic hardware at the moment are betting or banking on the fact that AI is gonna make sufficient
progress towards that. - And they're hoping to
discover a product in it too- - [Yann] Yeah. - Before you have a
really strong world model, there'll be an almost strong world model. And people are trying to find a product in a clumsy robot, I suppose. Like not a perfectly efficient robot. So there's the factory setting where humanoid robots can help automate some
aspects of the factory. I think that's a crazy difficult task 'cause of all the safety required and all this kind of stuff, I think in the home is more interesting. But then you start to think... I think you mentioned loading
the dishwasher, right? - [Yann] Yeah. - Like I suppose that's
one of the main problems you're working on. - I mean there's cleaning up. (laughs) - [Lex] Yeah. - Cleaning the house, clearing up the table after a meal, washing the dishes, all
those tasks, cooking. I mean all the tasks that in
principle could be automated but are actually incredibly sophisticated, really complicated. - But even just basic navigation around a space full of uncertainty. - That sort of works. Like you can sort of do this now. Navigation is fine. - Well, navigation in a way
that's compelling to us humans is a different thing. - Yeah. It's not gonna be necessarily... I mean we have demos actually 'cause there is a so-called
embodied AI group at FAIR and they've been not
building their own robots but using commercial robots. And you can tell the robot
dog like go to the fridge and they can actually open the fridge and they can probably pick
up a can in the fridge and stuff like that and bring it to you. So it can navigate, it can grab objects as long as it's been
trained to recognize them, which vision systems work
pretty well nowadays. But it's not like a
completely general robot that would be sophisticated enough to do things like clearing
up the dinner table. (laughs) - Yeah, to me that's an exciting future of getting humanoid robots. Robots in general in
the home more and more because it gets humans to really directly
interact with AI systems in the physical space. And in so doing it allows us to philosophically,
psychologically explore our relationships with robots. It can be really, really interesting. So I hope you make progress
on the whole JEPA thing soon. (laughs) - Well, I mean, I hope
things can work as planned. I mean, again, we've been like
kinda working on this idea of self supervised learning
from video for 10 years. And only made significant
progress in the last two or three. - And actually you've mentioned that there's a lot of
interesting breakthroughs that can happen without having
access to a lot of compute. So if you're interested in doing a PhD in this kind of stuff, there's a lot of possibilities still to do innovative work. So like what advice would you give to a undergrad that's
looking to go to grad school and do a PhD? - So basically, I've listed them already. This idea of how do you train
a world model by observation? And you don't have to train necessarily on gigantic data sets. I mean, it could turn out to be necessary to actually train on large data sets to have emergent properties
like we have with LLMs. But I think there is a lot of
good ideas that can be done without necessarily scaling up. Then there is how do you do planning with a learn world model? If the world the system evolves in is not the physical world, but is the world of let's say the internet or some sort of world of where an action consists in doing a search in a search engine or interrogating a database, or running a simulation or calling a calculator or solving a differential equation, how do you get a system to actually plan a sequence of actions to give the solution to a problem? And so the question of planning is not just a question of
planning physical actions, it could be planning actions to use tools for a dialogue system or for any kind of intelligence system. And there's some work on
this but not a huge amount. Some work at FAIR, one called Toolformer,
which was a couple years ago and some more recent work on planning, but I don't think we
have like a good solution for any of that. Then there is the question
of hierarchical planning. So the example I mentioned of planning a trip from New York to Paris, that's hierarchical, but almost every action that we take involves hierarchical
planning in some sense. And we really have absolutely
no idea how to do this. Like there's zero demonstration of hierarchical planning in AI, where the various levels
of representations that are necessary have been learned. We can do like two level
hierarchical planning when we design the two levels. So for example, you have like
a dog legged robot, right? You want it to go from the
living room to the kitchen. You can plan a path that
avoids the obstacle. And then you can send this
to a lower level planner that figures out how to move the legs to kind of follow that
trajectories, right? So that works, but that two level planning
is designed by hand, right? We specify what the proper
levels of abstraction, the representation at each
level of abstraction have to be. How do you learn this? How do you learn that
hierarchical representation of action plans, right? With com nets and deep learning, we can train the system to learn hierarchical
representations of percepts. What is the equivalent when what you're trying to
represent are action plans? - For action plans. Yeah. So you want basically a
robot dog or humanoid robot that turns on and travels
from New York to Paris all by itself. - [Yann] For example. - All right. It might have some trouble at the TSA but- - No, but even doing
something fairly simple like a household task. - [Lex] Sure. - Like cooking or something. - Yeah. There's a lot involved. It's a super complex task. Once again, we take it for granted.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 022",61,63
193,Lex Fridman,"Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416",Hope for the future,9480,10037,"What hope do you have for
the future of humanity? We're talking about so
many exciting technologies, so many exciting possibilities. What gives you hope when you look out over the next 10, 20, 50, 100 years? If you look at social media, there's wars going on, there's
division, there's hatred, all this kind of stuff
that's also part of humanity. But amidst all that, what gives you hope? - I love that question. We can make humanity smarter with AI. Okay? I mean AI basically will
amplify human intelligence. It's as if every one of us will have a staff of smart AI assistants. They might be smarter than us. They'll do our bidding, perhaps execute a task in ways that are much better
than we could do ourselves because they'd be smarter than us. And so it's like everyone
would be the boss of a staff of super smart virtual people. So we shouldn't feel threatened by this any more than we should feel threatened by being the manager of a group of people, some of whom are more intelligent than us. I certainly have a lot
of experience with this. (laughs) Of having people working with
me who are smarter than me. That's actually a wonderful thing. So having machines that
are smarter than us, that assist us in all of
our tasks, our daily lives, whether it's professional or personal, I think would be an
absolutely wonderful thing. Because intelligence is the commodity that is most in demand. I mean, all the mistakes
that humanity makes is because of lack of
intelligence, really, or lack of knowledge, which is related. So making people smarter
which can only be better. I mean, for the same reason that public education is a good thing and books are a good thing, and the internet is also a
good thing, intrinsically. And even social networks are a good thing if you run them properly. (laughs) It's difficult, but you can. Because it helps the communication of information and knowledge and the transmission of knowledge. So AI is gonna make humanity smarter. And the analogy I've been using is the fact that perhaps
an equivalent event in the history of humanity to what might be provided by
generalization of AI assistant is the invention of the printing press. It made everybody smarter. The fact that people could
have access to books. Books were a lot cheaper
than they were before. And so a lot more people had
an incentive to learn to read, which wasn't the case before. And people became smarter. It enabled the enlightenment, right? There wouldn't be an enlightenment without the printing press. It enabled philosophy, rationalism, escape from religious doctrine, democracy, science. And certainly without this there wouldn't have been
the American Revolution or the French Revolution. And so we'll still be under
feudal regimes perhaps. And so it completely transformed the world because people became smarter and kinda learned about things. Now, it also created 200 years of essentially religious
conflicts in Europe, right? Because the first thing that
people read was the Bible and realized that perhaps there was a different
interpretation of the Bible than what the priests were telling them. And so that created
the Protestant movement and created a rift. And in fact, the Catholic church didn't like the idea of the printing press but they had no choice. And so it had some bad
effects and some good effects. I don't think anyone today would say that the invention
of the printing press had an overall negative effect despite the fact that it created 200 years of religious conflicts in Europe. Now compare this, and I was very proud of myself to come up with this analogy, but realized someone else came
with the same idea before me. Compare this with what
happened in the Ottoman Empire. The Ottoman Empire banned the
printing press for 200 years. And it didn't ban it for all languages, only for Arabic. You could actually print books in Latin or Hebrew or whatever
in the Ottoman Empire, just not in Arabic. And I thought it was because the rulers just wanted to preserve the control over the
population and the dogma, religious dogma and everything. But after talking with
the UAE Minister of AI, Omar Al Olama, he told me no, there was another reason. And the other reason was that it was to preserve the corporation
of calligraphers, right? There's like an art form which is writing those
beautiful Arabic poems or whatever religious text in this thing. And it was very powerful
corporation of scribes basically that kinda run a big chunk of the empire. And we couldn't put them out of business. So they banned the bridging press in part to protect that business. Now, what's the analogy for AI today? Like who are we protecting by banning AI? Like who are the people who
are asking that AI be regulated to protect their jobs? And of course, it's a real question of what is gonna be the effect of technological transformation like AI on the job market and the labor market? And there are economists who are much more expert
at this than I am, but when I talk to them, they tell us we're not
gonna run out of job. This is not gonna cause mass unemployment. This is just gonna be gradual shift of different professions. The professions that are gonna be hot 10 or 15 years from now, we have no idea today
what they're gonna be. The same way if we go
back 20 years in the past, like who could have thought 20 years ago that like the hottest job, even like 5, 10 years ago
was mobile app developer? Like smartphones weren't invented. - Most of the jobs of the future might be in the Metaverse. (laughs) - Well, it could be. Yeah. - But the point is you
can't possibly predict. But you're right. I mean, you've made a
lot of strong points. And I believe that people
are fundamentally good, and so if AI, especially open source AI can make them smarter, it just empowers the goodness in humans. - So I share that feeling. Okay? I think people are
fundamentally good. (laughing) And in fact a lot of doomers are doomers because they don't think that
people are fundamentally good. And they either don't trust people or they don't trust the
institution to do the right thing so that people behave properly. - Well, I think both you
and I believe in humanity, and I think I speak for a lot of people in saying thank you for pushing
the open source movement, pushing to making both
research and AI open source, making it available to people, and also the models themselves, making that open source also. So thank you for that. And thank you for speaking your mind in such colorful and beautiful
ways on the internet. I hope you never stop. You're one of the most fun people I know and get to be a fan of. So Yann, thank you for
speaking to me once again, and thank you for being you. - Thank you Lex. - Thanks for listening to this
conversation with Yann LeCun. To support this podcast, please check out our
sponsors in the description. And now let me leave you with some words from Arthur C. Clarke, ""the only way to discover
the limits of the possible is to go beyond them into the impossible."" Thank you for listening and
hope to see you next time.","Yann Lecun: Meta AI, Open Source, Limits of LLMs, AGI & the Future of AI | Lex Fridman Podcast #416 - 023",62,