[[0]]hi listeners and welcome to another
[[3.56]]episode of no priors today we're talking
[[7.48]]to Harrison chase the CEO and co-founder
[[11.48]]of linkchain a popular open source
[[15.44]]framework and developer toolkit that
[[20.0]]helps people build llm applications
[[24.56]]we're excited to talk to Harrison about
[[28.52]]the state of AI application development
[[32.48]]the open source ecosystem and its open
[[36.88]]questions welcome Harrison thanks for
[[41.32]]having me I'm excited to be here Lang
[[46.0]]Chain's a a really unique story and it
[[50.84]]started actually as a personal project
[[55.04]]for you can you talk a little bit about
[[58.84]]what what Lang chain is and what it was
[[63.68]]originally yeah absolutely so how how
[[68.2]]would answer the question what Lang
[[71.52]]chain is has kind of evolved over time
[[75.84]]as as the entire landscape Lang chain
[[81.36]]the open source uh package started yeah
[[86.36]]as a side project um so so my
[[90.76]]backgrounds in ml and mlops I was at I
[[95.16]]was at my previous company I I knew I
[[98.36]]was going to leave I didn't know what I
[[102.4]]was going to do this was in September
[[108.0]]October of 2022 um and so went to a
[[112.16]]bunch of hackathons went bunch of
[[114.96]]meetups chatted with folks that were
[[119.88]]playing around with llms um and saw some
[[125.52]]Comet abstractions put it in a python
[[129.8]]project as a just fun side project
[[133.76]]turned out to strike a chord be
[[137.44]]fantastic timing you know chat GPD came
[[141.16]]out like a month later um and it's kind
[[146.08]]of evolved from there so right now
[[151.44]]linkchain the company um there's really
[[155.52]]two main products that we have one is
[[159.4]]the Lang chain open source packages and
[[163.76]]happy to dive into that more and then
[[168.24]]the other is Lang Smith a platform for
[[173.84]]for testing evaluation monitoring and
[[179.2]]and all of those types of things and so
[[183.36]]you know what Lang chain is has evolved
[[188.2]]over time as yeah the company's grown
[[193.16]]one thing that we talked about the last
[[197.04]]time we saw each other in person was
[[202.32]]just how quickly like the AI um
[[209.44]]ecosystem and research field is evolving
[[214.48]]and what it means to manage an open
[[218.2]]source project through that can you talk
[[222.12]]a little bit about what you decide to
[[225.76]]keep stable and change when you both
[[230.4]]have like big ecosystem of users now and
[[235.0]]like very rapidly changing environment
[[238.72]]of applications and Technology that's
[[242.88]]been a fun exercise so I mean if we go
[[247.4]]back to the original version of Lang
[[251.6]]chain what it was when it came out was
[[256.16]]essentially three kind of like highlevel
[[260.32]]implementations two were based on
[[263.8]]research papers and then one was based
[[268.4]]on natat fredman's like gbot type of
[[273.32]]agent web crawler thing and so there was
[[277.12]]some high level kind of like
[[280.52]]abstractions and then there was a few
[[284.12]]like Integrations so we had Integrations
[[287.76]]with I think like open Ai cohere and
[[291.48]]hugging face to start or something like
[[295.12]]that and those two layers have kind of
[[298.84]]like remained so we have you know 700
[[302.56]]different Integrations we have a bunch
[[306.32]]of kind of like higher level chains and
[[310.72]]agents for for doing particular things I
[[314.44]]think the thing that we've put a lot of
[[318.24]]emphasis in um to your point around kind
[[322.08]]of like what's remained constant and
[[325.52]]what's remains uh and what's changed is
[[329.32]]like a lower level kind of like
[[333.04]]abstraction and runtime for for joining
[[336.96]]these things together one of the things
[[341.04]]that we pretty quickly saw was that as
[[346.08]]people wanted to improve the performance
[[350.36]]go from prototype to production they
[[354.24]]wanted to customize a lot of these bits
[[359.04]]and so we've invested a lot in uh a
[[363.16]]lower level kind of like chaining
[[366.64]]protocol so laying chain expression
[[370.92]]language and then in in a different
[[375.4]]protocol laying graph which is one
[[378.6]]something we're really excited about and
[[382.84]]that's more aimed at uh basically graphs
[[387.68]]that are not dags so you know all these
[[391.56]]agents are basically running an llm in a
[[395.84]]loop you need Cycles um and so Lane
[[399.96]]graph helps with that and so I think
[[404.88]]what we've kind of seen is underlying
[[411.12]]bits of um there's all these different
[[416.48]]Integrations and like you know there's
[[421.92]]there's LMS Vector stores and sometimes
[[426.76]]they change right when chat models came
[[430.16]]out like that was a that's a very big
[[433.68]]change in the API interface and so we
[[437.4]]had to add a new abstraction for that
[[441.68]]but those have especially over the past
[[446.2]]few months remained relatively stable um
[[450.36]]we've invested a lot in this underlying
[[455.52]]runtime which emphasizes a few things uh
[[461.2]]streaming structured outputs and and the
[[465.12]]importance of those has remained
[[468.32]]relatively stable but then the way that
[[472.32]]you put things together and the kind of
[[477.36]]like patterns um for building things has
[[482.28]]definitely evolved over time from like
[[486.88]]simple chains to complex chains to then
[[491.6]]these kind of like autonomous agents to
[[496.28]]now something um maybe in the middle of
[[501.2]]like complex State machines or graphs or
[[505.2]]something and so it's really that upper
[[508.68]]layer which is like the common ways to
[[512.4]]put things together that I think we've
[[516.48]]seen the most rapid kind of like churn
[[520.32]]what do you think is still missing from
[[524.08]]uh really getting to performing agents
[[527.44]]there's a number of companies that have
[[529.8]]been started recently that are really
[[533.28]]focused on sort of the agentic world and
[[537.12]]pushing that whole thread in certain
[[540.32]]types of automation forward what do you
[[543.28]]use the big components that you all
[[546.12]]don't have or that maybe the industry
[[548.96]]more generally doesn't have that that
[[551.64]]still needs to come into place to help
[[554.56]]drive those things ahead yeah that's a
[[557.24]]that's a really good question I think
[[560.92]]there's a few things one I think like um
[[565.36]]like figuring out the right ux for a lot
[[568.64]]of these things is still an open
[[572.36]]question in my mind um and you know
[[576.52]]that's not necessarily something we can
[[580.08]]help with I think there's a lot of
[[583.56]]exploration that applications need to do
[[587.72]]to figure out how to you know
[[591.56]]communicate what these agents are good
[[595.24]]at and bad at to end users and expose
[[600.48]]ways to um maybe let them course correct
[[605.44]]and see what's going on and so you know
[[608.92]]I think we try to emphasize a lot of
[[613.44]]this um observability of intermediate
[[618.12]]steps and even correcting intermediate
[[621.28]]steps but but there's a lot of
[[624.92]]experimentation around ux that I think
[[630.2]]needs to happen um another big part I
[[636.0]]think is is basically the planning
[[641.64]]ability of the underlying llms um I
[[647.32]]think that's probably the biggest I
[[653.0]]think when we see people building agents
[[657.84]]that work right now it's often breaking
[[661.08]]it down into a bunch of smaller
[[664.04]]components and and kind of like
[[667.4]]imparting their domain knowledge about
[[670.8]]how information should flow through
[[675.08]]these components um because I think the
[[680.08]]Els by themselves still aren't able to
[[685.24]]to reason fully about how that should
[[690.0]]happen and I think we see a few kind of
[[695.24]]like uh a lot of research is actually
[[699.84]]around this I would say in the academic
[[703.76]]space specifically I think there are two
[[708.24]]different types of research papers
[[712.08]]around agents that we see we see some
[[715.84]]around like planning for agents so
[[719.12]]there's a bunch of papers that do kind
[[722.36]]of like an explicit planning Step Up
[[726.88]]Front um and then there are uh other
[[731.16]]research papers that do a bunch around
[[735.24]]reflection so like after it after uh an
[[739.56]]agent does something is this actually
[[744.08]]right how can I kind of like um you know
[[748.88]]improve upon that and I think both of
[[752.84]]those are basically trying to get around
[[757.76]]the shortcomings of llms and that in
[[762.52]]theory they should do that automatically
[[765.68]]right like you shouldn't have to ask an
[[769.92]]llm to plan or to think about whether
[[774.28]]what it's done is correct it should know
[[777.12]]to do that and then it can kind of like
[[780.04]]run in a cycle but we see a lot of
[[783.72]]shortcomings there um and so I think
[[788.64]]planning ability of llms is is is a big
[[793.52]]one and that'll get better over time the
[[797.04]]last one is maybe a little bit more
[[800.12]]vague but I think even just as Builders
[[803.36]]we're still figuring out the right ways
[[807.0]]how to make all these things work what's
[[810.6]]the right information flow between all
[[814.76]]the different nodes um in order to get
[[819.04]]those nodes which are typically an llm
[[823.44]]call to work do you want to do F shop
[[827.52]]prompting do you want to fine-tune
[[830.68]]models do you want to just work on
[[834.4]]improving the instructions and the
[[838.4]]prompt um and so I think there's a lot
[[843.6]]of uh how how do you test those nodes uh
[[847.92]]that's a big thing as well how do you
[[851.48]]get confidence in your llm systems and
[[856.48]]llm agents um and so I think there's a
[[860.84]]lot of workflow around that to to kind
[[864.76]]of like be discovered and figured out
[[867.84]]one thing that's sort of come up
[[870.76]]repeatedly Rel relative to agents has
[[874.92]]just been like memory and so I wasn't
[[879.0]]sure how you think about memory and
[[882.32]]implementing that and what that should
[[885.6]]look like and because it seems like
[[888.52]]there's a few different Notions that
[[890.68]]people have been putting forward and I
[[892.8]]think it's super interesting so I was
[[895.64]]just curious about your thinking on that
[[899.92]]I also think it's super interesting um I
[[905.8]]have a few thoughts here um so I think
[[912.36]]there's maybe two types of memory and
[[916.76]]they're and they're related but I'll
[[919.64]]draw some distinction between kind of
[[924.04]]like system level procedural memory and
[[929.8]]then like personalization type memory um
[[934.92]]so system level memory I mean more like
[[939.24]]what's the right way to use a tool
[[943.12]]what's the right way to accomplish this
[[948.0]]objective independent of of who exactly
[[953.16]]the person is and how I'm different than
[[958.0]]Sarah or something like that um and then
[[963.12]]for the personalization bit I think it's
[[968.0]]like okay you know Harrison likes soccer
[[972.12]]and he likes basketball and I should
[[976.4]]remember that when he asked questions um
[[981.36]]and so I think there's there's uh maybe
[[984.88]]slightly different ways that we see
[[989.0]]teams thinking about both of these so on
[[993.36]]the procedural side I think the main
[[996.96]]thing that we see people doing um and
[[1000.96]]that we think is pretty effective is uh
[[1005.76]]few shot prompting and maybe find tuning
[[1010.92]]for how to use uh for how to use tools
[[1014.52]]because that's basically what it comes
[[1017.08]]down to what's the right way to use
[[1020.44]]tools what's the right way to plan and
[[1024.24]]we see F shot examples being really
[[1028.24]]really impactful for that and so that's
[[1032.52]]something where and so there I think
[[1036.32]]there's this really interesting data
[[1040.4]]flywheel of like monitoring your
[[1045.72]]application Gathering good examples um
[[1050.36]]and then and then plugging those back
[[1053.56]]into your application in the form of few
[[1057.04]]shot examples that we're pushing really
[[1061.4]]heavily with laying Smith right now and
[[1065.12]]then the other side of it is just like
[[1069.64]]personalization level memory um and I
[[1074.0]]think there's a few different ways to do
[[1077.08]]this like I think open AI implemented it
[[1082.16]]in their chat uh in their chat uh GPT
[[1087.0]]where it in the way I think it does it
[[1090.16]]under the hood is it basically has
[[1093.36]]functions that it can call to say like
[[1097.72]]remember this fact or delete this fact
[[1102.16]]um and so that's a really interesting
[[1107.0]]like active um active Loop that the
[[1111.92]]agent is engaging in where it explicitly
[[1115.24]]decides what it wants to remember and
[[1118.52]]what it doesn't want to remember I also
[[1122.64]]think one thing that I'm bullish on is a
[[1127.8]]more kind of like uh passive background
[[1132.28]]uh process that kind of looks at
[[1135.88]]conversations and almost like extracts
[[1139.96]]insights um and then you can use those
[[1143.44]]insights in kind of like future
[[1146.16]]conversations and I think there's pros
[[1149.28]]and cons to each and I think it speaks
[[1153.52]]to the memory in general I feel is like
[[1158.2]]a field that's just like super super
[[1163.96]]nent like I don't I actually am am
[[1168.96]]underwhelmed at the amount of like
[[1171.96]]really interesting stuff that's going on
[[1175.68]]there um and so I think you know bunch
[[1180.08]]of different approaches no no kind of
[[1185.16]]like overwhelmingly best solution has
[[1190.8]]the um sophistication shape type of
[[1196.44]]application that you see people building
[[1201.04]]with Lang chain or just generally in the
[[1204.56]]ecosystem dramatically changed over the
[[1208.36]]last few months I do think that there
[[1212.52]]are more examples kind of as elad
[[1217.52]]mentioned of um agentic applications
[[1223.24]]that are much more productive and more
[[1228.28]]sophisticated like multi-step rag
[[1233.08]]systems with much more useful ranking
[[1237.16]]like does that match with the patterns
[[1239.96]]you're seeing or like what are you what
[[1242.76]]are you seeing that excites you the most
[[1246.0]]that you think is most useful that does
[[1249.68]]generally match I think Lang chain from
[[1253.28]]the beginning has always been focused on
[[1258.16]]those types of applications um and and
[[1263.0]]uh not only the open source but also
[[1266.6]]Lang Smith the platform so I think you
[[1269.96]]know a lot of the emphasis that we put
[[1273.44]]into like the testing and the
[[1277.24]]observability is really focused on these
[[1281.08]]like multi-step things we've always been
[[1284.8]]focused on those probably it's generally
[[1288.0]]true in the market that there's been
[[1292.24]]more uh uh of a trend towards those but
[[1296.4]]from our perspective we've always been
[[1299.48]]focused on those and so I think you know
[[1302.6]]that hasn't been as dramatic I think
[[1306.32]]there has been like interesting um
[[1310.6]]things within that that have emerged
[[1314.56]]just calling out like a few things um
[[1319.36]]within rag I think we've seen really
[[1324.48]]interesting and advanced query analysis
[[1329.0]]start to come into play so uh you know
[[1332.52]]you're not just passing the user
[[1336.0]]question directly to an embedding model
[[1340.08]]you're maybe doing some analysis on it
[[1344.24]]to figure out which which retriever
[[1348.04]]should I send it to or like what is the
[[1351.0]]bit that I should search is there kind
[[1354.76]]of like a explicit metadata filter so
[[1359.04]]some and then so that retrieval is like
[[1363.72]]a multi-step uh process and more there
[[1368.76]]um and explicitly around query analysis
[[1373.2]]um f shop prompting and that whole data
[[1376.84]]flywheel I think we're starting to see
[[1381.12]]come into play more on the agent side um
[[1385.28]]I kind of alluded to this earlier but I
[[1390.48]]think you know um the way that we've
[[1395.52]]kind of thought about things is there's
[[1398.68]]kind of like chains which are sequential
[[1401.76]]steps you're going to do this and then
[[1404.0]]you're going to do this and then you're
[[1406.04]]going to do this and you're always going
[[1409.28]]to do those in the exact sequence and
[[1414.28]]then you know last March or April or
[[1418.96]]whenever Auto GPD came out and it was
[[1421.96]]like we're literally just going to run
[[1424.64]]this in a for Loop and it's going to be
[[1428.12]]you know this autonomous agent and I
[[1432.04]]think the things that we see making it
[[1437.04]]into production and and informed um a
[[1442.8]]lot of the development of Lange graph um
[[1447.32]]is are is something in the Middle where
[[1450.92]]it's like this controlled State machine
[[1455.16]]type thing um and so we've seen a lot of
[[1459.64]]that come out recently and so i' maybe
[[1463.72]]call out that as like one um thing that
[[1467.0]]we've really updated a lot of our
[[1470.28]]beliefs on over the past few months yeah
[[1474.6]]I think a combination of that and Tre
[[1478.44]]search and just like trying to be
[[1482.04]]efficient with like your sampling at
[[1486.4]]every step has shown like a lot of
[[1490.92]]really interesting uh effective
[[1496.52]]applications recently and I think the
[[1503.24]]like cognition as one example of like a
[[1509.56]]surprisingly amazing agent has has come
[[1514.96]]out like where else do you think agent
[[1519.56]]agentic applications will begin to work
[[1523.4]]or that you've already seen I think on
[[1526.48]]the customer support side that's a
[[1530.2]]pretty obvious use case I think Sierra
[[1534.48]]um you know has emerged there and is
[[1539.08]]doing is doing quite well there um I
[[1543.6]]think yeah the cognition demo was very
[[1546.64]]impressive I think they did a lot of
[[1549.28]]things right I think they really nailed
[[1552.96]]a really interesting ux um and that was
[[1556.56]]maybe one of the things that that I was
[[1560.08]]most excited about um and then obviously
[[1564.48]]it seems to work very well and so I I
[[1567.88]]don't know exactly what they're doing
[[1572.8]]under the hood um uh but but those type
[[1578.8]]like coding coding problems in general
[[1582.64]]we see a lot of people working on I
[[1585.68]]think you there's a really nice feedback
[[1588.44]]loop that you can get by just like
[[1591.4]]executing the code and seeing if it
[[1595.24]]works um and you know as well as the
[[1599.2]]fact that people building it are
[[1602.76]]developers and so they can they can uh
[[1608.28]]test it um coding customer support there
[[1614.28]]there's some interesting stuff around
[[1620.04]]like recommend like recommendation um
[[1626.52]]chat Bots almost um so I draw a
[[1631.04]]distinction between that and customer
[[1634.0]]support or with customer support you're
[[1637.16]]maybe trying to explicitly kind of like
[[1641.16]]resolve a ticket or something like that
[[1646.36]]and the um and the recommendation bit is
[[1651.0]]a bit more focused on like a user's
[[1655.24]]preferences and and what they like um
[[1659.64]]and I think we've seen a few uh I think
[[1664.12]]we've seen a few things emerge there
[[1668.44]]um but I'd say customer support and
[[1672.4]]coding are the two Clara as well you
[[1676.68]]know they came out and and had a pretty
[[1681.88]]good release one um pattern that I think
[[1688.0]]is very popular and I can't tell if it
[[1693.84]]is real or transient is whether or not
[[1700.48]]companies will be able to switch between
[[1707.96]]different um llm models right whether
[[1714.6]]it's a you know self-hosted like
[[1721.76]]dedicated um uh inference
[[1728.72]]um you know instance for for them or if
[[1734.48]]it's an actual API provider but for any
[[1739.56]]given application take your prompts and
[[1745.0]]go from you know um anthropic to mraw to
[[1751.8]]open AI to something else um in reality
[[1757.44]]it feels like you know the way an
[[1762.08]]application uh responds is probably
[[1766.4]]going to be sensitive to the fact that
[[1770.12]]these LMS are actually going to predict
[[1773.28]]differently like what do you think about
[[1776.04]]this can you can you switch is that a
[[1779.48]]real pattern it's not as easy as it
[[1783.04]]seems like it should be and I think the
[[1787.04]]main main thing is that the prompts
[[1792.52]]still need to be different um for each
[[1799.24]]model I do think um the prompts will
[[1806.24]]probably start to converge in the sense
[[1811.44]]that if you think the models are getting
[[1814.64]]more and more intelligent then like
[[1818.16]]hopefully these small idiosyncratic EES
[[1822.92]]don't matter as much um and as more and
[[1827.32]]more model providers start supporting
[[1831.04]]the same things um then that will make
[[1834.16]]it easier and what I mean by that is you
[[1837.92]]know so many prompts for open AI which
[[1842.0]]is you know the leading and most used
[[1846.52]]one use function calling um and you know
[[1851.36]]up until some period ago like no other
[[1854.88]]models did and so you just like couldn't
[[1858.64]]use those prompts at all um but now like
[[1862.76]]mraw has function calling and and and
[[1866.2]]Google has function calling and so I
[[1868.68]]think they're a little bit more
[[1872.0]]transferable there what else is on that
[[1876.48]]list there's function calling there's
[[1880.6]]visual input like what else is going to
[[1886.24]]differentiate these um model apis
[[1891.96]]context Windows one as well so I think
[[1895.44]]this gets to like yeah what's the right
[[1898.56]]context that you can be passing if it's
[[1902.72]]longer you know if that changes then
[[1907.4]]changes that doesn't like that changes
[[1911.72]]the whole architecture of your
[[1917.72]]application um modalities one prompt
[[1923.44]]injection for
[[1928.4]]safety yeah I I think that's interesting
[[1933.68]]um I think that's a real Enterprise
[[1937.76]]concern I think a lot of the agents are
[[1941.04]]still just figuring out how to make
[[1945.0]]agents work this is a different axis
[[1949.48]]almost but to the point around like
[[1954.04]]switching models I do think we see a
[[1958.16]]desire for this especially when you
[[1961.8]]start going to scale um so I think it's
[[1966.88]]like make something work with gp4 but
[[1971.68]]then okay you're rolling it out are you
[[1975.32]]really is that like you know are you
[[1978.68]]really going to eat that much cost with
[[1983.0]]gp4 can you use GPD 3.5 do you want to
[[1988.72]]fine tune and so I think that's that
[[1993.76]]like that transition is where we really
[[1998.2]]start to see people um thinking about
[[2002.44]]switching models um there's definitely
[[2005.96]]some switching models at the beginning
[[2008.68]]like if you just want to play around
[[2010.68]]with different models and see their
[[2013.44]]capabilities but I think the most like
[[2016.88]]pressy need to switch models happens
[[2021.64]]when you go from prototype to to scale
[[2026.48]]cost and latency would be
[[2030.0]]differentiators there as well one thing
[[2032.6]]you mentioned I thought was really
[[2035.0]]interesting is just context windows and
[[2039.68]]obviously Gemini launched with um a
[[2044.84]]million token context window and I was
[[2049.68]]just curious um how you think about
[[2054.92]]context window versus rag versus other
[[2058.8]]aspects of the model and how all those
[[2062.84]]things tie together and you know once we
[[2067.04]]get to very long context windows and the
[[2070.32]]tens of millions of tokens like does
[[2074.36]]that really shift things radically or
[[2078.12]]how does that change functionality and
[[2080.44]]so I was just curious since you've
[[2082.64]]thought about how all these things piece
[[2085.52]]together
[[2088.48]]um I was just curious how you think
[[2091.52]]about those different factors and what
[[2095.12]]they mean very good question that a lot
[[2098.32]]of people are thinking about who are a
[[2102.64]]lot smarter than me I think um I mean a
[[2107.16]]few thoughts I think like longer context
[[2111.48]]Windows definitely make like single shot
[[2117.76]]things much more realistic um like
[[2124.28]]extraction of elements in a long PDF you
[[2129.8]]can do that one shot um rag over a
[[2135.72]]single long PDF or like five long PDFs
[[2140.64]]okay cool you can do that you can do
[[2145.56]]that one shot there I think um there are
[[2151.16]]definitely things at scale that don't
[[2156.4]]fit um you know into a single uh uh
[[2161.48]]context window there are also things
[[2165.48]]where it requires iterations you need to
[[2169.28]]like decide what to do interact with the
[[2172.72]]environment get that back so this whole
[[2177.0]]idea of chaining um and agents I don't
[[2181.68]]like that's that's less around context
[[2185.24]]windows and more around interacting with
[[2188.32]]the environment and getting feedback and
[[2191.12]]and so I don't think that's going
[[2195.76]]anywhere um I think with respects to rag
[[2200.44]]in particular because I think that's
[[2203.08]]where it often comes up like you know
[[2206.48]]did this kill rag um I think there's a
[[2210.2]]few things actually just today one of
[[2214.28]]our team members Lance Martin there's
[[2218.4]]that like everyone's doing the needle in
[[2221.76]]the Hast stack thing and now all these
[[2225.28]]models are like green across the board
[[2228.6]]for whatever reason they've all figured
[[2232.88]]it out um but I think like that that
[[2237.52]]actually really doesn't reflect a lot of
[[2241.48]]rag use cases in in my opinion because
[[2245.32]]like that's the needle in the Hast stack
[[2248.6]]is like okay given this long context can
[[2252.76]]I find a single information point but
[[2258.4]]often times rag is about seeing multiple
[[2263.2]]information points and then reasoning
[[2266.44]]over them and so I think with the
[[2269.6]]Benchmark he released is exactly that
[[2273.12]]like as you increase the number of of
[[2277.12]]needles um you know performance goes
[[2281.08]]down as you might expect and then also
[[2284.92]]when you ask it to reason rather than
[[2288.88]]just retrieve the performance drops as
[[2292.76]]well and so there I think there's more
[[2296.56]]work to be done there and then I think
[[2300.52]]another thing is just around the
[[2305.04]]ingestion for rag in the in the indexing
[[2309.12]]process like a lot of attention has been
[[2313.68]]paid to like um Tex splitting and
[[2319.28]]chunking and and and all of that and I
[[2323.6]]don't know exactly how that will change
[[2327.32]]like will you still do that but you now
[[2331.12]]just retrieve the whole document like we
[[2334.36]]have a concept in Lang chain of like a
[[2337.16]]parent document retriever which
[[2340.96]]basically creates multiple vectors for
[[2345.72]]for each document so maybe you just do
[[2350.44]]that maybe you still maybe you chunk it
[[2354.44]]up into larger chunks and just retrieve
[[2358.88]]those larger trunks maybe use a a
[[2363.64]]traditional search engine like elastic
[[2367.12]]search or something I'm not I'm I'm not
[[2369.96]]sure that's probably the place I have
[[2372.92]]the least confidence in the one other
[[2375.88]]area that I see a lot of people talking
[[2378.6]]about and I see a fewer people actually
[[2382.84]]doing uh is fine tuning
[[2387.44]]and um to some extent I think that's
[[2390.44]]because with fine tunes you lose
[[2393.08]]generalizability and so people just
[[2396.44]]start focusing on prompt engineering or
[[2399.64]]other ways to effectively get the same
[[2402.88]]performance without the actual fine tune
[[2406.76]]but it's something that feels very um
[[2410.68]]Aaron and people talk about it a lot and
[[2414.8]]people talk about doing it a lot um You
[[2418.04]]probably have a great perspective since
[[2420.16]]you see so many different types of
[[2423.08]]customers are are you seeing a lot of
[[2426.4]]fine tuning happening in the wild and if
[[2430.08]]so there's specific common applications
[[2434.44]]or use cases for it we see people
[[2438.28]]experimenting with it I think the only
[[2441.32]]real place where they're doing it is
[[2443.8]]when they've reached like really
[[2447.44]]critical scale um which I still don't
[[2452.12]]think is that many applications to to
[[2456.48]]date um I think there's a lot of
[[2462.16]]difficulties with it um one's like
[[2468.36]]Gathering the data set for it and so I
[[2472.04]]think a lot of the things we have in
[[2474.6]]link Smith tackle a lot of these issues
[[2478.24]]but like Gathering the data set for it
[[2482.2]]um so like having that data visibility
[[2487.28]]and starting to curate that data set um
[[2493.04]]evaluating the fine-tuned model um so
[[2497.56]]like evaluation and testing is is a huge
[[2500.72]]pain point there that we're trying to
[[2503.76]]tackle in a few ways the third is just
[[2507.08]]like yeah back to this point of people
[[2510.16]]are still just like experimenting so
[[2513.48]]rapidly it's much harder to change a
[[2517.0]]fine-tuned model than it is to change a
[[2521.32]]prompt or even changed few shot examples
[[2525.6]]and so I think we're seeing more and
[[2530.44]]more people use few shot examples um but
[[2537.16]]not a ton graduating to the fine-tuning
[[2543.72]]just because yeah I think uh much harder
[[2548.6]]to just like iterate quickly on in terms
[[2552.52]]of other major changes in the landscape
[[2557.32]]it's been it's been a big year the first
[[2561.76]]commit to Lang chain I think was in
[[2565.84]]octob October of 22 which is like when I
[[2571.72]]launched um conviction as a fund as well
[[2577.16]]uh at that time we didn't have LL 2 we
[[2581.96]]didn't have mraw there were not um
[[2588.28]]nearly as many open-source models with
[[2594.08]]um what people would consider to be more
[[2598.76]]useful reasoning ability has that
[[2604.72]]changed in terms of like what you see um
[[2610.04]]application developers do with linkchain
[[2614.92]]Gemini too oh and Gemini yeah fun story
[[2619.72]]about that the original models that we
[[2623.0]]launched with open AI actually
[[2626.84]]deprecated like a month ago so the like
[[2631.56]]actual original L chain you can't run
[[2635.88]]because the models don't exist anymore
[[2643.24]]um but yeah um like there's I think we
[[2652.12]]see increasingly interest in open source
[[2658.52]]but the reasoning abilities are still
[[2664.92]]just like lagging behind clae 3 or
[[2671.24]]gp4 um and I think like for a lot of the
[[2676.24]]applications that it kind it probably
[[2679.8]]depends on the types of applications
[[2682.64]]that you're building but a lot of the
[[2685.36]]applications that Lang train is focused
[[2688.12]]on with this kind of like reasoning
[[2693.32]]aspect those are just so crucial um and
[[2699.64]]I don't think we
[[2706.24]]see super compelling um I don't I still
[[2711.48]]don't think we see super compelling
[[2714.04]]reasoning abilities in the open source
[[2717.12]]models and maybe that's one of my hot
[[2720.08]]takes but I think for a lot of the Lang
[[2725.08]]train apps the open source models maybe
[[2730.92]]don't live up to a lot of kind of like
[[2734.32]]the the Twitter hype or Twitter
[[2738.8]]excitement at least not yet zooming out
[[2745.12]]like you have really Broad View like
[[2749.68]]what do you feel like that no one is
[[2752.44]]working on that it's going to enable
[[2755.6]]better applications that should be I
[[2759.08]]think the most exciting stuff is at the
[[2763.08]]application in ux Lay right now I think
[[2767.36]]that's where the most exciting stuff is
[[2772.12]]there one of the uh I don't know if this
[[2777.52]]is this this is maybe this is in more
[[2783.2]]the capabilities side is but like memory
[[2787.92]]I think is super interesting especially
[[2792.88]]like personalized long-term memory um I
[[2798.0]]don't know if I don't know if it's
[[2801.48]]necessarily tooling so much that needs
[[2805.16]]to be built there as it's just
[[2809.6]]like an application in a ux that's
[[2815.16]]really focused on that um and and you
[[2820.8]]know if I if I wasn't doing link chain
[[2824.76]]if I was starting a company right now
[[2827.48]]I'd probably start something at the
[[2830.4]]application layer and it would probably
[[2833.56]]be something that really takes advantage
[[2837.28]]of like long-term memory I guess at the
[[2840.64]]the high level similarly is there
[[2844.08]]anything that you view as like a major
[[2847.68]]prediction or things that'll change over
[[2851.12]]the next year that nobody's really
[[2855.2]]paying as much attention to memory is a
[[2860.0]]big interest of of of ours um and so I
[[2864.6]]hope that will have some kind of like
[[2868.8]]breakthroughs there I think a lot of the
[[2872.88]]specifically around yeah learning from
[[2876.52]]interactions incorporating that back in
[[2882.24]]at a user level um in a similar vein
[[2888.64]]also this uh type of more like system
[[2892.48]]level memory I think is really
[[2895.24]]interesting and building up building
[[2898.6]]towards this idea of almost like
[[2902.6]]continual learning so there's you know
[[2905.92]]like can you learn from your
[[2908.36]]interactions and you can do that in a
[[2911.6]]variety of different ways this may just
[[2916.4]]be where we sit in the ecosystem but one
[[2921.96]]exciting um and probably under talked
[[2926.48]]about ways it's just idea of of building
[[2930.36]]up F shot example data sets and really
[[2934.48]]using those I think it's much faster and
[[2939.48]]cheaper than fine-tuning models um it's
[[2944.36]]easier to do than trying to like
[[2948.36]]programmatically change the prompt in
[[2952.36]]some way like that's still kind of like
[[2958.32]]a a a bit of a art um and so yeah
[[2964.32]]continual towards continual learning
[[2968.64]]with few shot examples is is maybe one
[[2972.64]]like really interesting area that that
[[2975.92]]we're excited about can you help our
[[2980.32]]listeners ex like just imagine like a
[[2986.28]]little bit more vly like what um type of
[[2992.36]]application experience that would enable
[[2996.6]]like you know a consumer application or
[[2999.88]]a business application of what that type
[[3004.0]]of continuous learning would allow you
[[3008.0]]to do yeah absolutely I think at a high
[[3011.24]]level it would basically allow the
[[3015.28]]application to automatically get better
[[3019.76]]over time and it could get better in the
[[3023.28]]sense that it's just more accurate so
[[3027.72]]you know it's it maybe you know it first
[[3032.28]]does a mistake you then like tell it
[[3035.12]]that it made a mistake and it
[[3037.84]]automatically kind of like incorporates
[[3041.16]]that either as a few shot example or
[[3044.0]]update to a prompt but it starts
[[3047.76]]learning from its its mistakes and its
[[3052.48]]successes as well right there's a really
[[3057.44]]cool project called dpy or DSP I don't
[[3061.92]]know how to pronounce it um but it's out
[[3066.76]]of Stanford I say dispy oh no there's
[[3073.24]]there's three ways now I say the aspa no
[[3078.4]]I'm just
[[3083.0]]kidding so and I think that actually
[[3088.12]]tackles like I actually see a lot of
[[3093.56]]similarities between that and Lang chain
[[3098.48]]Lang Smith in some way and I think it's
[[3102.4]]all towards this idea of like so so so
[[3107.48]]dpy or disp or or whatever um is
[[3112.6]]basically this idea of like optimization
[[3117.08]]you have kind of like inputs outputs you
[[3121.72]]then have your application which uh they
[[3125.72]]similarly think as as like multiple
[[3130.4]]steps and you basically uh you basically
[[3135.92]]optimize your your application through a
[[3140.48]]variety of different ways the main one
[[3144.8]]of which I would say is probably F shot
[[3149.56]]examples so they we'll probably do a
[[3153.48]]webinar with Omar and he can correct me
[[3157.32]]if I'm wrong um and I think the idea of
[[3160.96]]like continual learning is basically
[[3164.32]]doing that optimization but in an online
[[3168.28]]manner where your feed you don't have
[[3171.72]]like ground truth necessarily but you
[[3174.96]]get feedback from the environment thumbs
[[3178.28]]up thumbs down if if things are good and
[[3181.64]]so I think yeah that kind of like
[[3185.48]]optimization Loop whether offline or
[[3190.52]]online is really really exciting and I
[[3195.8]]think a similar thing could maybe I
[[3199.6]]think you can think of like
[[3203.08]]personalization also as like what this
[[3206.84]]would look like to end users and and
[[3210.72]]maybe like consumer facing apps so you
[[3215.04]]start with like a generic application
[[3218.96]]that does the same thing for everyone
[[3222.68]]but then it maybe learns to to search
[[3226.64]]the web differently for for me and elad
[[3230.84]]or something like that um and so I think
[[3234.76]]that's like concretely how it could
[[3238.92]]could manifest cool thanks so much for
[[3243.28]]doing this um it's obviously a pleasure
[[3247.08]]to have you on no thank you guys good to
[[3249.76]]see
[[3254.16]]you find us on Twitter at no prior pod
[[3259.36]]subscribe to our YouTube channel if you
[[3263.2]]want to see our faces follow the show on
[[3267.84]]Apple podcast Spotify or wherever you
[[3271.92]]listen that way you get a new episode
[[3275.4]]every week and sign up for emails or
[[3279.28]]find transcripts for every episode at
[[3284.28]]no- prior
[[3287.2]].c
